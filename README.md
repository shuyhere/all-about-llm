# all-about-llm
å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒå’ŒæœåŠ¡è°ƒç ”
ä¸œè¥¿å¤ªå¤šäº†æ„Ÿè§‰ä¼šæ²¡æ¡ç†ï¼Œå†™åˆ°gitbookä¸Šäº†ï¼šhttps://bbb-fullstack-aier.gitbook.io/all-about-llm/

## GITBOOK æ›´æ–°
- ğŸ‘‹ğŸ½2023 -07-21 [ACL2023 Tutorial å…³äºæ£€ç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ä»¥åŠåº”ç”¨(æœªå®Œå¾…ç»­)](https://bbb-fullstack-aier.gitbook.io/all-about-llm/tototolearn/Tutorial&Workshop/acl2023-retrieval-lm)
- ğŸ‘‹ğŸ½2023-07-23 [baichuanç³»åˆ—æ¨¡å‹æ€»ç»“](https://bbb-fullstack-aier.gitbook.io/all-about-llm/tototolearn/openllm/bai-chuan-da-mo-xing)
- ğŸ‘‹ğŸ½2023-07-24 [Personality Traits in Large Language Models -Google DeepMind](https://bbb-fullstack-aier.gitbook.io/all-about-llm/tototolearn/personality-traits-and-bias-in-llm/personality-traits-in-large-language-models)
- ğŸ‘‹ğŸ½2023-07-25 [ç‚¼ä¸¹å·¥å…·ç®±--æ¨¡å‹çš„è®­ç»ƒé‡åŒ–æ¨ç†éƒ¨ç½²å·¥å…·æ€»ç»“](https://bbb-fullstack-aier.gitbook.io/all-about-llm/tototodo/lian-dan-gong-ju-xiang)
- ğŸ‘‹ğŸ½2023-07-25 [å¤§æ¨¡å‹ç»¼è¿°æ€»ç»“](https://bbb-fullstack-aier.gitbook.io/all-about-llm/tototolearn/surveys)


## ç›®å½•

- [all-about-llm](#all-about-llm)
  - [GITBOOK æ›´æ–°](#gitbook-æ›´æ–°)
  - [ç›®å½•](#ç›®å½•)
  - [LLM](#llm)
    - [1. ã€ŠGPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoEã€‹](#1-gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe)
    - [2. ç™¾å·å¤§æ¨¡å‹](#2-ç™¾å·å¤§æ¨¡å‹)
      - [æ¦‚è¿°](#æ¦‚è¿°)
      - [æ•°æ®](#æ•°æ®)
      - [æ¨¡å‹ç»“æ„](#æ¨¡å‹ç»“æ„)
      - [è®­ç»ƒç®—åŠ›](#è®­ç»ƒç®—åŠ›)
      - [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
    - [3. Aquila æ‚Ÿé“å¤©é¹°ç³»åˆ—å•†ç”¨å¼€æºæ¨¡å‹](#3-aquila-æ‚Ÿé“å¤©é¹°ç³»åˆ—å•†ç”¨å¼€æºæ¨¡å‹)
      - [æ¦‚è¿°](#æ¦‚è¿°-1)
      - [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
    - [4. Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆ](#4-chinese-vicuna-a-chinese-instruction-following-llama-based-model--ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llamaloraæ–¹æ¡ˆ)
      - [æ¦‚è¿°](#æ¦‚è¿°-2)
      - [æ•°æ®](#æ•°æ®-1)
      - [æ¨¡å‹](#æ¨¡å‹)
    - [5.  Chinese-LLaMA-Alpaca å¼€æºä¸­æ–‡LLaMAæ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹](#5--chinese-llama-alpaca-å¼€æºä¸­æ–‡llamaæ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒçš„alpacaå¤§æ¨¡å‹)
      - [æ¦‚è¿°](#æ¦‚è¿°-3)
      - [æ¨¡å‹](#æ¨¡å‹-1)
      - [é‡åŒ–æ¨ç†å’Œéƒ¨ç½²](#é‡åŒ–æ¨ç†å’Œéƒ¨ç½²)
      - [æ•°æ®](#æ•°æ®-2)
    - [6.  BELLE: Be Everyone's Large Language Model Engine](#6--belle-be-everyones-large-language-model-engine)
    - [7.Tigerbot](#7tigerbot)
      - [æ¦‚è¿°](#æ¦‚è¿°-4)
      - [æ•°æ®](#æ•°æ®-3)
      - [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½-1)
    - [8. Fastchat](#8-fastchat)
    - [9. Vicuna](#9-vicuna)
      - [æ¦‚è¿°](#æ¦‚è¿°-5)
      - [æ•°æ®](#æ•°æ®-4)
      - [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)
      - [æ¨¡å‹](#æ¨¡å‹-2)
    - [10. Koala: A Dialogue Model for Academic Research è€ƒæ‹‰å¯¹è¯æ¨¡å‹](#10-koala-a-dialogue-model-for-academic-research-è€ƒæ‹‰å¯¹è¯æ¨¡å‹)
    - [11.Fireflyï¼ˆæµè¤ï¼‰: ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹](#11fireflyæµè¤-ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹)
    - [12. Stanford Alpaca: An Instruction-following LLaMA Model](#12-stanford-alpaca-an-instruction-following-llama-model)
    - [13. GPT4all](#13-gpt4all)
    - [14. WizardLM: An Instruction-following LLM Using Evol-Instruct](#14-wizardlm-an-instruction-following-llm-using-evol-instruct)
    - [15. æ–‡å¿ƒåƒå¸†å¤§æ¨¡å‹å¹³å°](#15-æ–‡å¿ƒåƒå¸†å¤§æ¨¡å‹å¹³å°)
  - [å¤šæ¨¡æ€ Model \& å¤šæ¨¡æ€ä»»åŠ¡å®ç°æ–¹æ¡ˆ](#å¤šæ¨¡æ€-model--å¤šæ¨¡æ€ä»»åŠ¡å®ç°æ–¹æ¡ˆ)
    - [1. I-JEPA (the Image-based Joint-Embedding Predictive Architecture)](#1-i-jepa-the-image-based-joint-embedding-predictive-architecture)
    - [2. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](#2-video-llama-an-instruction-tuned-audio-visual-language-model-for-video-understanding)
    - [3. MM-React](#3-mm-react)
    - [4. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](#4-chameleon-plug-and-play-compositional-reasoning-with-large-language-models)
    - [5. Brain-inspired multimodal hybrid neural network for robot place recognition](#5-brain-inspired-multimodal-hybrid-neural-network-for-robot-place-recognition)
  - [å¤§æ¨¡å‹æœ‰å…³é¡¹ç›®å®ç° å·¥å…·\&å‚è€ƒé¡¹ç›®](#å¤§æ¨¡å‹æœ‰å…³é¡¹ç›®å®ç°-å·¥å…·å‚è€ƒé¡¹ç›®)
    - [ä¼˜åŒ–æ–¹å‘](#ä¼˜åŒ–æ–¹å‘)
    - [ä¸€äº›æ€è€ƒ](#ä¸€äº›æ€è€ƒ)
    - [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
    - [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
    - [æ¨¡å‹ä¸‹è½½\&è½¬æ¢ï¼Œè®­ç»ƒ\&å¾®è°ƒ](#æ¨¡å‹ä¸‹è½½è½¬æ¢è®­ç»ƒå¾®è°ƒ)
    - [æ¨¡å‹é‡åŒ–æ¨ç†å’Œéƒ¨ç½²](#æ¨¡å‹é‡åŒ–æ¨ç†å’Œéƒ¨ç½²)
    - [æ¡†æ¶](#æ¡†æ¶)
    - [å¤§æ¨¡å‹å¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·](#å¤§æ¨¡å‹å¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·)
      - [TALM: tool augmented language models](#talm-tool-augmented-language-models)
      - [PAL: Program-aided Language Models](#pal-program-aided-language-models)
      - [Atlas: Few-shot Learning with Retrieval Augmented Language Models](#atlas-few-shot-learning-with-retrieval-augmented-language-models)
      - [Toolformer: Language Models Can Teach Themselves to Use Tools](#toolformer-language-models-can-teach-themselves-to-use-tools)
      - [ART: Automatic multi-step reasoning and tool-use for large language models](#art-automatic-multi-step-reasoning-and-tool-use-for-large-language-models)
      - [ReAct: Synergizing Reasoning and Acting in Language Models](#react-synergizing-reasoning-and-acting-in-language-models)
      - [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](#critic-large-language-models-can-self-correct-with-tool-interactive-critiquing)
      - [Making Language Models Better Tool Learners with Execution Feedback](#making-language-models-better-tool-learners-with-execution-feedback)
    - [äººå·¥æ™ºèƒ½ä½“ AI Agent](#äººå·¥æ™ºèƒ½ä½“-ai-agent)
      - [é¡¹ç›®1ï¼šæ–¯å¦ç¦ã€è°·æ­Œã€Œè¥¿éƒ¨ä¸–ç•Œ](#é¡¹ç›®1æ–¯å¦ç¦è°·æ­Œè¥¿éƒ¨ä¸–ç•Œ)
      - [é¡¹ç›®2ï¼šCamel](#é¡¹ç›®2camel)
      - [é¡¹ç›®3ï¼šBabyAGI](#é¡¹ç›®3babyagi)
      - [é¡¹ç›®4ï¼šAutoGPT](#é¡¹ç›®4autogpt)
    - [æ•ˆæœè¯„ä¼°](#æ•ˆæœè¯„ä¼°)
    - [å¸¸è§bugè§£å†³](#å¸¸è§bugè§£å†³)
  - [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)
    - [Paper\&Post](#paperpost)
    - [å·¥å…·ç½‘å€](#å·¥å…·ç½‘å€)
    - [API tools](#api-tools)
      - [æœç´¢ç±»ï¼š](#æœç´¢ç±»)
      - [è‡ªåŠ¨åŒ–å·¥ä½œç±»ï¼š](#è‡ªåŠ¨åŒ–å·¥ä½œç±»)
    - [é¡¹ç›®æ•´åˆ](#é¡¹ç›®æ•´åˆ)
    - [ä¸€äº›å€¼å¾—å…³æ³¨çš„issues](#ä¸€äº›å€¼å¾—å…³æ³¨çš„issues)
    - [LLMæˆé•¿è·¯çº¿å›¾](#llmæˆé•¿è·¯çº¿å›¾)
    - [LLMæœ¯è¯­](#llmæœ¯è¯­)
      - [Adapter](#adapter)
      - [Alpaca](#alpaca)
      - [Attention](#attention)
      - [Cosine Similarity](#cosine-similarity)
      - [Embedding](#embedding)
      - [GPTQ](#gptq)
      - [Instruction Tuning](#instruction-tuning)
      - [LoRA](#lora)
      - [Prefix tuning https://arxiv.org/abs/2101.00190](#prefix-tuning-httpsarxivorgabs210100190)
      - [Prompt Tuning](#prompt-tuning)
      - [QLoRA](#qlora)
      - [RLHF](#rlhf)
      - [Self-instruction](#self-instruction)
      - [semantic search](#semantic-search)
      - [Tokenization](#tokenization)
      - [Topic Modeling](#topic-modeling)
      - [Transformer](#transformer)
  - [LLMs Surveys](#llms-surveys)

## LLM

### 1. [ã€ŠGPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoEã€‹](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)




### 2. [ç™¾å·å¤§æ¨¡å‹](https://github.com/baichuan-inc/baichuan-7B)
	
å®Œå…¨æ”¯æŒå•†ç”¨çš„æ¨¡å‹
å‘å¸ƒå›¢é˜Ÿï¼šç™¾å·æ™ºèƒ½
å®¶æ—ï¼šllama

æ³¨æ„ä»“åº“ä¸­çš„è¯„æµ‹ç»“æœæ˜¯åŸºäºsftå’ŒRLHF ä½†æ˜¯ç›®å‰å¼€æºçš„modelæ˜¯pretrainç‰ˆæœ¬	

é¢„è®­ç»ƒæ¨¡å‹ç»­å†™çš„å¥å­æœ‰æ¯”è¾ƒå¤§çš„é—®é¢˜ï¼ˆå¯èƒ½æ¥è‡ªäºè¯­æ–™é—®é¢˜ï¼Ÿï¼‰

{'text': 'ä»Šå¤©å¤©æ°”æ˜¯çœŸçš„æœ‰ç‚¹çƒ­,æˆ‘èµ°åœ¨è¡—ä¸Šçš„æ—¶å€™,å‘ç°äº†å¾ˆå¤šäººçš„è„¸ä¸Šæ³›äº†çº¢è‰²çš„......\næˆ‘èµ°åœ¨è¡—ä¸Šçš„æ—¶å€™,å‘ç°äº†å¾ˆå¤šäººçš„è„¸ä¸Šæ³›äº†çº¢è‰²çš„,æˆ‘é—®è€çˆ¸:ä¸ºä»€ä¹ˆç°åœ¨æœ‰è¿™ä¹ˆå¤šäººçš„è„¸ä¸Šéƒ½é•¿äº†æ–‘å‘¢?è€çˆ¸è¯´:å› ä¸ºå¤©æ°”å¤ªçƒ­äº†,è„¸ä¸Šæ²¡æœ‰æ±—æ°´çš„æ»‹æ¶¦,çš®è‚¤æ²¡æœ‰å…‰æ³½äº†ã€‚ä½ åˆ«é—®äº†,æˆ‘ç°åœ¨æ­£åœ¨å»å¼€ä¼šçš„åœ°æ–¹çš„è·¯ä¸Šã€‚'}

**7bé¢„è®¡åœ¨A100å ç”¨æ˜¾å­˜ ï¼š27689MB  ï¼› 4090å ç”¨æ˜¾å­˜23.6G æ¨ç†é€Ÿåº¦å¾ˆæ…¢**

####  æ¦‚è¿°

åŸºäºTransformerç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿tokensä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º**4096**P

**é’ˆå¯¹chatçš„å¾®è°ƒæ–¹æ¡ˆï¼ˆéå®˜æ–¹ï¼‰**ï¼š
- MedicalGPTï¼šhttps://github.com/shibing624/MedicalGPT/blob/main/README.md
- LLaMA-Efficient-Tuning ï¼š
https://github.com/hiyouga/LLaMA-Efficient-Tuning

**é—®é¢˜ï¼šè¯„æµ‹å°é—­æ€§ä¸å¥½**ï¼Ÿâ¬‡ï¸

	è¾“å…¥ï¼š
	åœ¨Unixä¸­ï¼Œpasswdå‘½ä»¤ä½äº____ç›®å½•ä¸­çš„ã€‚
	è¾“å‡ºï¼š
	åœ¨Unixä¸­ï¼Œpasswdå‘½ä»¤ä½äº____ç›®å½•ä¸­çš„ã€‚ A. /etc/ B. /usr/ C. /bin/ D. /usr/bin/ ç­”æ¡ˆï¼šA</s>
**åˆ†è¯**
ä½¿ç”¨SentencePieceä¸­çš„Byte-PairEncoding(BPE)ä½œä¸ºåˆ†è¯ç®—æ³•ï¼Œå¹¶ä¸”è¿›è¡Œäº†ä»¥ä¸‹çš„ä¼˜åŒ–ï¼š
* ä½¿ç”¨2000ä¸‡æ¡ä»¥ä¸­è‹±ä¸ºä¸»çš„å¤šè¯­è¨€è¯­æ–™è®­ç»ƒåˆ†è¯æ¨¡å‹ï¼Œæå‡å¯¹äºä¸­æ–‡çš„å‹ç¼©ç‡ã€‚
* å¯¹äºæ•°å­¦é¢†åŸŸï¼Œå‚è€ƒäº†LLaMAå’ŒGalacticaä¸­çš„æ–¹æ¡ˆï¼Œå¯¹æ•°å­—çš„æ¯ä¸€ä½å•ç‹¬åˆ†å¼€ï¼Œé¿å…å‡ºç°æ•°å­—ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¹äºæå‡æ•°å­¦èƒ½åŠ›æœ‰é‡è¦å¸®åŠ©ã€‚
* å¯¹äºç½•è§å­—è¯ï¼ˆå¦‚ç‰¹æ®Šç¬¦å·ç­‰ï¼‰ï¼Œæ”¯æŒUTF-8charactersçš„byteç¼–ç ï¼Œå› æ­¤åšåˆ°æœªçŸ¥å­—è¯çš„å…¨è¦†ç›–ï¼Œè¯è¡¨å¤§å°è¾¾åˆ°6.4ä¸‡ã€‚
* å¯¹ä¸­æ–‡çš„å‹ç¼©ç‡æ¯”è¾ƒé«˜ï¼ˆï¼Ÿå‹ç¼©ç‡å’‹ç®—çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼‰

| Model         | baichuan\-7B | LLaMA  | Falcon | mpt\-7B | ChatGLM | moss\-moon\-003 |
|---------------|--------------|--------|--------|---------|---------|-----------------|
| Compress Rate | 0\.737       | 1\.312 | 1\.049 | 1\.206  | 0\.631  | 0\.659          |
| Vocab Size    | 64,000       | 32,000 | 65,024 | 50,254  | 130,344 | 106,029         |

#### æ•°æ®
![æ•°æ®å¤„ç†æµç¨‹](figure/image.png)

#### æ¨¡å‹ç»“æ„
  æ•´ä½“æ¨¡å‹åŸºäºæ ‡å‡†çš„ Transformer ç»“æ„ï¼Œé‡‡ç”¨äº†å’Œ LLaMA ä¸€æ ·çš„æ¨¡å‹è®¾è®¡
   ![æ¨¡å‹ç»“æ„](figure/image0.png)
- **ä½ç½®ç¼–ç ** rotary-embedding æ˜¯ç°é˜¶æ®µè¢«å¤§å¤šæ¨¡å‹é‡‡ç”¨çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå…·æœ‰æ›´å¥½çš„å¤–å»¶æ•ˆæœã€‚è™½ç„¶è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§é•¿åº¦ä¸º4096ï¼Œä½†æ˜¯å®é™…æµ‹è¯•ä¸­æ¨¡å‹å¯ä»¥å¾ˆå¥½çš„æ‰©å±•åˆ° 5000 tokens ä¸Šï¼Œå¦‚ä¸‹å›¾ï¼š
  ![Alt text](figure/image1.png)
- **å…·ä½“å‚æ•°**

	| **Hyperparameter** | **Value**  |
	|--------------------|------------|
	| n_parameters       | 7000559616 |
	| n_layers           | 32         |
	| n_heads            | 32         |
	| d_model            | 4096       |
	| vocab size         | 64000      |
	| sequence length    | 4096       |


- æ¿€æ´»å±‚ï¼šSwiGLU, Feedforward å˜åŒ–ä¸º(8/3)å€çš„éšå«å±‚å¤§å°ï¼Œå³11008
- Layer-Normalization: åŸºäº RMSNorm çš„ Pre-Normalization
#### è®­ç»ƒç®—åŠ›
åƒå¡A800æœºå™¨ä¸Šè¾¾åˆ°äº†7Bæ¨¡å‹182Tflopsçš„ååï¼ŒGPUå³°å€¼ç®—åŠ›åˆ©ç”¨ç‡é«˜è¾¾58.3% ã€‚è®­ç»ƒç¨³å®šæ€§å’Œåå
#### æ¨¡å‹æ¨ç†

[huggingface](https://huggingface.co/baichuan-inc/baichuan-7B)


### 3. [Aquila æ‚Ÿé“å¤©é¹°ç³»åˆ—å•†ç”¨å¼€æºæ¨¡å‹](https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README.md)

**å‘å¸ƒå›¢é˜Ÿ**ï¼šæ™ºæºç¤¾åŒº
è¦ç”¨flagAIæ¡†æ¶ï¼ˆè¿™ä¸ªæ¡†æ¶æºç &ä¾èµ–çš„å…¼å®¹æ€§è¿˜æœ‰ä¸€äº›é”™è¯¯ï¼‰
å…³æ³¨issueï¼šhttps://github.com/FlagAI-Open/FlagAI/issues/334
è¿™ä¸ªé—®é¢˜å®˜æ–¹æ­£åœ¨ä¿®å¤
#### æ¦‚è¿°
**å®˜æ–¹ç®€ä»‹**
Aquilaè¯­è¨€å¤§æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº†GPT-3ã€LLaMAç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„tokenizerï¼Œå‡çº§äº†BMTrainå¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œåœ¨Aquilaçš„è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†æ¯”Magtron+DeepSpeed zero-2å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚Aquilaè¯­è¨€å¤§æ¨¡å‹æ˜¯åœ¨ä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™åŸºç¡€ä¸Šä»ï¼å¼€å§‹è®­ç»ƒçš„ï¼Œé€šè¿‡æ•°æ®è´¨é‡çš„æ§åˆ¶ã€å¤šç§è®­ç»ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œå®ç°åœ¨æ›´å°çš„æ•°æ®é›†ã€æ›´çŸ­çš„è®­ç»ƒæ—¶é—´ï¼Œè·å¾—æ¯”å…¶å®ƒå¼€æºæ¨¡å‹æ›´ä¼˜çš„æ€§èƒ½ã€‚ä¹Ÿæ˜¯é¦–ä¸ªæ”¯æŒä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€ç¬¦åˆå›½å†…æ•°æ®åˆè§„éœ€è¦çš„å¤§è§„æ¨¡å¼€æºè¯­è¨€æ¨¡å‹ã€‚

**æœ€ä½ç¡¬ä»¶éœ€æ±‚**ï¼š **è¿è¡ŒAquila-7Bç³»åˆ—éœ€è¦å†…å­˜30G, æ˜¾å­˜18Gï¼Œç”Ÿæˆæœ€å¤§é•¿åº¦ 2048 tokensã€‚**

Aquilaæ¨¡å‹æ‰€é‡‡ç”¨çš„tokenizeræ˜¯ç”±ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚æˆ‘ä»¬åœ¨å¤„ç†è‹±æ–‡ã€ä¸­æ–‡ä»¥åŠä»£ç æ•°æ®æ—¶ï¼Œé‡‡ç”¨äº†ä¸åŒçš„åˆ†è¯å™¨å¯¹ä¸€ä¸‡ä¸ªæ ·æœ¬è¿›è¡Œäº†æŠ½å–ã€‚Aquila tokenizerä¸å…¶ä»–tokenizerçš„å‚æ•°å¯¹æ¯”è§ä¸‹è¡¨:

| **æ¨¡å‹/Model** | **è¯è¡¨å¤§å°/Vocab size** | **è¯´æ˜/Note** | **è‹±æ–‡å¹³å‡tokensé‡/Avg tokens \(English \)** | **ä¸­æ–‡å¹³å‡tokensé‡/Avg tokens \(Chinesse \)** | **ä»£ç å¹³å‡tokensé‡/Avg tokens \(code\)** |
|--------------|---------------------|-------------|---------------------------------------|----------------------------------------|------------------------------------|
| GPT2         | 50527               | bpe         | 1717                                  | 1764                                   | 2323                               |
| LlaMA        | 32000               | sp\(bpe\)   | 1805                                  | 1257                                   | 1970                               |
| Aquila       | 100000              | bpe         | 1575                                  | 477                                    | 1679                               |

#### æ¨¡å‹ä¸‹è½½
- è„šæœ¬ä¸‹è½½
-  FlagOpen æ¨¡å‹ä»“åº“ä¸‹è½½ https://model.baai.ac.cn/models

å…¶ä»–ä¿¡æ¯ç­‰å¾…å…­æœˆåº•å®˜æ–¹æŠ€æœ¯æŠ¥å‘Š


### 4. [Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆ](https://github.com/Facico/Chinese-Vicuna)

è¿™ç§æ–¹æ¡ˆå¯èƒ½ä¼šé™ä½æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

gitä»“åº“æä¾›äº†ä¸€ä¸ªé’ˆå¯¹llamaç³»åˆ—å·¥ç¨‹å¾ˆå¥½çš„QAæ–‡æ¡£ï¼šhttps://github.com/Facico/Chinese-Vicuna/blob/master/docs/notes.md

æœ‰å…³æŠ€æœ¯
- LLaMA paper: https://arxiv.org/abs/2302.13971v1
- Self-Instruct paper: https://arxiv.org/abs/2212.10560
- data generation: https://github.com/LianjiaTech/BELLE and https://guanaco-model.github.io/
- the first work: https://github.com/tatsu-lab/stanford_alpaca
- Lora paperï¼šhttps://arxiv.org/pdf/2106.09685.pdf
#### æ¦‚è¿°
**ç‰¹ç‚¹**ï¼šllama+ä¸­æ–‡+ä½èµ„æº+å‚æ–™è®­ç»ƒçš„æ–¹æ¡ˆ 
  åŸºäºLLaMA+instructionæ•°æ®æ„å»ºä¸€ä¸ªä¸­æ–‡çš„ç¾Šé©¼æ¨¡å‹ï¼Œå¹¶å¸®åŠ©å¤§å®¶èƒ½å¿«é€Ÿå­¦ä¼šä½¿ç”¨å¼•å…¥è‡ªå·±çš„æ•°æ®ï¼Œå¹¶è®­ç»ƒå‡ºå±äºè‡ªå·±çš„å°ç¾Šé©¼ï¼ˆVicunaï¼‰
  ç±»ä¼¼äºstable diffusionæ¨¡å‹çš„çˆ†ç«ï¼Œå‡ºç°äº†åƒcivitaiç­‰å¹³å°ï¼Œç”±ä¸€ä¸ªåŸºç¡€çš„æ¨¡å‹+å„ç§LORAæ¨¡å‹çš„å¼€æºç¤¾åŒºã€‚è¿™ä¸ªé¡¹ç›®å°±æ˜¯å»è®­ç»ƒè¿™ä¸ªLORA
**Loraè®­ç»ƒæ–¹å¼**ï¼š
- ä»£ç åŸºäºalpaca-loraå¼€å‘ï¼Œhttps://github.com/tloen/alpaca-lora
- è¿™æ˜¯ä¸€å¥—æ¯”è¾ƒç®€å•çš„ä»£ç ï¼ŒåŸºæœ¬æ€è·¯å°±æ˜¯ç”¨PEFTçš„loraæ¥å£+transformerçš„trainer+instructionçš„æ•°æ®é…ç½®
**è®­ç»ƒç®—åŠ›é…ç½®ï¼š**ï¼ˆæ¥æºä¸ºé¡¹ç›®githubï¼‰
	4090çš„ç®—åŠ›çº¦ä¸º3090ä¸¤å€ï¼ŒA100-40Gçš„int8ç®—åŠ›ä¸4090ç›¸è¿‘
  
	| **Model** | **GPU** | **lora+fp16+512** |  | **lora+int8+256** |  | **lora+int8+512** |  | **lora+int8+2048** | |
	|-----------|---------|--------------------|------|--------------------|------|--------------------|------|---------------------|------|
	|           |         | speed              | size | speed              | size | speed              | size | speed               | size |
	| LLaMA-7B  | 2080Ti  |                    |      | 0.2h/w            | 11G  |                    |      |                     |      |
	|           | 3090    |                    |      |                    |      |                    |      |                     |      |
	|           | 4090    | 0.3h/w            | 20G  |                    |      | 0.8h/w            |      | 3.5h/w              | 20G  |
	| LLaMA-13B | 3090    |                    |      | 0.9h/w            |      |                    |      | 7.5h/w              | 24G  |
	|           | 4090    |


**æ³¨æ„ï¼š**
- int8 ä»…åŠ è½½æ¨¡å‹æ˜¾å­˜å ç”¨ï¼ˆVRAMï¼‰$ \approx $ ç¡¬ç›˜ç©ºé—´å¤§å°ï¼Œæ¯”å¦‚7Bå¤§æ¦‚8Gå·¦å³ï¼Œ13Bå¤§æ¦‚14Gå·¦å³ï¼›å¦‚æœæ˜¯fp16å’Œfp32åˆ™ç›¸åº”ä¹˜2å’Œä¹˜4
- è®­ç»ƒçš„æ—¶å€™ï¼Œæ˜¾å­˜å ç”¨å’Œè®­ç»ƒçš„é€Ÿåº¦å’Œåºåˆ—é•¿åº¦å¯†åˆ‡ç›¸å…³ï¼Œæ¯”å¦‚åºåˆ—é•¿åº¦256æ˜¾å­˜å ç”¨ä¸è¶…è¿‡11Gï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥åœ¨2080Tiä¸Šå¾®è°ƒ7Bï¼Œåºåˆ—é•¿åº¦å¦‚æœæ˜¯2048ï¼Œåˆ™æ˜¾å­˜å ç”¨ä¼šéª¤å¢åˆ°20Gï¼Œå°±è¦ä¸Š3090æˆ–è€…4090æ‰èƒ½å¾®è°ƒ7Bäº†ï¼›
- åŒç†ï¼Œ13Båœ¨3090/4090ä¸Šæ˜¯å¯ä»¥å¾®è°ƒçš„ï¼Œ2048çš„æ—¶å€™microbatché™åˆ°1ä¹Ÿæ˜¯å¯ä»¥è·‘çš„
- å¦å¤–ï¼Œæœ‰äººå‘ç°åœ¨A100-40Gä¸Šå¢å¤§batchæ²¡æœ‰æ˜æ˜¾çš„æé€Ÿï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºint8æ¯”è¾ƒåƒç®—åŠ›ï¼ˆæ¯”å¦‚ç›¸åŒçš„é…ç½®fp16å¿«äºint8ï¼‰ï¼Œç®—åŠ›åƒæ»¡åå¢åŠ batchä¹Ÿä¸èƒ½æé«˜ååé‡ï¼Œå¦ä¸€æ–¹é¢A100-40Gçš„int8ç®—åŠ›å…¶å®å’Œ4090å·®ä¸å¤šã€‚
#### æ•°æ®
åˆ©ç”¨äº†ç›®å‰å‡ ä»½é«˜è´¨é‡çš„å¼€æºæ•°æ®ï¼ˆè¿™äº›æ•°æ®å¾ˆå¤šéƒ½åƒalpacaé‚£æ ·ï¼Œä½¿ç”¨chatgptçš„æ¥å£ï¼Œç”Ÿæˆé«˜è´¨é‡çš„instructionæ•°æ®ã€‚ï¼‰
{"instruction": "ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\\n\n", "input": "", "output": "åœ°çƒä¸Šæœ‰é€‚å®œç”Ÿå‘½å­˜åœ¨çš„æ¡ä»¶å’Œå¤šæ ·åŒ–çš„ç”Ÿå‘½å½¢å¼ã€‚"}
- Belle https://github.com/LianjiaTech/BELLE
- guanaco https://huggingface.co/datasets/JosephusCheung/GuanacoDataset
æ•°æ®ä¸‹è½½é“¾æ¥
- æœ‰æ›´å¥½å¯¹è¯èƒ½åŠ›çš„chatv1çš„å¾®è°ƒæ•°æ®ï¼šhttps://huggingface.co/datasets/Chinese-Vicuna/instruct_chat_50k.jsonl ï¼šç”±3ä¸‡æ¡sharegptä¸­æ–‡æ•°æ®å’Œ2ä¸‡æ¡alpaca-instruction-Chinese-datasetæ•°æ®ç»„æˆ
- é“¾æ¥: https://pan.baidu.com/s/1WSxuhSAotl14ifaAiz5eKw?pwd=b4kb æå–ç : b4kb
- é“¾æ¥: https://drive.google.com/file/d/1tzXVhS74m-EtoFot7hEc005LDeZGPit_/view?usp=sharing
- é“¾æ¥: https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0 ï¼ˆ694K rows  409MBï¼‰
#### æ¨¡å‹
- ä¸Šæ¸¸æ¨¡å‹ï¼šLLAMA 7B
- é¡¹ç›®æä¾›äº†loraæ¨¡å‹
  - åŠ è½½æ–¹å¼å‚è€ƒgenerate.py
    - Chinese-Vicuna/Chinese-Vicuna-lora-7b-belle-and-guanaco
    - Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco
  - æ¨¡å‹ä½¿ç”¨çš„æ˜¯8bit+lora+256 tokens
  - æ›´å¤šæ¨¡å‹ï¼šhttps://huggingface.co/Chinese-Vicuna

### 5.  [Chinese-LLaMA-Alpaca å¼€æºä¸­æ–‡LLaMAæ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

chinese-vicunaä¸­æåˆ°è¿™ä¸ªé¡¹ç›®ï¼š**åšäº†è¯è¡¨æ‰©å……ä½†æ˜¯æ•ˆæœä¸åŠæ²¡æœ‰æ‰©å……è¯è¡¨çš„fastchat-vicuna**

è¿™ä¸ªé¡¹ç›®ç»™äº†ä¸€ä¸ªæ‰©å……è¯è¡¨çš„æ€è·¯&å¼€æºäº†å¾ˆå¤šæœ‰ç”¨çš„å·¥å…·å’Œä»£ç ;é¡¹ç›®wikiæœ‰å¾ˆå¤šå¹²è´§;è®­ç»ƒä»£ä»·æ¯”è¾ƒå¤§

#### æ¦‚è¿°

**æŠ€æœ¯æŠ¥å‘Š**ï¼š[Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)

- ğŸš€ é’ˆå¯¹åŸç‰ˆLLaMAæ¨¡å‹æ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œæå‡äº†ä¸­æ–‡ç¼–è§£ç æ•ˆç‡
- ğŸš€ å¼€æºäº†ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬æ•°æ®é¢„è®­ç»ƒçš„ä¸­æ–‡LLaMAä»¥åŠç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ä¸­æ–‡Alpaca
- ğŸš€ å¼€æºäº†é¢„è®­ç»ƒè„šæœ¬ã€æŒ‡ä»¤ç²¾è°ƒè„šæœ¬ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€è¦è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹
- ğŸš€ å¿«é€Ÿä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆä¸ªäººPCï¼‰çš„CPU/GPUæœ¬åœ°é‡åŒ–å’Œéƒ¨ç½²ä½“éªŒå¤§æ¨¡å‹
- ğŸš€ æ”¯æŒğŸ¤—transformers, llama.cpp, text-generation-webui, LlamaChat, LangChain, privateGPTç­‰ç”Ÿæ€
- ç›®å‰å·²å¼€æºçš„æ¨¡å‹ç‰ˆæœ¬ï¼š7Bï¼ˆåŸºç¡€ç‰ˆã€Plusç‰ˆï¼‰ã€13Bï¼ˆåŸºç¡€ç‰ˆã€Plusç‰ˆï¼‰ã€33Bï¼ˆåŸºç¡€ç‰ˆï¼‰ ï¼ˆæ¨¡å‹å¤§å°ç›¸åŒ plusç‰ˆæœ¬ä½¿ç”¨çš„æ•°æ®æ›´å¤šï¼‰
  
**è®­ç»ƒé…ç½®ï¼š**
ä»¥ä¸‹æ˜¯è®­ç»ƒåŸºç¡€ç‰ˆ7Bæ¨¡å‹çš„è®­ç»ƒé…ç½®ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒæŠ€æœ¯æŠ¥å‘Šã€‚

| **å®éªŒè®¾ç½®**                   | **é¢„è®­ç»ƒ-ç¬¬ä¸€é˜¶æ®µ**     | **é¢„è®­ç»ƒ-ç¬¬äºŒé˜¶æ®µ**     | **æŒ‡ä»¤ç²¾è°ƒ**          |
|----------------------------|-------------------|-------------------|-------------------|
| Batch Size                 | 1024              | 1024              | 512               |
| Initial Learning Rate      | 2.00E-04        | 1.00E-04        | 1.00E-04        |
| Training Steps             | 3K                | 6K                | 6K-10K           |
| Max Length                 | 512               | 512               | 512               |
| Trainable Parameters (%) | 2.97%            | 6.06%            | 6.22%            |
| Training Device            | 8 Ã— A100          | 16 Ã— A100         | 16 Ã— A100         |
| Distributed Training       | DeepSpeed Zero-2 | DeepSpeed Zero-2 | DeepSpeed Zero-2 |

**è®­ç»ƒç»†èŠ‚**
- åœ¨é€šç”¨ä¸­æ–‡è¯­æ–™ä¸Šè®­ç»ƒäº†åŸºäºsentencepieceçš„20Kä¸­æ–‡è¯è¡¨å¹¶ä¸åŸç‰ˆLLaMAæ¨¡å‹çš„32Kè¯è¡¨è¿›è¡Œåˆå¹¶--æœ€ç»ˆè¯è¡¨å¤§å°49953&49954
- æ›´å¤šç»†èŠ‚è§ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82


#### æ¨¡å‹
å‘å¸ƒçš„æ˜¯LoRAæƒé‡ï¼Œéœ€è¦æ­é…[åŸç‰ˆLLaMAæ¨¡å‹](https://github.com/facebookresearch/llama)ï¼Œç„¶å[åˆå¹¶æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E5%90%88%E5%B9%B6%E6%A8%A1%E5%9E%8B)ã€‚
ä¸‹è½½ååŠ¡å¿…æ£€æŸ¥å‹ç¼©åŒ…ä¸­æ¨¡å‹æ–‡ä»¶çš„SHA256æ˜¯å¦ä¸€è‡´ï¼Œè¯·æŸ¥çœ‹[SHA256.md](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md)ã€‚

**æ¨¡å‹å¯¹æ¯”**

| **å¯¹æ¯”é¡¹**                  | **ä¸­æ–‡LLaMA**                 | **ä¸­æ–‡Alpaca**                            |
|--------------------------|-----------------------------|-----------------------------------------|
| è®­ç»ƒæ–¹å¼                     | ä¼ ç»ŸCLM                       | æŒ‡ä»¤ç²¾è°ƒ                                    |
| è®­ç»ƒè¯­æ–™                     | æ— æ ‡æ³¨é€šç”¨è¯­æ–™                     | æœ‰æ ‡æ³¨æŒ‡ä»¤æ•°æ®                                 |
| è¯è¡¨å¤§å°              | 49953                       | 49954=49953\+1ï¼ˆpad tokenï¼‰               |
| è¾“å…¥æ¨¡æ¿                     | ä¸éœ€è¦                         | éœ€è¦ç¬¦åˆæ¨¡æ¿è¦æ±‚                         |
| é€‚ç”¨åœºæ™¯ âœ”ï¸                  | æ–‡æœ¬ç»­å†™ï¼šç»™å®šä¸Šæ–‡å†…å®¹ï¼Œè®©æ¨¡å‹ç»§ç»­å†™ä¸‹å»ï¼Œç”Ÿæˆä¸‹æ–‡   | "1ã€æŒ‡ä»¤ç†è§£ï¼ˆé—®ç­”ã€å†™ä½œã€å»ºè®®ç­‰ï¼‰                      |
|å¤šè½®ä¸Šä¸‹æ–‡ç†è§£ï¼ˆèŠå¤©ç­‰ï¼‰          |
| ä¸é€‚ç”¨åœºæ™¯ âŒ                  | æŒ‡ä»¤ç†è§£ ã€å¤šè½®èŠå¤©ç­‰                 | æ–‡æœ¬æ— é™åˆ¶è‡ªç”±ç”Ÿæˆ                               |
| llama\.cpp               | ä½¿ç”¨\-på‚æ•°æŒ‡å®šä¸Šæ–‡                 | ä½¿ç”¨\-inså‚æ•°å¯åŠ¨æŒ‡ä»¤ç†è§£\+èŠå¤©æ¨¡å¼                   |
| text\-generation\-webui  | ä¸é€‚åˆchatæ¨¡å¼                   | ä½¿ç”¨\-\-cpuå¯åœ¨æ— æ˜¾å¡å½¢å¼ä¸‹è¿è¡Œï¼Œè‹¥ç”Ÿæˆå†…å®¹ä¸æ»¡æ„ï¼Œå»ºè®®ä¿®æ”¹prompt |
| LlamaChat                | åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©"LLaMA"              | åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©"Alpaca"                         |
| HFæ¨ç†ä»£ç                    | æ— éœ€æ·»åŠ é¢å¤–å¯åŠ¨å‚æ•°                  | å¯åŠ¨æ—¶æ·»åŠ å‚æ•° \-\-with\_prompt                |
| web\-demoä»£ç               | ä¸é€‚ç”¨                         | ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯ï¼›æ”¯æŒå¤šè½®å¯¹è¯                 |
| LangChainç¤ºä¾‹ / privateGPT | ä¸é€‚ç”¨                         | ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯                        |
| å·²çŸ¥é—®é¢˜                     | å¦‚æœä¸æ§åˆ¶ç»ˆæ­¢ï¼Œåˆ™ä¼šä¸€ç›´å†™ä¸‹å»ï¼Œç›´åˆ°è¾¾åˆ°è¾“å‡ºé•¿åº¦ä¸Šé™ã€‚ | ç›®å‰ç‰ˆæœ¬æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ç›¸å¯¹çŸ­ä¸€äº›ï¼Œæ¯”è¾ƒæƒœå­—å¦‚é‡‘ã€‚å¯åœ¨æŒ‡ä»¤ä¸­è¦æ±‚è¯¦ç»†å›ç­”ã€‚  |

**Loraä¸‹è½½ï¼š**

| **æ¨¡å‹åç§°**                     | **è®­ç»ƒæ•°æ®** | **é‡æ„æ¨¡å‹**     | **å¤§å°** | **LoRAä¸‹è½½** |
|------------------------------|----------|-------------------|-------------|-----------------|
| Chinese\-LLaMA\-7B           | é€šç”¨20G    | åŸç‰ˆLLaMA\-7B       | 770M        | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1xV1UXjh1EPrPtXg6WyG7XQ?pwd=923e) [googledrive](https://drive.google.com/file/d/1JvFhBpekYiueWiUL3AF1TtaWDb3clY5D/view?usp=sharing)    |          |
| Chinese\-LLaMA\-Plus\-7B â­ï¸  | é€šç”¨120G   | åŸç‰ˆLLaMA\-7B       | 790M        |[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/12tjjxmDWwLBM8Tj_7FAjHg?pwd=32hc) [googledrive](https://drive.google.com/file/d/1EDcTmq6tDmRxqarpapdyDGBE9opY0zrB/view?usp=share_link)     |         |
| Chinese\-LLaMA\-13B          | é€šç”¨20G    | åŸç‰ˆLLaMA\-13B      | 1\.0G       | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1wYoSF58SnU9k0Lndd5VEYg?pwd=mm8i) [googledrive](https://drive.google.com/file/d/1gzMc0xMCpXsXmU1uxFlgQ8VRnWNtDjD8/view?usp=share_link)       |          |
| Chinese\-LLaMA\-Plus\-13B â­ï¸ | é€šç”¨120G   | åŸç‰ˆLLaMA\-13B      | 1\.0G       | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1Mew4EjBlejWBBB6_WW6vig?pwd=mf5w) [googledrive](https://drive.google.com/file/d/1CcLJvY7XsFAOjfSIqCpDI7jf3EEPDcEF/view?usp=share_link)       |              |
| Chinese\-LLaMA\-33B          | é€šç”¨20G    | åŸç‰ˆLLaMA\-33B\ | 2\.7G       |  [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1fey7lGMMw3GT982l8uJYMg?pwd=2f2s) [googledrive](https://drive.google.com/file/d/1YeSgnZWaRkKdmYa-JHiIlcvqhrDd4-Y4/view?usp=share_link)          |

åŒæ—¶ï¼šå¯ä»¥åœ¨ğŸ¤—Model Hubä¸‹è½½ä»¥ä¸Šæ‰€æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”ä½¿ç”¨transformerså’ŒPEFTè°ƒç”¨ä¸­æ–‡LLaMAæˆ–Alpaca LoRAæ¨¡å‹ã€‚è¯¦ç»†è§é¡¹ç›®gitä»“åº“

#### é‡åŒ–æ¨ç†å’Œéƒ¨ç½²
[å‚è€ƒ](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%B8%8E%E9%83%A8%E7%BD%B2)

#### æ•°æ®

**é¢„è®­ç»ƒè¯­æ–™**
åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨çº¦20Gå·¦å³çš„é€šç”¨ä¸­æ–‡è¯­æ–™ï¼ˆä¸ä¸­æ–‡BERT-wwmã€MacBERTã€LERTã€PERTä¸­ä½¿ç”¨çš„è¯­æ–™ä¸€è‡´ï¼‰åœ¨åŸç‰ˆLLaMAæƒé‡çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è¿›è¡Œé¢„è®­ç»ƒ

**æŒ‡ä»¤ç²¾è°ƒè¯­æ–™**

| **æ•°æ®**              | **é‡çº§** | **æ¥æº** | **è¯´æ˜**                         |
|---------------------|--------|--------|--------------------------------|
| ä¸­è‹±ç¿»è¯‘æ•°æ®              | 500K   | [å¤–éƒ¨é“¾æ¥](https://github.com/brightmart/nlp_chinese_corpus#5%E7%BF%BB%E8%AF%91%E8%AF%AD%E6%96%99translation2019zh)   | åœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡‡æ ·\+è§„åˆ™ç­›é€‰           |
| pCLUEæ•°æ®             | 300K   | [å¤–éƒ¨é“¾æ¥](https://github.com/CLUEbenchmark/pCLUE)   | åœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡‡æ ·\+è§„åˆ™ç­›é€‰           |
| Alpacaæ•°æ®ï¼ˆè‹±ï¼‰         | 50K    | [å¤–éƒ¨é“¾æ¥](htthttps://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/dataps://github.com/tatsu-lab/stanford_alpaca)  | æ–¯å¦ç¦åŸç‰ˆAlpacaè®­ç»ƒæ•°æ®                |
| Alpacaæ•°æ®ï¼ˆä¸­ï¼‰         | 50K    | [å¤–éƒ¨é“¾æ¥]()  | æœ¬é¡¹ç›®ä½¿ç”¨ChatGPTæ¥å£å°†è‹±æ–‡ç‰ˆç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆç­›æ‰ä¸€éƒ¨åˆ†ï¼‰ |
| Self\-instructionæ•°æ® | 1~2M   | ï¼ˆæš‚æ— ï¼‰   | æœ¬é¡¹ç›®ä½¿ç”¨ChatGPTæ¥å£è¿›è¡Œçˆ¬å–ï¼Œå…·ä½“è§ä»¥ä¸‹è„šæœ¬æè¿°   |

æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªåŠ¨æ€ç”Ÿæˆä¸åŒé¢†åŸŸå’ŒæŒ‡ä»¤ç±»å‹çš„promptçˆ¬å–è„šæœ¬ `script/crawl_prompt.py`

### 6.  [BELLE: Be Everyone's Large Language Model Engine](https://github.com/LianjiaTech/BELLE)

ç›¸æ¯”å¦‚ä½•åšå¥½å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒï¼ŒBELLEæ›´å…³æ³¨å¦‚ä½•åœ¨å¼€æºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¸®åŠ©æ¯ä¸€ä¸ªäººéƒ½èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå±äºè‡ªå·±çš„ã€æ•ˆæœå°½å¯èƒ½å¥½çš„å…·æœ‰æŒ‡ä»¤è¡¨ç°èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ï¼Œé™ä½å¤§è¯­è¨€æ¨¡å‹ã€ç‰¹åˆ«æ˜¯ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨é—¨æ§›.

### 7.[Tigerbot](https://github.com/TigerResearch/TigerBot)
- [ ] ä¸ºä»€ä¹ˆtigerbotä½¿ç”¨100wæ•°æ®æŒ‡ä»¤å¾®è°ƒçš„ç»“æœæ¯”ä¸ä¸Švicunaä½¿ç”¨50kæ•°æ®å¾®è°ƒçš„ç»“æœï¼Œæ˜¯pretrainingæ¨¡å‹æœ‰é—®é¢˜è¿˜æ˜¯bloomåº•åº§å°±æ˜¯ä¸è¡Œå‘¢ï¼ˆç”¨basemodelæµ‹è¯•ä¸€ä¸‹ï¼‰
  
æ¨¡å‹å¯¹æ ‡instuctGPT-6B ç›®å‰7bæ¨¡å‹çš„æ•ˆæœä¸€èˆ¬ å¯èƒ½è®­ç»ƒè¿‡ç¨‹æœ‰å¾ˆå¤šè„æ•°æ® æ¯”è¾ƒæ˜æ˜¾çš„é—®é¢˜æ˜¯ä¼šè¾“å‡ºå„ç§å¤šä½™çš„ç¬¦å·

demoç½‘é¡µï¼šhttps://tigerbot.com/chat

#### æ¦‚è¿°

**å‘å¸ƒæœºæ„**ï¼šè™åšç§‘æŠ€
**å®¶æ—**ï¼šbloom  
å…³äºä¸ºä»€ä¹ˆä½¿ç”¨bloomåŸºåº§è€Œä¸æ˜¯æ›´ç«3çš„llamaï¼š1 bloomåŸºåº§å¯ä»¥å•†ç”¨ / 2 é¡¹ç›®å¼€å§‹æ—¶é—´æ—©äºllama
**æˆå‘˜**ï¼šTigerBot-7B, TigerBot-7B-baseï¼ŒTigerBot-180B (research version)
**åŠŸèƒ½**ï¼šæä¾›äº†ä¸‰ç§ APIï¼ŒåŒ…æ‹¬ Chat-APIï¼ŒPlug-insï¼Œ**Fine-Tunes**ã€‚
**æ•°æ®**ï¼šé¢„è®­ç»ƒ 100Gï¼Œä» 2TB è¿‡æ»¤åçš„æ•°æ®ä¸­ç»è¿‡å»å™ªå»é‡æ¸…æ´—è€Œå¾—ï¼›ç›‘ç£å¾®è°ƒ 1G æˆ– 100 ä¸‡æ¡æ•°æ®ï¼ŒæŒ‰æ¯”ä¾‹æ¶µç›–ç”¨æˆ·æŒ‡ä»¤å¸¸è§çš„ 10 å¤§ç±» 120 å°ç±»ä»»åŠ¡

**å®˜æ–¹ä»‹ç»:**
åœ¨ BLOOM åŸºç¡€ä¸Šï¼Œåœ¨æ¨¡å‹æ¶æ„å’Œç®—æ³•ä¸Šåšäº†å¦‚ä¸‹ä¼˜åŒ–ï¼š
- æŒ‡ä»¤å®Œæˆç›‘ç£å¾®è°ƒçš„åˆ›æ–°ç®—æ³•ä»¥è·å¾—æ›´å¥½çš„å¯å­¦ä¹ å‹(learnability)ï¼Œ
- è¿ç”¨ ensemble å’Œ probabilistic modeling çš„æ–¹æ³•å®ç°æ›´å¯æ§çš„äº‹å®æ€§(factuality)å’Œåˆ›é€ æ€§(generativeness)ï¼Œ
- åœ¨å¹¶è¡Œè®­ç»ƒä¸Šï¼Œçªç ´äº† deep-speed ç­‰ä¸»æµæ¡†æ¶ä¸­è‹¥å¹²å†…å­˜å’Œé€šä¿¡é—®é¢˜ï¼Œä½¿å¾—åœ¨åƒå¡ç¯å¢ƒä¸‹æ•°æœˆæ— é—´æ–­ï¼ˆè´Ÿè´£äººè¯´æ˜¯æ”¹äº†deepseedæºç å®ç°ï¼‰
- å¯¹ä¸­æ–‡è¯­è¨€çš„æ›´ä¸è§„åˆ™çš„åˆ†å¸ƒï¼Œä» tokenizer åˆ°è®­ç»ƒç®—æ³•ä¸Šåšäº†æ›´é€‚åˆçš„ç®—æ³•ä¼˜åŒ–ã€‚
é‡åŒ–ï¼šä½¿ç”¨GPTQç®—æ³•å’ŒGPTQ-for-LLaMaå®ç°é‡åŒ–ï¼š

**è®­ç»ƒç®—åŠ›**ï¼š
7bæ¨¡å‹  å•å¡a100 40Gå°±å¯ä»¥è®­ç»ƒ
180bæ¨¡å‹  
**æ¨ç†ç®—åŠ›**ï¼š
é‡åŒ–å‰ï¼š`tigerbot-7b-sft` æ¨ç†å¯åœ¨1å¼ RXT3090ä¸Šè¿›è¡Œï¼›`tigerbot-180b-sft` æ¨ç†å¯åœ¨5å¼ A100(80G)ä¸Šè¿›è¡Œ    
**é‡åŒ–å**ï¼š`tigerbot-7b-sft-4bit-128g` æ¨ç†å¯åœ¨ä¸€å¼ RTX3090ä¸Šè¿›è¡Œ `tigerbot-180b-research-4bit-128g` æ¨ç†å¯åœ¨ä¸¤å¼ A100(80G)ä¸Šè¿›è¡Œ

#### æ•°æ®

**é¢„è®­ç»ƒæ•°æ®**
åŸºäº GPT3 çš„ pretrain çš„æ•°æ®åˆ†å¸ƒï¼Œé‡‡é›†ä¸­æ–‡ä¹¦ç±ï¼Œäº’è”ç½‘ï¼Œå’Œç™¾ç§‘ç±»æ•°æ®ï¼Œå¹¶é€šè¿‡æ•°æ®æºè´¨é‡åˆ†è¿‡æ»¤å’Œ tf-idf soft dedupingï¼Œä» 20TB æ•°æ®è¿‡æ»¤åˆ° 2TBï¼Œä¿æŒè¯­è¨€å’Œç±»ç›®çš„æ¯”ä¾‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸ŠéšæœºæŠ½æ · 100G æ•°æ®å¼€æº
å¼€æºæ•°æ®é›†
- [ä¸­æ–‡å¼€æºé¢„è®­ç»ƒé›† - 55Gï¼ŒåŒ…å«ä¸­æ–‡ä¹¦ç±ã€ä¸­æ–‡äº’è”ç½‘ã€ä¸­æ–‡ç™¾ç§‘](https://huggingface.co/datasets/TigerResearch/pretrain_zh)
- [è‹±æ–‡å¼€æºé¢„è®­ç»ƒé›† - 51Gï¼ŒåŒ…å«è‹±æ–‡ä¹¦ç±ã€è‹±æ–‡äº’è”ç½‘ã€è‹±æ–‡ç™¾ç§‘](https://huggingface.co/datasets/TigerResearch/pretrain_en)
  
**å¾®è°ƒæ•°æ®**
æ•°æ®ä¸‹è½½é“¾æ¥å’Œæ¸…æ´—è§„åˆ™è§é¡¹ç›®github

å¼€æºæ•°æ®é›†
- æŒ‡ä»¤æ•°æ®é›†, å½“å‰å¼€æº 120W é—®ç­”å¯¹ï¼Œç£ç›˜ç©ºé—´ 1.1G ï¼ˆæ•°æ®é›†å¼€æ”¾åˆ° huggingfaceï¼‰
- é¢†åŸŸæ•°æ®å¼€æ”¾é‡‘èã€æ³•å¾‹ã€ç™¾ç§‘ç›¸å…³é¢†åŸŸæ•°æ®ï¼Œä½œä¸º rethink å¤–éƒ¨æ•°æ®
  

#### æ¨¡å‹ä¸‹è½½
| **Tigerbot\-7B**              | **Bits** | **memory\(GB\)** |
|-------------------------------|----------|------------------|
| tigerbot\-7b\-base            | 16       | 17\.2            |
| tigerbot\-7b\-sft             | 16       | 17\.2            |
| tigerbot\-7b\-sft\-4bit\-128g | 4        | 8\.5             |

| **Tigerbot\-180B\-Research**    | **Bits** | **memory\(GB\)** |
|---------------------------------|----------|------------------|
| tigerbot\-180b\-sft             | 16       | 347\.6           |
| tigerbot\-180b\-sft\-4bit\-128g | 4        | 108\.5           |

### 8. [Fastchat](https://github.com/lm-sys/FastChat)

FastChat æ˜¯ä¸€ä¸ªå¼€æ”¾å¹³å°ï¼Œç”¨äºè®­ç»ƒã€æœåŠ¡å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººã€‚æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

æœ€å…ˆè¿›æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒVicunaã€FastChat-T5ï¼‰çš„æƒé‡ã€è®­ç»ƒä»£ç å’Œè¯„ä¼°ä»£ç ã€‚
å…·æœ‰ Web UI å’Œä¸ OpenAI å…¼å®¹çš„ RESTful API çš„åˆ†å¸ƒå¼å¤šæ¨¡å‹æœåŠ¡ç³»ç»Ÿã€‚


### 9. Vicuna

ç”¨ ShareGPT æ”¶é›†çš„å¯¹è¯æ•°æ®å¾®è°ƒ LLaMAå®ç°   ç”¨GPT-4è¯„ä»·æ•ˆæœ
ä¸æ“…é•¿æ•°å­¦æ¨ç†ã€ç¼–ç ä»»åŠ¡ï¼Œè¯­è¨€é€»è¾‘ä¸Šæ•´ä½“æ˜¯è‹±æ–‡é€»è¾‘ï¼ˆæ¯”å¦‚ä¸­æ–‡ä»»åŠ¡ä¼šå¯¼è‡´çŠ¶è¯­åç½®&å¤¹æ‚è‹±æ–‡å•è¯ç­‰ï¼‰

![Alt text](./figure/image2.png)

ä½¿ç”¨openai [moderation](https://platform.openai.com/docs/guides/moderation/overview) apiæ¥è¿›è¡Œå®¡æ ¸ 

#### æ¦‚è¿°

**æ•°æ®**ï¼š70K å¯¹è¯å¯¹
**è®­ç»ƒ**ï¼šå¢å¼ºäº† Alpaca æä¾›çš„è®­ç»ƒè„šæœ¬ï¼Œä»¥æ›´å¥½åœ°å¤„ç†å¤šè½®å¯¹è¯å’Œé•¿åºåˆ—ã€‚ä¸€å¤©å†…åœ¨ 8 ä¸ª A100 GPU ä¸Šä½¿ç”¨ PyTorch FSDP å®Œæˆ
**ç®—åŠ›éœ€æ±‚**ï¼šè®­ç»ƒ ï¼š8 ä¸ª A100 GPU

å®˜æ–¹[blog](https://lmsys.org/blog/2023-03-30-vicuna/#how-good-is-vicuna)æä¾›äº†ä¸€ä¸ªå¯¹æ¯”è¡¨æ ¼ï¼š

 **Model Name**          | **LLaMA**                                | **Alpaca**                                           | **Vicuna**                                 | **Bard/ChatGPT** |
|-------------------------|------------------------------------------|------------------------------------------------------|--------------------------------------------|------------------|
| Dataset                 | Publicly available datasets\(1T token\) | Self\-instruct from davinci\-003 API\(52K samples\) | User\-shared conversations\(70K samples\) | N/A              |
| Training code           | N/A                                      | Available                                            | Available                                  | N/A              |
| Evaluation metrics      | Academic benchmark                       | Author evaluation                                    | GPT\-4 assessment                          | Mixed            |
| Training cost(7B)   | 82K GPU\-hours                           | \$ 500 (data) + $100 (training)                   | $140(training)                          | N/A              |
| Training cost (13B) | 135K GPU\-hours                          | N/A                                                  | $300 (training)                          | N/A              |

#### æ•°æ®
ä» ShareGPT.com ä½¿ç”¨å…¬å…± API æ”¶é›†çš„å¤§çº¦ 70K ç”¨æˆ·å…±äº«å¯¹è¯
ä¸ºäº†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå°† HTML è½¬æ¢å› markdown å¹¶è¿‡æ»¤æ‰ä¸€äº›ä¸åˆé€‚æˆ–ä½è´¨é‡çš„æ ·æœ¬ï¼Œå°†å†—é•¿çš„å¯¹è¯åˆ†æˆé€‚åˆæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„è¾ƒå°ç‰‡æ®µ


#### è®­ç»ƒè¿‡ç¨‹ 
è®­ç»ƒè„šæœ¬åœ¨ Stanfordâ€™s alpaca çš„åŸºç¡€ä¸Šåšäº†å¦‚ä¸‹æ”¹è¿›ï¼š
- å†…å­˜ä¼˜åŒ–ï¼šä¸ºäº†ä½¿ Vicuna èƒ½å¤Ÿç†è§£é•¿ä¸Šä¸‹æ–‡ï¼Œå°†æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä»ç¾Šé©¼ä¸­çš„ 512 æ‰©å±•åˆ° 2048ï¼Œè¿™å¤§å¤§å¢åŠ äº† GPU å†…å­˜éœ€æ±‚ã€‚é€šè¿‡åˆ©ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œé—ªå­˜æ³¨æ„åŠ›æ¥è§£å†³å†…å­˜å‹åŠ›ã€‚ 
- å¤šè½®å¯¹è¯ï¼šè°ƒæ•´è®­ç»ƒæŸå¤±ä»¥è€ƒè™‘å¤šè½®å¯¹è¯ï¼Œå¹¶ä»…æ ¹æ®èŠå¤©æœºå™¨äººçš„è¾“å‡ºè®¡ç®—å¾®è°ƒæŸå¤±ã€‚ 
- é€šè¿‡ Spot Instanceé™ä½æˆæœ¬ï¼š40 å€å¤§çš„æ•°æ®é›†å’Œ 4 å€çš„è®­ç»ƒåºåˆ—é•¿åº¦å¯¹è®­ç»ƒè´¹ç”¨æå‡ºäº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ vicunaä½¿ç”¨ **SkyPilot managed spot**æ¥é™ä½æˆæœ¬ï¼ˆåˆ©ç”¨æ›´ä¾¿å®œçš„ç‚¹å®ä¾‹ä»¥åŠè‡ªåŠ¨æ¢å¤æŠ¢å å’Œè‡ªåŠ¨åŒºåŸŸåˆ‡æ¢ï¼‰ã€‚ è¯¥è§£å†³æ–¹æ¡ˆå°† 7B æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä» 500 ç¾å…ƒå‰Šå‡è‡³ 140 ç¾å…ƒå·¦å³ï¼Œå°† 13B æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä» 1000 ç¾å…ƒå·¦å³å‰Šå‡è‡³ 300 ç¾å…ƒã€‚ ï¼ˆAlpaServe ï¼‰

#### æ¨¡å‹ 
åœ°å€: https://huggingface.co/lmsys

è¿˜æ˜¯deltaçš„æ ¼å¼å‘å¸ƒæƒé‡ 

ç±»ä¼¼è¿™æ ·è½¬æ¢ â†“
éœ€è¦èµ„æºï¼š30GB of CPU RAMï¼ˆ7bï¼‰ 60GB of CPU RAMï¼ˆ13bï¼‰

æä¾›äº† Low CPU Memory `Conversion æ–¹å¼ï¼š--low-cpu-mem 
```
python3 -m fastchat.model.apply_delta \
    --base-model-path /path/to/llama-7b \
    --target-model-path /path/to/output/vicuna-7b \
    --delta-path lmsys/vicuna-7b-delta-v1.1  
--low-cpu-mem`
```

### 10. [Koala: A Dialogue Model for Academic Research è€ƒæ‹‰å¯¹è¯æ¨¡å‹](https://bair.berkeley.edu/blog/2023/04/03/koala/)

![Alt text](./figure/image3.png)

### 11.[Fireflyï¼ˆæµè¤ï¼‰: ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹](https://github.com/yangjianxin1/Firefly)

**å®¶æ—** : bloom 
- å¼€æºQLoRAçš„è®­ç»ƒä»£ç ï¼Œä½¿ç”¨ä¸€å¼ æ˜¾å¡å¯¹bloom-7b1è¿›è¡Œå¾®è°ƒï¼Œå¼€æºfirefly-7b1-qlora-v0.1æ¨¡å‹ ã€‚
https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M
æœ¬æ•°æ®åº”ç”¨äºé¡¹ç›®ï¼šFireflyï¼ˆæµè¤ï¼‰: ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹ ï¼Œè®­ç»ƒåå¾—åˆ°çš„æ¨¡å‹[firefly-1b4](https://huggingface.co/YeungNLP/firefly-bloom-1b4-sft)

### 12. Stanford Alpaca: An Instruction-following LLaMA Model 

### 13. [GPT4all](https://github.com/nomic-ai/gpt4all)

### 14. [WizardLM: An Instruction-following LLM Using Evol-Instruct](https://github.com/nlpxucan/WizardLM)

### 15. [æ–‡å¿ƒåƒå¸†å¤§æ¨¡å‹å¹³å°](https://login.bce.baidu.com/?account=&redirect=http%3A%2F%2Fconsole.bce.baidu.com%2Fai%2F%3F_%3D1687657920924#/ai/wenxinworkshop/create/home)

## å¤šæ¨¡æ€ Model & å¤šæ¨¡æ€ä»»åŠ¡å®ç°æ–¹æ¡ˆ
### 1. [I-JEPA (the Image-based Joint-Embedding Predictive Architecture)](https://arxiv.org/pdf/2301.08243.pdf)
metaå¼€æº
é¦–ä¸ªä¸–ç•ŒèƒŒæ™¯çŸ¥è¯†å­¦ä¹ æ¨¡å‹ 
å›¾åƒè¡¥å…¨ä»»åŠ¡ https://github.com/facebookresearch/ijepa

### 2. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding
https://arxiv.org/abs/2306.02858

### 3. MM-React 

### 4. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models

### 5. Brain-inspired multimodal hybrid neural network for robot place recognition

  å¤§è„‘å¯å‘çš„å¤šæ¨¡æ€æ··åˆç¥ç»ç½‘ç»œï¼Œç”¨äºæœºå™¨äººä½ç½®è¯†åˆ« (ğŸç€å¯èƒ½ä»¥åæœ‰ç”¨)


## å¤§æ¨¡å‹æœ‰å…³é¡¹ç›®å®ç° å·¥å…·&å‚è€ƒé¡¹ç›®

### ä¼˜åŒ–æ–¹å‘
1. ä¼˜åŒ– text_split ç®—æ³•ï¼Œä½¿åŒ¹é…å‡ºçš„ç»“æœä½œä¸ºä¸Šä¸‹æ–‡æ—¶èƒ½å¤Ÿæä¾›æ›´åˆç†çš„æ¨ç†/å›ç­”ä¾æ®ï¼›
2. ä¼˜åŒ– embedding æ¨¡å‹ï¼Œæå‡è¯­ä¹‰å‘é‡åŒ–çš„æ•ˆæœï¼Œä½¿å¾—è¯­ä¹‰åŒ¹é…è¿‡ç¨‹ä¸­èƒ½å¤ŸåŒ¹é…å‡ºæœ€æ»¡è¶³è¦æ±‚çš„æ–‡æœ¬æ®µè½ä½œä¸ºä¸Šä¸‹æ–‡ï¼›
3. ä¼˜åŒ– LLM æ¨¡å‹ï¼Œä½¿å¾—ç»™å®šæé—®ç›¸åŒæƒ…å†µä¸‹ï¼Œå¾—åˆ°æ›´ç†æƒ³çš„æ¨ç†/å›ç­”ç»“æœã€‚

### ä¸€äº›æ€è€ƒ
* å¤§æ¨¡å‹çš„äººæ ¼ç‰¹å¾ï¼Œç©¶ç«Ÿè¦æ€ä¹ˆæ ·å»å¡‘é€ å¤§æ¨¡å‹çš„äººæ ¼ç‰¹å¾å‘¢
* æˆ‘ä»¬ç°åœ¨ç”¨loraç­‰æ–¹æ³•å»è¿›è¡ŒçŸ¥è¯†æ³¨å…¥ï¼Œåº”è¯¥å°±æ˜¯è®©å¤§æ¨¡å‹è®°ä½äº†ç­”æ¡ˆã€‚æˆ‘ä»¬é—®ï¼Œå¤§æ¨¡å‹ç­”ï¼Œå…¶å®æœ€ååªæ˜¯æå‡ºäº†é—®ç­”æœºå™¨ã€‚ä½†æ˜¯å¯¹é—®é¢˜çš„åº”è¯¥æœ‰æ€ä¹ˆæ ·çš„ç­”æ¡ˆåº”è¯¥æ˜¯åŸºäºå¤§æ¨¡å‹æˆé•¿è¿‡ç¨‹ä¸­çš„ç»å†çš„ï¼Œæ¯”å¦‚ä¸€ä¸ªå°å­©ç»å¸¸å—åˆ°é¼“åŠ±çš„è¯ä»–ä¼šæ›´ç§¯æä¸€äº›ï¼Œè¿™ä¸ªæ˜¯å¤–éƒ¨çš„åé¦ˆå†³å®šæ¨¡å‹çš„è®¤çŸ¥ï¼Œå’Œæˆ‘ä»¬ç°åœ¨è®­ç»ƒloraçš„æ–¹å¼å¥½åƒæ˜¯ç›¸åçš„ï¼Œæ‰€ä»¥loraæ˜¯ä¸æ˜¯åº”è¯¥æŒ‰ç…§RLHFå»åšï¼Ÿæ¯ä¸ªloraå½“åšä¸€ä¸ªäº‹ä»¶ï¼Œå¤§æ¨¡å‹ç»å†äº†è¿™ä¸ªäº‹ä»¶æ‰€ä»¥è·å¾—äº†ä»€ä¹ˆæ”¶è·åˆæ‹¥æœ‰äº†é‚£äº›æ–°çš„äººæ ¼ç‰¹è´¨ï¼Ÿï¼ˆåªæ˜¯ä¸€äº›æ€è€ƒï¼‰

### æ•°æ®å‡†å¤‡
- **åˆ†è¯å·¥å…·**
  1. [SentencePiece](https://github.com/google/sentencepiece)  
  åœ¨chinese-llama&aplaceé¡¹ç›®ä¸­ç”¨äºè¯è¡¨æ‰©å……
- **ç½‘ç«™çˆ¬è™«å·¥å…·** https://apify.com/apify/website-content-crawler
- **PDFç­‰æ–‡æœ¬è§£æç»„ä»¶**ï¼š
  1. PDFminer é¡¹ç›®åœ°å€ï¼šhttps://pdfminersix.readthedocs.io

      ï¼ˆæ¯”è¾ƒå¤è€æ•ˆç‡æ¯”è¾ƒä½ï¼Œä½†æ˜¯æœ‰å¾ˆå¤šç¤ºä¾‹ï¼Œæ¨èæŒ‡æ•°ï¼šğŸŒŸï¼‰

      PDFMineræ˜¯ä¸€ä¸ªPythonçš„PDFè§£æå™¨ï¼Œå¯ä»¥ä»PDFæ–‡æ¡£ä¸­æå–ä¿¡æ¯ã€‚ä¸å…¶ä»–PDFç›¸å…³çš„å·¥å…·ä¸åŒï¼Œå®ƒä¾§é‡çš„æ˜¯è·å–å’Œåˆ†ææ–‡æœ¬æ•°æ®ã€‚PDFMinerå…è®¸è·å–æŸä¸€é¡µä¸­æ–‡æœ¬çš„å‡†ç¡®ä½ç½®å’Œä¸€äº›è¯¸å¦‚å­—ä½“ã€è¡Œæ•°çš„ä¿¡æ¯ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªPDFè½¬æ¢å™¨ï¼Œå¯ä»¥æŠŠPDFæ–‡ä»¶è½¬æ¢æˆHTMLç­‰æ ¼å¼ã€‚è¿˜æœ‰ä¸€ä¸ªæ‰©å±•çš„PDFè§£æå™¨ï¼Œå¯ä»¥ç”¨äºé™¤æ–‡æœ¬åˆ†æä»¥å¤–çš„å…¶ä»–ç”¨é€”ã€‚

      PDFMinerå†…ç½®ä¸¤ä¸ªå·¥å…·ï¼špdf2txt.pyå’Œdumppdf.pyï¼š
      pdf2txt.pyä»PDFæ–‡ä»¶ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ã€‚ä½†ä¸èƒ½è¯†åˆ«ç”»æˆå›¾ç‰‡çš„æ–‡æœ¬ï¼Œè¿™éœ€è¦ç‰¹å¾è¯†åˆ«ã€‚
  
  2. pdfplumber é¡¹ç›®åœ°å€ï¼šhttps://github.com/jsvine/pdfplumber
  
      pdfplumberåº“æŒ‰é¡µå¤„ç† pdf ï¼Œè·å–é¡µé¢æ–‡å­—ï¼Œæå–è¡¨æ ¼ç­‰æ“ä½œã€‚
  
  3. pypdf2 é¡¹ç›®åœ°å€ï¼šhttps://pythonhosted.org/PyPDF2
  
      PyPDF2æ˜¯ä¸€ä¸ªçº¯Python PDFåº“ï¼Œå¯ä»¥è¯»å–æ–‡æ¡£ä¿¡æ¯ï¼ˆæ ‡é¢˜ï¼Œä½œè€…ç­‰ï¼‰ã€å†™å…¥ã€åˆ†å‰²ã€åˆå¹¶PDFæ–‡æ¡£ï¼Œå®ƒè¿˜å¯ä»¥å¯¹pdfæ–‡æ¡£è¿›è¡Œæ·»åŠ æ°´å°ã€åŠ å¯†è§£å¯†ç­‰ã€‚
  
  4. pymupdf é¡¹ç›®åœ°å€ï¼šhttps://pypi.org/project/PyMuPDF/
  
      ä½¿ç”¨PyMuPDFï¼Œå¯ä»¥è®¿é—®æ‰©å±•åä¸ºâ€œ.pdfâ€ã€â€œ.xpsâ€ã€â€œ.oxpsâ€ã€â€œ.cbzâ€ã€â€œ.fb2â€æˆ–â€œ.epubâ€ã€‚æ­¤å¤–ï¼Œå¤§çº¦10ç§æµè¡Œçš„å›¾åƒæ ¼å¼ä¹Ÿå¯ä»¥åƒæ–‡æ¡£ä¸€æ ·å¤„ç†:â€œ.pngâ€ï¼Œâ€œ.jpgâ€ï¼Œâ€œ.bmpâ€ï¼Œâ€œ.tiffâ€ç­‰ã€‚
  
  5. ppstructure é¡¹ç›®åœ°å€ï¼šhttps://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppstructure
  
      PP-StructureV2æ”¯æŒå¯¹å›¾ç‰‡/pdfå½¢å¼çš„æ–‡æ¡£è¿›è¡Œç‰ˆé¢åˆ†æï¼Œå¯ä»¥åˆ’åˆ†æ–‡å­—ã€æ ‡é¢˜ã€è¡¨æ ¼ã€å›¾ç‰‡ã€å…¬å¼ç­‰åŒºåŸŸï¼›æ”¯æŒé€šç”¨çš„ä¸­è‹±æ–‡è¡¨æ ¼æ£€æµ‹ä»»åŠ¡ï¼›æ”¯æŒè¡¨æ ¼åŒºåŸŸè¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œæœ€ç»ˆç»“æœè¾“å‡ºExcelæ–‡ä»¶ï¼›
      
      PP-Structureæ˜¯PaddleOCRå›¢é˜Ÿè‡ªç ”çš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ›´å¥½çš„å®Œæˆç‰ˆé¢åˆ†æã€è¡¨æ ¼è¯†åˆ«ç­‰æ–‡æ¡£ç†è§£ç›¸å…³ä»»åŠ¡ã€‚
      ç‰ˆé¢åˆ†æä»»åŠ¡ä¸­ï¼Œå›¾åƒé¦–å…ˆç»è¿‡ç‰ˆé¢åˆ†ææ¨¡å‹ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºæ–‡æœ¬ã€è¡¨æ ¼ã€å›¾åƒç­‰ä¸åŒåŒºåŸŸï¼Œéšåå¯¹è¿™äº›åŒºåŸŸåˆ†åˆ«è¿›è¡Œè¯†åˆ«ï¼Œå¦‚ï¼Œå°†è¡¨æ ¼åŒºåŸŸé€å…¥è¡¨æ ¼è¯†åˆ«æ¨¡å—è¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œå°†æ–‡æœ¬åŒºåŸŸé€å…¥OCRå¼•æ“è¿›è¡Œæ–‡å­—è¯†åˆ«ï¼Œæœ€åä½¿ç”¨ç‰ˆé¢æ¢å¤æ¨¡å—å°†å…¶æ¢å¤ä¸ºä¸åŸå§‹å›¾åƒå¸ƒå±€ä¸€è‡´çš„wordæˆ–è€…pdfæ ¼å¼çš„æ–‡ä»¶ã€‚


- **æ–‡æœ¬å‘é‡åŒ–ç»„ä»¶**
  1. text2vec åœ°å€ï¼šhttps://github.com/shibing624/text2vec
   
      å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚


  2. SGPT åœ°å€ï¼šhttps://arxiv.org/abs/2202.08904
   
      SGPTï¼šGPT Sentence Embeddings for Semantic Searchï¼Œæ˜¯ä¸€ä¸ªä½¿ç”¨GPTæ¶æ„ç”Ÿæˆembeddingçš„æ–¹æ³•ï¼Œä¸BERTæ¨¡å¼ä¸åŒã€‚
      
  3. M3E
   
        Moka Massive Mixed Embeddingçš„ç¼©å†™ï¼Œç”±MokaAIè®­ç»ƒï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniemï¼Œè¯„æµ‹BenchMarkä½¿ç”¨MTEB-zhï¼Œé€šè¿‡åƒä¸‡çº§ (2200w+) çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚


### æ•°æ®å¤„ç†
- Vectara  æ•°æ®ç­›é€‰å’ŒåŒ¹é…å·¥å…· github:https://github.com/vectara/vectara-answer
æ•ˆæœï¼š åŒ¹é…æ•ˆæœé’ˆå¯¹äºä¸­æ–‡ä¸å¤ªå¥½ï¼Œä½†æ˜¯è¯†åˆ«æ–‡å­—çš„èƒ½åŠ›ï¼Œç•Œé¢çš„è®¾ç½®ï¼Œæ–‡å­—çš„ç« èŠ‚è¯†åˆ«ç­‰å¾ˆå¥½ï¼Œæ•°æ®åº“å¼ç®¡ç†ï¼Œé€‚åˆæ‰¹é‡å¤„ç†æ–‡æ¡£
	![Alt text](./figure/image4.png)

- ç”¨pgvectoræ›¿æ¢Faissï¼š
  https://github.com/pgvector/pgvector

### æ¨¡å‹ä¸‹è½½&è½¬æ¢ï¼Œè®­ç»ƒ&å¾®è°ƒ
- Training Open Instruction-following Language Models https://github.com/allenai/open-instruct
- å„ç§æ¨¡å‹çš„ä½èµ„æºé‡åŒ–å’Œéƒ¨ç½² https://github.com/jianzhnie/Efficient-Tuning-LLMs
- Chinese-vicuna é¡¹ç›®æä¾›  Chinese-Vicuna/readme_zh.md at master Â· Facico/Chinese-Vicuna
- Chinese-Llama-Aplaceé¡¹ç›®æä¾›:
  
| **æ–¹å¼** | **é€‚ç”¨åœºæ™¯**                            | **æ•™ç¨‹** |
|--------|-------------------------------------|--------|
| åœ¨çº¿è½¬æ¢   | Colabç”¨æˆ·å¯åˆ©ç”¨æœ¬é¡¹ç›®æä¾›çš„notebookè¿›è¡Œåœ¨çº¿è½¬æ¢å¹¶é‡åŒ–æ¨¡å‹ | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E5%9C%A8%E7%BA%BF%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2)     |
| æ‰‹åŠ¨è½¬æ¢   | ç¦»çº¿æ–¹å¼è½¬æ¢ï¼Œç”Ÿæˆä¸åŒæ ¼å¼çš„æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œé‡åŒ–æˆ–è¿›ä¸€æ­¥ç²¾è°ƒ       | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2)     |
- [MeZO: Fine-Tuning Language Models with Just Forward Passes](https://security.feishu.cn/link/safety?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.17333.pdf&scene=ccm&logParams=%7B%22location%22%3A%22ccm_default%22%7D&lang=zh)
  å†…å­˜é«˜æ•ˆçš„é›¶é˜¶ä¼˜åŒ–å™¨ï¼Œå®ç°ä¸æ¨ç†é˜¶æ®µç›¸åŒçš„å†…å­˜å ç”¨å¤§å°
- Fairseq https://github.com/facebookresearch/fairseq
- Deepseed

### æ¨¡å‹é‡åŒ–æ¨ç†å’Œéƒ¨ç½²

- å¤§æ¨¡å‹é«˜æ•ˆæ¨ç†ä¸æœåŠ¡åº“vllm ï¼šæ¨ç†å†…å­˜å‹å¥½ï¼Œé«˜ååé‡ å¼•å…¥äº†PagedAttention
  
    fschat+vllm ä¾‹å­ ï¼šhttps://github.com/lm-sys/FastChat/blob/main/fastchat/serve/vllm_worker.py

    postï¼šhttps://vllm.ai/

    gitï¼šhttps://github.com/vllm-project/vllm

    æ–‡æ¡£ï¼šhttps://vllm.readthedocs.io/en/latest/getting_started/installation.html

- æ¨¡å‹é‡åŒ– https://github.com/qwopqwop200/GPTQ-for-LLaMa
- Chinese-Llama-Aplaceé¡¹ç›®æ•´ç†   å…·ä½“å†…å®¹è¯·å‚è€ƒæœ¬é¡¹ç›® >>> ğŸ“š https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%B8%8E%E9%83%A8%E7%BD%B2


| **æ¨ç†å’Œéƒ¨ç½²æ–¹å¼**             | **ç‰¹ç‚¹**                            | **å¹³å°** | **CPU** | **GPU** | **é‡åŒ–åŠ è½½** | **å›¾å½¢ç•Œé¢** | **æ•™ç¨‹** |
|-------------------------|-----------------------------------|--------|---------|---------|----------|----------|--------|
| [llama\.cpp](https://github.com/ggerganov/llama.cpp)              | ä¸°å¯Œçš„é‡åŒ–é€‰é¡¹å’Œé«˜æ•ˆæœ¬åœ°æ¨ç†                    | é€šç”¨     | âœ…       | âœ…       | âœ…        | âŒ        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2)     |
| ğŸ¤—[Transformers](https://github.com/huggingface/transformers)         | åŸç”Ÿtransformersæ¨ç†æ¥å£                | é€šç”¨     | âœ…       | âœ…       | âœ…        | âœ…        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8Transformers%E6%8E%A8%E7%90%86)     |
| [text\-generation\-webui](https://github.com/oobabooga/text-generation-webui) | å‰ç«¯Web UIç•Œé¢çš„éƒ¨ç½²æ–¹å¼                   | é€šç”¨     | âœ…       | âœ…       | âœ…        | âœ…        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8text-generation-webui%E6%90%AD%E5%BB%BA%E7%95%8C%E9%9D%A2)     |
| [LlamaChat](https://github.com/alexrozanski/LlamaChat)               | macOSä¸‹çš„å›¾å½¢äº¤äº’ç•Œé¢ï¼ˆéœ€æ­é…llama\.cppæ¨¡å‹ï¼‰    | MacOS  | âœ…       | âŒ       | âœ…        | âœ…        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8LlamaChat%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%EF%BC%88macOS%EF%BC%89)     |
| [LangChain](https://github.com/hwchase17/langchain)               | LLMåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œé€‚ç”¨äºè¿›è¡ŒäºŒæ¬¡å¼€å‘               | é€šç”¨     | âœ…â€       | âœ…       | âœ…â€        | âŒ        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%B8%8ELangChain%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90)     |
| [privateGPT](https://github.com/imartinez/privateGPT)              | åŸºäºLangChainçš„å¤šæ–‡æ¡£æœ¬åœ°é—®ç­”æ¡†æ¶             | é€šç”¨     | âœ…       | âœ…       | âœ…        | âŒ        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8privateGPT%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%96%87%E6%A1%A3%E9%97%AE%E7%AD%94)     |
| [Colab Gradio Dem](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb)       | åœ¨Colabä¸­å¯åŠ¨åŸºäºGradioçš„äº¤äº’å¼WebæœåŠ¡ï¼Œä½“éªŒæ¨¡å‹æ•ˆæœ | é€šç”¨     | âœ…       | âœ…       | âœ…        | âŒ        | [é“¾æ¥](https://colab.research.google.com/github/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb)     |


### æ¡†æ¶
- Langchain
- llama-index
- Emerging Architectures for LLM Applications ï¼ˆa16zï¼šLLMåº”ç”¨çš„æ–°å…´æ¶æ„ï¼‰ï¼šhttps://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/ 
  ![Alt text](./figure/image15.png)

  æä¾›äº†ä¸€ä¸ªLLMåº”ç”¨å¼€å‘å·¥å…·çš„è¡¨æ ¼

### å¤§æ¨¡å‹å¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·

#### TALM: tool augmented language models

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2205.12255

æ—¶é—´ï¼š2022.05

åŸºäºtransformerçš„è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡æå‡è§„æ¨¡æ¥æå‡å®ƒåœ¨å„ç§ä»»åŠ¡å½“ä¸­çš„è¡¨ç°ã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸€äº›éœ€è¦è®¿é—®ç‰¹å®šæ•°æ®ï¼ˆå¦‚ï¼šè®­ç»ƒæ—¶æ²¡æœ‰è§è¿‡çš„æ•°æ®ã€ç»å¸¸å‘ç”Ÿå˜åŒ–çš„æ•°æ®ã€éšç§æ•°æ®ç­‰ï¼‰çš„ä»»åŠ¡ï¼Œè¯­è¨€æ¨¡å‹å°±æ— æ³•å•çº¯çš„ä½¿ç”¨æå‡è§„æ¨¡çš„æ–¹å¼æ¥æå‡åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TALMï¼ŒåŸºäºæ–‡æœ¬çš„æ–¹å¼ï¼Œä½¿ç”¨å¤–éƒ¨å·¥å…·æ¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚

ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„T5ä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œä½¿ç”¨æ–‡æœ¬åˆ°æ–‡æœ¬çš„æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼š**é¦–å…ˆï¼Œè®­ç»ƒè¯­è¨€æ¨¡å‹æ ¹æ®é—®é¢˜è¾“å‡ºéœ€è¦ä½¿ç”¨çš„å·¥å…·å’Œå·¥å…·ç›¸å…³çš„å‚æ•°ã€‚ç„¶åï¼Œæ ¹æ®æ¨¡å‹çš„è¾“å‡ºç›¸åŒç›¸åº”çš„å¤–éƒ¨å·¥å…·å¹¶è¿”å›ç›¸å…³ç»“æœã€‚æœ€åï¼Œè®­ç»ƒæ¨¡å‹æ ¹æ®é—®é¢˜å’Œå¤–éƒ¨å·¥å…·è¿”å›çš„ç»“æœï¼Œè¾“å‡ºæœ€ç»ˆçš„ç­”æ¡ˆ**ã€‚

![Alt text](figure/image6.png)
![Alt text](figure/image5.png)


ä¸ºäº†èƒ½å¤Ÿè§£å†³æ ·ä¾‹ä¸è¶³çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†self-playæŠ€æœ¯ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†åˆDè®­ç»ƒå¾—åˆ°ä¸€ä¸ªTALMï¼Œç„¶åé’ˆå¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ªæ ·ä¾‹ï¼Œå°è¯•ä½¿ç”¨ä¸åŒçš„å·¥å…·æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœTALMèƒ½å¤Ÿæ­£ç¡®çš„è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‚£ä¹ˆå°±æŠŠè¿™æ¡æ•°æ®åŠå…¶ç›¸å…³çš„å·¥å…·åŠ å…¥åˆ°æ•°æ®é›†åˆDä¸­ï¼Œä¸æ–­å¯¹è®­ç»ƒé›†è¿›è¡Œæ‰©å……ï¼Œä»¥å¾—åˆ°ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ã€‚ 

![Alt text](figure/image7.png)

åœ¨å®éªŒæ–¹é¢ï¼Œä½œè€…ä½¿ç”¨Natural Questionå’ŒMathQAä¸¤ä¸ªæ•°æ®é›†å¯¹TALMè¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ä½¿ç”¨äº†å¤–éƒ¨å·¥å…·è¿›è¡Œå¢å¼ºçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½çš„å¤„ç†è¿™äº›é—®ç­”ç›¸å…³çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œä½œè€…è¿˜å¯¹æ¨¡å‹ä¸åŒå‚æ•°é‡çš„ç‰ˆæœ¬è¿›è¡Œäº†æµ‹è¯•ï¼Œå®éªŒå‘ç°éšç€å‚æ•°é‡çš„æå‡ï¼Œè¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½çš„ä½¿ç”¨å¤–éƒ¨å·¥å…·ã€‚

#### PAL: Program-aided Language Models

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2211.10435

å‘å¸ƒæ—¥æœŸï¼š2022.11

ä»£ç ï¼šhttp://reasonwithpal.com/

åœ¨æ¨ç†ä»»åŠ¡å½“ä¸­ï¼Œå°½ç®¡æ¨ç†é—®é¢˜èƒ½å¤Ÿè¢«æ­£ç¡®åœ°æ‹†è§£ï¼Œå¤§è¯­è¨€æ¨¡å‹ç»å¸¸ä¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é€»è¾‘é”™è¯¯æˆ–è€…è®¡ç®—é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•**PAL**ï¼Œå°†**è‡ªç„¶è¯­è¨€å½¢å¼çš„æ¨ç†é—®é¢˜ä½œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„è¾“å…¥ï¼Œå¹¶è¦æ±‚å¤§æ¨¡å‹ä»¥ä»£ç çš„å½¢å¼è¾“å‡ºç›¸å…³çš„æ¨ç†è¿‡ç¨‹ï¼Œæœ€åä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ï¼šç¼–è¯‘å™¨ã€æ‰§è¡Œå™¨ï¼‰æ¥æ‰§è¡Œä»£ç å¹¶å¾—åˆ°æœ€ç»ˆçš„ç­”æ¡ˆ**ã€‚

![Alt text](figure/image8.png)

#### Atlas: Few-shot Learning with Retrieval Augmented Language Models

å‘å¸ƒæ—¥æœŸï¼š2022.08
è®ºæ–‡ï¼šhttps://arxiv.org/abs/2208.03299

åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡å½“ä¸­ï¼Œä½¿ç”¨æ£€ç´¢å¢å¼ºçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ‹¥æœ‰å¾ˆå¥½çš„è¡¨ç°ï¼Œå¹¶ä¸”ä¸éœ€è¦å¾ˆå¤§çš„å‚æ•°é‡ã€‚ä½†æ˜¯ï¼Œå¯¹äºfew-shotåœºæ™¯ä¸‹ï¼Œæ£€ç´¢èƒ½å¦æœ‰æ•ˆå¢å¼ºè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¿˜ä¸å¾—è€ŒçŸ¥ã€‚åŸºäºè¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†Atlasï¼Œä¸€ä¸ªé¢„è®­ç»ƒçš„æ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ï¼Œåªéœ€è¦ä½¿ç”¨éå¸¸å°‘çš„è®­ç»ƒæ ·æœ¬å°±èƒ½å¤Ÿå­¦ä¹ å’Œå¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚

å…·ä½“æ¥è¯´ï¼Œä½œè€…ä½¿ç”¨çš„æ£€ç´¢æ¨¡å—æ˜¯Contrieverï¼Œä½¿ç”¨çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸ºT5ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†ADistï¼ŒEMDR^2ï¼ŒPDistå’ŒLOOPå››ä¸ªè®­ç»ƒç›®æ ‡å¯¹æ£€ç´¢å™¨å’Œè¯­è¨€æ¨¡å‹è¿›è¡Œè”åˆè®­ç»ƒã€‚åŒæ—¶ï¼Œä½œè€…ä¹Ÿä½¿ç”¨äº†æ— ç›‘ç£çš„æ–¹å¼è¿›è¡Œè”åˆè®­ç»ƒï¼ŒåŒ…æ‹¬Prefix language modelingï¼ŒMasked language modelingå’ŒTitle to section generationã€‚åœ¨å®éªŒæ–¹é¢ï¼Œä½¿ç”¨äº†KILTï¼ŒMMLUå’Œä¸€äº›å…¶ä»–çš„é—®ç­”æ•°æ®é›†å¯¹Atlasè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯æ˜äº†æ£€ç´¢å¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚

![Alt text](figure/image9.png)

#### Toolformer: Language Models Can Teach Themselves to Use Tools

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2302.04761
å‘å¸ƒæ—¥æœŸï¼š2022.02

åœ¨ä¼—å¤šä»»åŠ¡ä¸Šï¼Œå¤§æ¨¡å‹å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ä»…ä»…ä½¿ç”¨å¾ˆå°‘çš„æ ·ä¾‹æˆ–è€…æ–‡æœ¬å½¢å¼çš„æŒ‡ä»¤ï¼Œå¤§è¯­è¨€æ¨¡å‹å°±èƒ½å¤Ÿå¾ˆå¥½çš„å®Œæˆç›¸å…³ä»»åŠ¡ã€‚ä½†æ˜¯ï¼Œå¤§æ¨¡å‹åœ¨ä¸€äº›åœºæ™¯ä¸‹ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œéœ€è¦å¤–éƒ¨å·¥å…·æ¥è¾…åŠ©ä»¥æ›´å¥½çš„è§£å†³è¿™äº›ä»»åŠ¡ã€‚åœ¨è¿™ç¯‡å·¥ä½œä¸­ï¼Œ**ä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼Œè®©è¯­è¨€æ¨¡å‹è‡ªå·±æ•™è‡ªå·±å­¦ä¹ ä½¿ç”¨å¤–éƒ¨å·¥å…·**ã€‚

å…·ä½“æ¥è¯´ï¼Œ**ä½œè€…æ„é€ äº†ä¸€ä¸ªåŒ…å«å¤–éƒ¨å·¥å…·APIä½¿ç”¨æ–¹æ³•çš„æ•°æ®é›†**ï¼Œä½¿ç”¨è¿™ä¸ªæ•°æ®é›†å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¯æ¨¡å‹èƒ½å¤ŸæŒæ¡å·¥å…·çš„ä½¿ç”¨æ–¹æ³•ã€‚åœ¨æ•°æ®é›†çš„æ„é€ å½“ä¸­ï¼Œä½œè€…é‡‡æ ·äº†ä¸€äº›ä½ç½®åŠ å…¥äº†å¯¹å·¥å…·çš„APIè°ƒç”¨ï¼Œå¹¶æ¯”è¾ƒäº†æ’å…¥APIå’Œæœªæ’å…¥APIæ—¶äºŒè€…çš„æŸå¤±å‡½æ•°ï¼Œè¿‡æ»¤å‡ºäº†ä¸€äº›æ„é€ è¾ƒä¸ºåˆç†çš„æ•°æ®ï¼Œå°†è¿™äº›æ•°æ®è¿›è¡Œæ•´åˆè·å¾—äº†åŒ…å«APIè°ƒç”¨çš„æ•°æ®é›†ã€‚ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†ï¼Œå°±èƒ½å¤Ÿå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ä»–ä»¬èƒ½å¤ŸæŒæ¡å·¥å…·çš„ä½¿ç”¨æ–¹å¼ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æ¨ç†å½“ä¸­ï¼Œå¦‚æœæ¨¡å‹ç”Ÿæˆäº†ç›¸å…³çš„APIè°ƒç”¨ï¼Œå°±ä¼šåœæ­¢ç”Ÿæˆï¼Œå¹¶ä½¿ç”¨å¤–éƒ¨å·¥å…·æ ¹æ®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹è·å¾—ç»“æœè¿”å›ç»™æ¨¡å‹ï¼Œæ¨¡å‹åœ¨è¿›è¡Œåç»­çš„ç”Ÿæˆã€‚

åœ¨å®éªŒæ–¹é¢ï¼Œä½œè€…ä½¿ç”¨GPT-Jä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†baselineã€‚



![Alt text](figure/image10.png)


#### ART: Automatic multi-step reasoning and tool-use for large language models

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2303.09014
å‘å¸ƒæ—¥æœŸï¼š2022.05

å¤§è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨few-shotæˆ–è€…zero-shotè®¾ç½®ä¸‹é€šè¿‡ç”Ÿæˆæ¨ç†è¿‡ç¨‹æ¥è¿›è¡Œæ¨ç†ã€‚åŒæ—¶ï¼Œå¤–éƒ¨å·¥å…·å¯ä»¥ç”¨æ¥è¾…åŠ©æ¨¡å‹å¤„ç†ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨å‰äººçš„å·¥ä½œä¸­ï¼Œå¯¹äºä¸€ä¸ªæ–°çš„ä»»åŠ¡æˆ–è€…æ–°çš„å·¥å…·ï¼Œéœ€è¦æ‰‹åŠ¨æ„é€ ç›¸å…³çš„æ ·ä¾‹ç”¨äºæç¤ºå¤§æ¨¡å‹ã€‚ä½œè€…æå‡ºäº†ARTï¼Œ**å®ƒèƒ½å¤Ÿå†»ç»“å¤§æ¨¡å‹å¹¶è‡ªåŠ¨ä»¥ç¨‹åºçš„å½¢å¼ç”Ÿæˆä¸­é—´çš„æ¨ç†æ­¥éª¤**ã€‚

å…·ä½“æ¥è¯´ï¼ŒARTåˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š**promptæ„å»ºã€ç”Ÿæˆã€äººç±»åé¦ˆ**ã€‚å…¶ä¸­ï¼Œäººç±»åé¦ˆçš„æ­¥éª¤æ˜¯ä¸€ä¸ªå¯é€‰çš„æ­¥éª¤ã€‚åœ¨promptæ„å»ºçš„è¿‡ç¨‹ä¸­ï¼ŒARTä»ä»»åŠ¡åº“ä¸­æ£€ç´¢å‡ºç›¸ä¼¼çš„ä»»åŠ¡ä½œä¸ºæ ·ä¾‹ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¤§æ¨¡å‹ç”Ÿæˆç›¸å…³çš„ä»£ç ï¼ŒARTåœ¨å¤–éƒ¨å·¥å…·è¢«è°ƒç”¨çš„æ—¶å€™åœæ­¢å¤§æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶è°ƒç”¨å¤–éƒ¨å·¥å…·è¿”å›ç»“æœã€‚åœ¨äººç±»åé¦ˆè¿‡ç¨‹ä¸­ï¼Œäººç±»å¯ä»¥æ·»åŠ æ–°çš„è§£ç¦»åçš„æ ·ä¾‹åˆ°ä»»åŠ¡åº“å½“ä¸­ï¼Œæˆ–è€…ä¿®æ”¹å·¥å…·åº“ä¸­å·¥å…·çš„ä½¿ç”¨ã€‚
![Alt text](figure/image11.png)

#### ReAct: Synergizing Reasoning and Acting in Language Models

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2210.03629

ä»£ç ï¼šhttps://react-lm.github.io/

å‰äººçš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§è¯­è¨€æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ¨ç†èƒ½åŠ›å’Œæ‰§è¡Œè§„åˆ’èƒ½åŠ›ã€‚åœ¨è¿™ç¯‡å·¥ä½œä¸­ï¼Œä½œè€…æ¢ç©¶äº†ä½¿ç”¨æ¨¡å‹å»ç”Ÿæˆæ¨ç†è¿‡ç¨‹å’Œä»»åŠ¡ç‰¹å®šçš„è¡ŒåŠ¨ã€‚æ€»çš„æ¥è¯´ï¼Œæ¨ç†è¿‡ç¨‹æœ‰åŠ©äºæ¨¡å‹å½’çº³ã€è·Ÿè¸ªå’Œæ›´æ–°è¡ŒåŠ¨è®¡åˆ’ä»¥åŠå¤„ç†å¼‚å¸¸ï¼Œä»»åŠ¡ç‰¹å®šçš„è¡ŒåŠ¨èƒ½å¤Ÿæ˜¯æ¨¡å‹ä¸å¤–éƒ¨ç¯å¢ƒæˆ–å·¥å…·è¿›è¡Œå¯¹æ¥å¹¶æ”¶é›†é¢å¤–ä¿¡æ¯ã€‚

å…·ä½“æ¥è¯´ï¼Œå¯¹äºä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œåœ¨æŸä¸€æ­¥çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æ¥æ”¶åˆ°ä¸€ä¸ªæ¥è‡ªå¤–éƒ¨ç¯å¢ƒçš„ç»“æœï¼Œå¹¶æ ¹æ®å†å²æ¨ç†è¿‡ç¨‹ä¸­å¤–éƒ¨ç¯å¢ƒçš„ç»“æœå’Œè¡ŒåŠ¨è¿‡ç¨‹æ¥ç¡®å®šä¸‹ä¸€æ­¥éœ€è¦é‡‡å–çš„è¡ŒåŠ¨ã€‚åœ¨ReActä¸­ï¼Œå¯ä»¥æ‰§è¡Œçš„è¡ŒåŠ¨çš„èŒƒå›´ä¸ä»…ä»…æ˜¯ä¸å¤–éƒ¨ç¯å¢ƒçš„äº¤äº’ï¼ŒåŒæ—¶ä¹ŸåŒ…æ‹¬äº†è‡ªç„¶è¯­è¨€çš„æ¨ç†ï¼ˆå¯ä»¥æˆä¸ºthoughtæˆ–è€…reasoning traceï¼‰ã€‚è¯­è¨€æ¨¡å‹å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡å†³å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨ã€‚

åŒæ—¶ï¼Œä½œè€…è®¤ä¸ºReActæ–¹å¼æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜ç‚¹ï¼š
1. ReActæ‰€éœ€çš„promptéå¸¸ç›´è§‚ä¸”æ˜“äºè®¾è®¡ï¼›
2. ReActæ˜¯é€šç”¨å¹¶ä¸”çµæ´»çš„ï¼›
3. ReActèƒ½å¤Ÿç»™æ¨¡å‹å¸¦æ¥æ€§èƒ½æå‡å¹¶ä¸”å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ï¼›4. ReActæ˜¯èƒ½å¤Ÿä¸å¯¹é½äººç±»åå¥½å¹¶ä¸”æ˜¯å¯æ§çš„ã€‚åœ¨å®éªŒæ–¹é¢ï¼Œåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡HotpotQAå’ŒFEVERä¸Šçš„æµ‹è¯•éƒ½è¾¨æ˜äº†ReActçš„æœ‰æ•ˆæ€§ã€‚
![Alt text](figure/image12.png)


#### CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2305.11738

å‘å¸ƒæ—¥æœŸï¼š2022.05

ä»£ç ï¼šhttps://github.com/microsoft/ProphetNet/tree/master/CRITIC

å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½å‡ºç°ä¸ä¸€è‡´æˆ–è€…æœ‰é—®é¢˜çš„è¡Œä¸ºï¼Œä¾‹å¦‚å¹»è±¡ã€ç”Ÿæˆå­˜åœ¨ç¼ºé™·çš„ä»£ç ã€ç”Ÿæˆæœ‰å®³çš„å†…å®¹ç­‰ã€‚ä½œè€…æ¨¡ä»¿äººç±»ä½¿ç”¨å·¥å…·éªŒè¯çš„è¡Œä¸ºï¼Œæå‡ºäº†CRITICã€‚è¯­è¨€æ¨¡å‹æ ¹æ®é—®é¢˜ç”Ÿæˆå‡ºç›¸å…³ç­”æ¡ˆä¹‹åï¼Œå¯ä»¥å’Œå¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ï¼Œä½¿ç”¨é€‚å½“çš„å·¥å…·å¯¹ç”Ÿæˆçš„å†…å®¹è¿›è¡Œè¯„ä¼°ï¼Œæ ¹æ®å¾—åˆ°çš„åé¦ˆå¯¹ä¹‹å‰ç”Ÿæˆçš„å†…å®¹è¿›è¡Œä¿®æ”¹ã€‚

å…·ä½“æ¥è¯´ï¼Œå¤§è¯­è¨€æ¨¡å‹è¢«ä½œä¸ºä¸€ä¸ªé»‘ç®±ï¼Œæ ¹æ®ä»»åŠ¡ç›¸å…³çš„è¾“å…¥å¾—åˆ°ä¸€ä¸ªåŸå§‹è¾“å‡ºã€‚ç„¶åï¼Œå’Œå¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±ï¼Œä»£ç è§£é‡Šå™¨ã€æœç´¢å¼•æ“ç­‰ã€‚é€šè¿‡å’Œå¤–éƒ¨å·¥å…·çš„äº¤äº’ï¼Œèƒ½å¤Ÿè·å¾—ä¸€ç³»åˆ—çš„ç»“æœï¼Œå¤§æ¨¡å‹æ ¹æ®è¿™äº›ç»“æœç”Ÿæˆç›¸å…³çš„åé¦ˆã€‚æœ€åï¼Œæ ¹æ®ä»»åŠ¡ç›¸å…³çš„è¾“å…¥ã€åŸå§‹è¾“å‡ºã€ä¸å·¥å…·äº¤äº’çš„ç»“æœç­‰å¤šæ–¹é¢çš„å†…å®¹ï¼Œå¤§æ¨¡å‹å¯¹åŸå§‹è¾“å‡ºè¿›è¡Œä¿®æ”¹ï¼Œç­‰åˆ°æ–°çš„è¾“å‡ºã€‚æ”¹è¿‡ç¨‹å¯ä»¥è¿­ä»£è¿›è¡Œï¼Œå¤šæ¬¡ä¿®æ­£è¾“å‡ºçš„å†…å®¹ã€‚åœ¨å®éªŒæ–¹é¢ï¼Œä½œè€…ä½¿ç”¨AmbigNQã€TriviaQAå’ŒHotpotQAä¸‰ä¸ªé—®ç­”æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼ŒCRITICèƒ½å¤Ÿè¶…è¶Šæ™®é€šCoTå’ŒReActç­‰æ–¹æ³•ï¼Œå–å¾—äº†è‰¯å¥½çš„è¡¨ç°ã€‚
![Alt text](figure/image13.png)

#### Making Language Models Better Tool Learners with Execution Feedback

è®ºæ–‡ï¼šhttps://arxiv.org/abs/2305.13068

ä»£ç ï¼šhttps://github.com/zjunlp/trice

å‰äººçš„å·¥ä½œè¡¨æ˜äº†AIç³»ç»Ÿå¯ä»¥åˆ©ç”¨å·¥å…·å¢å¼ºè‡ªå·±çš„èƒ½åŠ›å¹¶å’Œå¤–ç•Œè¿›è¡Œäº¤äº’ã€‚ä½†æ˜¯å¦‚ä½•å¼•å¯¼æ¨¡å‹æ­£ç¡®ä½¿ç”¨å·¥å…·ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªéœ€è¦æ¢ç©¶çš„é—®é¢˜ã€‚åœ¨è¿™ç¯‡å·¥ä½œä¸­ï¼Œä½œè€…æå‡ºäº†TRICEã€‚è¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿è¯­è¨€æ¨¡å‹é€šè¿‡å·¥å…·æ‰§è¡Œçš„ç»“æœçš„åé¦ˆè¿›è¡ŒæŒç»­åœ°å­¦ä¹ ï¼Œå¯ä»¥è®©æ¨¡å‹é«˜æ•ˆåœ°å­¦ä¹ ä½•æ—¶ä¸å¦‚ä½•ä½¿ç”¨å·¥å…·ã€‚

å…·ä½“æ¥è¯´ï¼Œè®­ç»ƒè¿‡ç¨‹çš„ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«ä¸ºï¼šBehavior Cloningï¼ŒRLEFã€‚åœ¨Behavior Cloningé˜¶æ®µï¼Œæ¨¡å‹æ ¹æ®ä»»åŠ¡è¾“å…¥ï¼Œè¾“å‡ºç›¸å…³çš„å·¥å…·çš„ä½¿ç”¨æ–¹æ¡ˆã€‚åœ¨RLEFé˜¶æ®µï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ ¹æ®å·¥å…·çš„ç»“æœè®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç»§ç»­å¢å¼ºæ¨¡å‹ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚åœ¨å®éªŒæ–¹é¢ï¼Œä½œè€…ä½¿ç”¨Alpaca-7Bä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œæ¯”è¾ƒäº†Toolformerç­‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨æ•°å­¦ç›¸å…³ä»»åŠ¡ä¸Šè¿›è¡Œè¯„æµ‹ï¼Œè¯æ˜äº†TRICEçš„æœ‰æ•ˆæ€§ã€‚

![Alt text](figure/image14.png)

å‚è€ƒèµ„æ–™ï¼šhttps://mp.weixin.qq.com/s/3CW6OrU5zwDktr9UIbcoGg

### äººå·¥æ™ºèƒ½ä½“ AI Agent

è®¿è°ˆ[Transformerä½œè€…ï¼šæŒ‡ä»¤å‹æ™ºèƒ½ä½“çš„æ„å»ºä¹‹æ³•](https://hub.baai.ac.cn/view/27802)
**æ¦‚å¿µ**ï¼šæ˜¯ä¸€ç§è‡ªåŠ¨æ™ºèƒ½ä½“ï¼Œä»¥æœ€ç®€å•çš„å½¢å¼ä¸­åœ¨å¾ªç¯ä¸­è¿è¡Œï¼Œæ¯æ¬¡è¿­ä»£æ—¶ï¼Œç”Ÿæˆè‡ªæˆ‘å¯¼å‘çš„æŒ‡ä»¤å’Œæ“ä½œï¼Œä¸ä¾èµ–äººç±»æ¥æŒ‡å¯¼å¯¹è¯ï¼Œå¹¶ä¸”æ˜¯é«˜åº¦å¯æ‰©å±•çš„ã€‚

#### é¡¹ç›®1ï¼š[æ–¯å¦ç¦ã€è°·æ­Œã€Œè¥¿éƒ¨ä¸–ç•Œ](https://arxiv.org/abs/2304.03442)

å®ƒç”¨ä¸‰ä¸ªé‡è¦çš„æ¶æ„åŸºæœ¬è¦ç´ â€”â€”è®°å¿†ã€åæ€å’Œè§„åˆ’ï¼Œå°†ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹åšäº†æ‰©å±•ã€‚

    1ï¼‰è®°å¿†å’Œæ£€ç´¢
    è®°å¿†æµåŒ…å«æ¯ä¸ªæ™ºèƒ½ä½“çš„è§‚å¯Ÿåˆ—è¡¨ï¼Œå…¶ä¸­ï¼Œæ¯ä¸ªè§‚å¯Ÿéƒ½æœ‰è‡ªå·±çš„æ—¶é—´æˆ³ã€‚
    è§‚å¯Ÿå¯ä»¥æ˜¯æ™ºèƒ½ä½“æ‰§è¡Œçš„è¡Œä¸ºï¼Œä¹Ÿå¯ä»¥æ˜¯æ™ºèƒ½ä½“ä»å…¶ä»–äººé‚£é‡Œæ„ŸçŸ¥åˆ°çš„è¡Œä¸ºã€‚è®°å¿†æµå¾ˆé•¿ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰è§‚å¯Ÿéƒ½æ˜¯é‡è¦çš„ã€‚
    ä¸ºäº†æ£€ç´¢æœ€é‡è¦çš„è®°å¿†ä»¥ä¼ é€’ç»™è¯­è¨€æ¨¡å‹ï¼Œæœ‰ä¸‰ä¸ªå› ç´ éœ€è¦è€ƒè™‘ï¼š
    1. æœ€è¿‘æ€§ï¼šè¿‘æœŸçš„è®°å¿†æ›´é‡è¦ã€‚
    2. é‡è¦æ€§ï¼šæ™ºèƒ½ä½“è®¤ä¸ºé‡è¦çš„è®°å¿†ã€‚ä¾‹å¦‚ï¼Œä¸æŸäººåˆ†æ‰‹æ¯”åƒæ—©é¤æ›´é‡è¦ã€‚
    3. ç›¸å…³æ€§ï¼šä¸æƒ…å¢ƒç›¸å…³çš„è®°å¿†ï¼Œå³æŸ¥è¯¢è®°å¿†ã€‚ä¾‹å¦‚ï¼Œåœ¨è®¨è®ºå¦‚ä½•ä¸ºåŒ–å­¦è€ƒè¯•å­¦ä¹ æ—¶ï¼Œå­¦æ ¡ä½œä¸šè®°å¿†æ›´é‡è¦ã€‚
      
    2ï¼‰åæ€
    åæ€æ˜¯ä¸€ç§é«˜å±‚æ¬¡çš„æŠ½è±¡æ€è€ƒï¼Œå¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“è¿›è¡Œæ¦‚æ‹¬å’Œæ¨ç†ã€‚
    åæ€ä¼šå®šæœŸäº§ç”Ÿä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜ï¼šã€Œå…³äºé™ˆè¿°ä¸­çš„ä¸»é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å›ç­”å“ª3ä¸ªæœ€çªå‡ºçš„é«˜å±‚æ¬¡é—®é¢˜ï¼Ÿï¼Œä½ èƒ½ä»ä¸Šè¿°é™ˆè¿°ä¸­æ¨æ–­å‡ºå“ª5ä¸ªé«˜å±‚æ¬¡çš„è§è§£ï¼Ÿã€

    3ï¼‰è§„åˆ’
    è§„åˆ’å¾ˆé‡è¦ï¼Œå› ä¸ºè¡ŒåŠ¨ä¸ä»…åº”è¯¥é›†ä¸­åœ¨å½“ä¸‹ï¼Œè€Œä¸”åº”è¯¥é›†ä¸­åœ¨æ›´é•¿çš„æ—¶é—´èŒƒå›´å†…ï¼Œè¿™æ ·ï¼Œè¡ŒåŠ¨æ‰èƒ½å¤Ÿè¿è´¯å’Œå¯ä¿¡ã€‚
    è§„åˆ’åŒæ ·å­˜å‚¨åœ¨è®°å¿†æµä¸­ã€‚æ™ºèƒ½ä½“å¯ä»¥æ ¹æ®è§„åˆ’åˆ›å»ºè¡ŒåŠ¨ï¼Œå¹¶æ ¹æ®è®°å¿†æµä¸­çš„å…¶ä»–è§‚å¯Ÿç»“æœåšå‡ºååº”å’Œæ›´æ–°è®¡åˆ’ã€‚

#### é¡¹ç›®2ï¼š[Camel](https://arxiv.org/abs/2303.17760 )

Camelä»¥ã€Œè§’è‰²æ‰®æ¼”ã€è€Œé—»åã€‚
ä½œä¸ºä¸€ä¸ªæ¢ç´¢å¤§è¯­è¨€æ¨¡å‹ç¤¾ä¼šã€Œå¿ƒæ™ºã€çš„äº¤æµæ™ºèƒ½ä½“ï¼Œå®ƒæå‡ºäº†ä¸€ä¸ªè§’è‰²æ‰®æ¼”æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå¯ä»¥å®ç°ä¸¤ä¸ªäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“çš„äº¤æµï¼š
1ï¼‰AIç”¨æˆ·æ™ºèƒ½ä½“ï¼šå‘AIåŠ©æ‰‹æä¾›æŒ‡ä»¤ï¼Œç›®æ ‡æ˜¯å®Œæˆä»»åŠ¡
2ï¼‰AIåŠ©æ‰‹æ™ºèƒ½ä½“ï¼šéµå¾ªAIç”¨æˆ·çš„æŒ‡ä»¤ï¼Œå¹¶ä»¥è§£å†³ä»»åŠ¡çš„æ–¹æ³•è¿›è¡Œå›åº”
3ï¼‰ä»»åŠ¡æŒ‡å®šæ™ºèƒ½ä½“ï¼šè¿™ä¸ªæ™ºèƒ½ä½“çš„ä½œç”¨ï¼Œæ˜¯ä¸ºAIç”¨æˆ·å’ŒAIåŠ©æ‰‹æ„æ€ä¸€ä¸ªå…·ä½“çš„ä»»åŠ¡ã€‚è¿™æ ·ï¼Œå®ƒå°±å¯ä»¥è‡ªä¸»ç¼–å†™ä¸€ä¸ªå…·ä½“çš„ä»»åŠ¡æç¤ºï¼Œè€Œä¸ç”¨ç”¨æˆ·å»èŠ±æ—¶é—´å®šä¹‰äº†ã€‚

ä¸€ç›´å¾ªç¯ä¸‹å»ç›´åˆ°è¾¾æˆç»“æŸæ¡ä»¶

å¯ä»¥ç”¨langchainå®ç°â†“

    åœ¨LangChainçš„å®ç°ä¸­ï¼Œç”¨çš„æ˜¯Camelè®ºæ–‡ä¸­ç»™å‡ºçš„æç¤ºï¼Œå¹¶å®šä¹‰äº†ä¸‰ä¸ªæ™ºèƒ½ä½“ï¼š
    1ï¼‰task_specify_agentï¼ˆä»»åŠ¡æŒ‡å®šæ™ºèƒ½ä½“ï¼‰
    2ï¼‰assistant_agentï¼ˆåŠ©æ‰‹æ™ºèƒ½ä½“ï¼‰
    3ï¼‰user_agentï¼ˆç”¨æˆ·æ™ºèƒ½ä½“ï¼‰ã€‚
    ç„¶åï¼Œä½¿ç”¨ä¸€ä¸ªwhileå¾ªç¯æ¥å¾ªç¯è¿›è¡ŒåŠ©æ‰‹æ™ºèƒ½ä½“å’Œç”¨æˆ·æ™ºèƒ½ä½“ä¹‹é—´çš„å¯¹è¯ï¼š

ç›®å‰è¿˜æ²¡æœ‰è°ƒç”¨å¤–éƒ¨å·¥å…·

#### é¡¹ç›®3ï¼š[BabyAGI](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)

BabyAGIçš„å…³é”®ç‰¹ç‚¹æ˜¯åªæœ‰ä¸‰ä¸ªæ™ºèƒ½ä½“ï¼šä»»åŠ¡æ‰§è¡Œæ™ºèƒ½ä½“ï¼ˆTask Execution Agentï¼‰ã€ä»»åŠ¡åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆTask Creation Agentï¼‰å’Œä»»åŠ¡ä¼˜å…ˆçº§æ™ºèƒ½ä½“ï¼ˆTask Prioritization Agentï¼‰ã€‚

    æ­¥éª¤ï¼š

    1ï¼‰ä»»åŠ¡æ‰§è¡Œæ™ºèƒ½ä½“æŒ‰é¡ºåºå®Œæˆåˆ—è¡¨ä¸­çš„ä»»åŠ¡

    2ï¼‰ä»»åŠ¡åˆ›å»ºæ™ºèƒ½ä½“æ ¹æ®å…ˆå‰ä»»åŠ¡çš„ç›®æ ‡å’Œç»“æœåˆ›å»ºæ–°ä»»åŠ¡

    3ï¼‰ä»»åŠ¡ä¼˜å…ˆçº§æ™ºèƒ½ä½“å¯¹ä»»åŠ¡è¿›è¡Œé‡æ–°æ’åº

    ç„¶åï¼Œè¿™ä¸ªç®€å•çš„è¿‡ç¨‹å°†ä¼šä¸æ–­åœ°é‡å¤

BabyAGI + LangChain â†“
 
    åœ¨LangChainæ¡†æ¶ä¸­ï¼Œè¿è¡ŒBabyAGIéå¸¸ç®€å•ã€‚

    é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªBabyAGIæ§åˆ¶å™¨ï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªé“¾ï¼š

    1ï¼‰ä»»åŠ¡åˆ›å»ºé“¾ï¼ˆTaskCreationChainï¼‰

    2ï¼‰ä»»åŠ¡ä¼˜å…ˆçº§é“¾ï¼ˆTaskPrioritizationChainï¼‰

    3ï¼‰æ‰§è¡Œé“¾ï¼ˆExecutionChainï¼‰

    ç„¶åï¼Œåœ¨ä¸€ä¸ªï¼ˆæ½œåœ¨çš„ï¼‰æ— é™å¾ªç¯ä¸­è¿è¡Œå®ƒä»¬ã€‚

    é€šè¿‡Langchainï¼Œå¯ä»¥å®šä¹‰æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿™æ ·å®ƒå°±ä¸ä¼šæ— é™è¿è¡Œå¹¶æ¶ˆè€—æ‰æ‰€æœ‰çš„OpenAI APIé¢åº¦ã€‚


#### é¡¹ç›®4ï¼š[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)

### æ•ˆæœè¯„ä¼°
* [Evaluating instruction following on more user-oriented data](https://github.com/tatsu-lab/alpaca_farm/) :AlpacaFarmæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ²™ç›’ï¼Œèƒ½å¤Ÿå¿«é€Ÿã€å»‰ä»·åœ°å¯¹ä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œå®éªŒã€‚å®ƒç”¨API LLMsæ¨¡æ‹Ÿäººç±»åé¦ˆï¼Œæä¾›ä¸€ä¸ªç»è¿‡éªŒè¯çš„è¯„ä¼°åè®®ï¼Œå¹¶æä¾›ä¸€å¥—å‚è€ƒæ–¹æ³•çš„å®ç°ã€‚ç ”ç©¶äººå‘˜å¯ä»¥å¿«é€Ÿè¿­ä»£æ¨¡å‹å¼€å‘ï¼Œå¹¶å°†ä»–ä»¬çš„æ–¹æ³•è½¬ç§»åˆ°äººç±»æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜æ€§èƒ½ã€‚
* Evaluating instruction following on more user-oriented data
* C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models  

### å¸¸è§bugè§£å†³
åœ¨python=3.9ç¯å¢ƒä¸­è¿è¡Œlangchianæ—¶å€™é‡åˆ°TypeError: issubclass() arg 1 must be a classç±»å‹çš„é”™è¯¯ï¼š

`pip install typing-inspect==0.8.0 typing_extensions==4.5.0`


## å‚è€ƒèµ„æ–™

### Paper&Post
- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)
- [Transformer models: an introduction and catalogâ€Šâ€”â€Š2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/)
- [LLM Learning Lab](https://lightning.ai/pages/llm-learning-lab/)
- [é«˜è´¨é‡çš„llmå¼€å‘æ•™ç¨‹](https://fullstackdeeplearning.com/llm-bootcamp/)
- [LLMå¤§æ¨¡å‹ä½èµ„æºå¾®è°ƒp tuning v2å’ŒloraåŒºåˆ«](https://zhuanlan.zhihu.com/p/622810394)
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)
- [Inside language models](https://lifearchitect.ai/models/)
- [retrieval-based-NLP](http://ai.stanford.edu/blog/retrieval-based-NLP/)
- [ACL2023tutorial çŸ¥è¯†æ£€ç´¢å¢å¼ºå‹å¤§æ¨¡å‹å‰è¨€æ•™ç¨‹-ä¸­æ–‡ç†è§£åŠ å„ç§è¡¥å……(è‡ªå·±æ‰£çš„å¯èƒ½æœ‰äº›è§‚ç‚¹ä¸æ˜¯å¾ˆç²¾ç¡®)](./acl2023-retrieval-lm.md#)

### å·¥å…·ç½‘å€

- https://civitai.com/
- æä¾›éœ€æ±‚ç”Ÿæˆä»£ç ä»“åº“ <https://github.com/AntonOsika/gpt-engineer>
- å¼€æºllm openaiæ ¼å¼æ¥å£è°ƒç”¨ <https://github.com/xusenlinzy/api-for-open-llm/tree/master>
- fachat ä¹Ÿç”¨äºæ¥å£è°ƒç”¨ï¼š<https://github.com/lm-sys/FastChat>
### API tools
#### æœç´¢ç±»ï¼š
* https://newsapi.org
* https://serpapi.com
* https://www.themoviedb.org/documentation/api
#### è‡ªåŠ¨åŒ–å·¥ä½œç±»ï¼š
* https://mailchimp.com è‡ªåŠ¨å‘é€é‚®ä»¶å’Œå¹¿å‘Š
* https://randomuser.me éšæœºåˆ›å»ºç”¨æˆ·æ•°æ®
### é¡¹ç›®æ•´åˆ
- YuLan-RETA-LLMï¼š<https://github.com/RUC-GSAI/YuLan-IR> 
  
  æ£€ç´¢å¢å¼ºçš„LLMæµç¨‹
  
### ä¸€äº›å€¼å¾—å…³æ³¨çš„issues
- **æ¨¡å‹ä¸ªæ€§å’Œç‰¹å®šçš„promptçš„é—®é¢˜ï¼š**
  
  åœ¨Aiden Gomezçš„[è®¿è°ˆ](https://www.youtube.com/watch?v=zBK2CPka5jo)ä¸­æåˆ°ï¼šâ€œæˆ‘å¸Œæœ›æœªæ¥ä¸å†éœ€è¦æç¤ºã€‚æˆ‘å¾ˆå–œæ¬¢æ—©æœŸæœ‰å…³è¯­è¨€å¤§æ¨¡å‹çš„ç±»æ¯”ï¼Œå½“æ—¶è¯­è¨€å¤§æ¨¡å‹å°±å¥½æ¯”æ˜¯ä¸€ç§å¤–æ˜ŸæŠ€æœ¯ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¹ å¦‚ä½•ä¸ä¹‹äº¤è°ˆã€‚ç„¶è€Œï¼Œè¦åšåˆ°è¿™ä¸€ç‚¹éå¸¸å›°éš¾ï¼Œä½ éœ€è¦å¯¹è¯­è¨€å¤§æ¨¡å‹ä»ç½‘ä¸Šå­¦åˆ°çš„è¯­è¨€åšé€†å‘å·¥ç¨‹ï¼Œè¿™éœ€è¦è€—è´¹å¤§é‡å¿ƒè¡€ï¼Œéšç€æ¨¡å‹è¶Šæ¥è¶Šå¼ºå¤§ï¼Œå¾®è°ƒè¶Šæ¥è¶Šç²¾ç»†ï¼Œæˆ‘å¸Œæœ›æŒæ¡å’Œäº†è§£ç‰¹å®šè¯­è¨€æ¨¡å‹ä¸å†æ˜¯ä¸€ç§è´Ÿæ‹…æˆ–ä¼˜åŠ¿ã€‚ä¸åŒçš„æ¨¡å‹æœ‰ä¸åŒçš„ä¸ªæ€§ï¼Œâ€œä¸ªæ€§â€æ˜¯æŒ‡å¦‚æœæƒ³è¦æ¨¡å‹æŒ‰ç…§ä½ çš„æ„å›¾è¡Œäº‹ï¼Œé‚£ä¹ˆå°±å¿…é¡»è¦å­¦ä¼šä¸æ¨¡å‹äº¤æµã€‚ç”±äºè®­ç»ƒæ•°æ®é›†å­˜åœ¨å·®å¼‚ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å­¦ä¹ ä¸åŒçš„è¯­è¨€ï¼Œè¿™æ„å‘³ç€æ¨¡å‹çš„æ¯æ¬¡æ›´æ–°ï¼Œéƒ½å¿…é¡»é‡æ–°è°ƒæ•´ä¸æ¨¡å‹å¯¹è¯çš„å†…éƒ¨è¡¨ç¤ºã€‚å› æ­¤ï¼ŒåŒ…æ‹¬æˆ‘åœ¨å†…çš„è¡Œä¸šé‡Œå¤§å¤šæ•°è¯­è¨€å¤§æ¨¡å‹å»ºé€ è€…å¸Œæœ›èƒ½å¤Ÿé€æ¸å‡å°‘å¹¶å…‹æœè¿™ä¸€é—®é¢˜ã€‚â€
- Retrieval-Augmented Generation æ£€ç´¢å¢å¼ºæŠ€æœ¯ 
RAGâ€”â€”ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆæ„å»ºç‰¹å®šè¡Œä¸šçš„å¤§å‹è¯­è¨€æ¨¡å‹
- [The AI Brick Wall â€“ A Practical Limit For Scaling Dense Transformer Models, and How GPT 4 Will Break Past It](https://www.semianalysis.com/p/the-ai-brick-wall-a-practical-limit)

- æ˜¯å¦è¦è¿›è¡Œè¯è¡¨æ‰©å……  
  
  [å¸¸è§é—®é¢˜](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E9%97%AE%E9%A2%984%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%89%A9%E5%85%85%E8%AF%8D%E8%A1%A8%E7%9B%B4%E6%8E%A5%E5%9C%A8%E5%8E%9F%E7%89%88llama%E4%B8%8A%E7%94%A8%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8D%E8%A1%8C%E5%90%97)
â€œLLaMAè¯è¡¨ä¸­ä»…åŒ…å«å¾ˆå°‘çš„ä¸­æ–‡å­—ç¬¦ï¼Œæ‰€ä»¥åœ¨åˆ‡è¯æ—¶ä¼šæŠŠä¸­æ–‡åˆ‡åœ°æ›´ç¢ï¼Œéœ€è¦å¤šä¸ªbyte tokenæ‰èƒ½æ‹¼æˆä¸€ä¸ªå®Œæ•´çš„æ±‰å­—ï¼Œè¿›è€Œå¯¼è‡´ä¿¡æ¯å¯†åº¦é™ä½ã€‚æ¯”å¦‚ï¼Œåœ¨æ‰©å±•è¯è¡¨åçš„æ¨¡å‹ä¸­ï¼Œå•ä¸ªæ±‰å­—å€¾å‘äºè¢«åˆ‡æˆ1ä¸ªtokenï¼Œè€Œåœ¨åŸç‰ˆLLaMAä¸­å¯èƒ½å°±éœ€è¦2-3ä¸ªæ‰èƒ½ç»„åˆæˆä¸€ä¸ªæ±‰å­—ï¼Œæ˜¾è‘—é™ä½ç¼–è§£ç çš„æ•ˆç‡ã€‚â€  ï¼ˆè¿™å¯èƒ½å°±æ˜¯åœ¨ç”¨vicunaçš„æ—¶å€™è™½ç„¶æ˜¯2048toknsä½†æ˜¯å®é™…ç”¨èµ·æ¥ä¸Šä¸‹æ–‡å¾ˆçŸ­çš„åŸå› ï¼‰

- â€œå‚ç›´é¢†åŸŸçš„æ•°æ®é‡è¿‡å¤§ä¼šå¯¼è‡´æ¨¡å‹å¤±å»æ³›ç”¨èƒ½åŠ›ï¼Œç”šè‡³å¤±å»è¯­è¨€èƒ½åŠ›ï¼Œå³è¯´äººè¯çš„èƒ½åŠ›â€ï¼šhttps://github.com/Facico/Chinese-Vicuna/issues/68
  
- åŒä¸Š ptuning loraç­‰å°å‚æ•°å¾®è°ƒæ–¹æ³•å®¹æ˜“äº§ç”Ÿçš„ç¾éš¾æ€§é—å¿˜é—®é¢˜
â€œæœ€å¥½èƒ½å¤Ÿåœ¨å¾®è°ƒè¯­æ–™ä¸­ä¹ŸåŠ å…¥é€šç”¨å­¦ä¹ è¯­æ–™ä¸€èµ·å¾®è°ƒï¼Œé¿å…äº§ç”Ÿå¯¹å¾®è°ƒè¯­æ–™æå¤§çš„åå‘ï¼Œåœ¨instruct gptè®ºæ–‡ä¸­ä¹Ÿæåˆ°åœ¨å¼ºåŒ–å­¦ä¹ ppoçš„æ—¶å€™æ¨¡å‹ä¹Ÿä¼šå¾ˆå®¹æ˜“å¯¹äºppoæ•°æ®æ‹Ÿåˆï¼Œé™ä½æ¨¡å‹é€šç”¨è‡ªç„¶è¯­è¨€ä»»åŠ¡èƒ½åŠ›ï¼Œæ‰€ä»¥åœ¨ppo lossä¸­åŠ å…¥äº†SFTæ¢¯åº¦å’Œé¢„è®­ç»ƒæ¢¯åº¦æ¥ç¼“è§£è¿™ç§é—å¿˜é—®é¢˜ã€‚â€

- ä¼˜åŒ–æ–¹å‘ https://github.com/imClumsyPanda/langchain-ChatGLM/issues/14
  
- fintuningæ•°æ®å‡†å¤‡é—®é¢˜   https://github.com/THUDM/ChatGLM-6B/issues/364
  doc2query/msmarco-chinese-mt5-base-v1ï¼Œ æ ¹æ®docç”Ÿæˆé—®é¢˜

### LLMæˆé•¿è·¯çº¿å›¾

ç¤ºä¾‹ï¼šhttps://github.com/LAION-AI/Open-Assistant#the-plan


### LLMæœ¯è¯­

#### Adapter

Adapteræ˜¯ä¸€ç§è½»é‡çº§çš„é€‚åº”æ–¹æ³•ï¼Œå®ƒåœ¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ä¸­æ·»åŠ å°‘é‡å¯å­¦ä¹ çš„å‚æ•°ï¼Œåœ¨ä¿ç•™å…¶åŸå§‹çŸ¥è¯†çš„åŒæ—¶å®ç°é«˜æ•ˆçš„å¾®è°ƒï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„å“åº”å¹¶æé«˜å„ç§ä»»åŠ¡çš„æ€§èƒ½ã€‚

#### Alpaca 
Alpaca æ˜¯åœ¨ 52K æŒ‡ä»¤è·Ÿè¸ªæ¼”ç¤ºä¸Šå¯¹ LLaMA 7B æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚å®ƒçš„è¡Œä¸ºåœ¨è´¨é‡ä¸Šä¸ OpenAI çš„ text-davinci-003 ç›¸ä¼¼ï¼ŒåŒæ—¶å‡ºäººæ„æ–™åœ°å°ä¸”æ˜“äº/å»‰ä»·å¤åˆ¶ï¼ˆ<600 ç¾å…ƒï¼‰
#### Attention
æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨çš„ä¸€ä¸ªç»„ä»¶ï¼Œç”¨äºåœ¨å¤„ç†è¿‡ç¨‹ä¸­å…³æ³¨è¾“å…¥åºåˆ—çš„ç‰¹å®šéƒ¨åˆ†ï¼Œæ ¹æ®å…ƒç´ çš„ç›¸å…³æ€§ä¸ºå…ƒç´ åˆ†é…ä¸åŒçš„æƒé‡ã€‚å®ƒæœ‰åŠ©äºæ•è·ä¾èµ–æ€§å¹¶æé«˜æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³é¢„æµ‹æˆ–è¾“å‡ºçš„èƒ½åŠ›ã€‚
#### Cosine Similarity
ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯é€šè¿‡è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´è§’åº¦çš„ä½™å¼¦æ¥ç¡®å®šä¸¤ä¸ªå‘é‡ä¹‹é—´ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚å…¶èŒƒå›´ä¸º -1 åˆ° 1ï¼Œå€¼è¶Šæ¥è¿‘ 1 è¡¨ç¤ºç›¸ä¼¼åº¦è¶Šé«˜ï¼Œå€¼è¶Šæ¥è¿‘ -1 è¡¨ç¤ºä¸ç›¸ä¼¼åº¦ã€‚
#### Embedding
åµŒå…¥æ˜¯è¿ç»­å‘é‡ç©ºé—´ä¸­å•è¯ã€å¥å­æˆ–å…¶ä»–è¯­è¨€å•å…ƒçš„å¯†é›†ã€ä½ç»´è¡¨ç¤ºï¼Œæ•è·è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®ƒä»¬æ˜¯é€šè¿‡æ— ç›‘ç£æ–¹æ³•å­¦ä¹ çš„ï¼Œå¹¶ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚
#### GPTQ
GPTQ æ˜¯ä¸€ç§åŸºäºè¿‘ä¼¼äºŒé˜¶ä¿¡æ¯çš„ä¸€æ¬¡æ€§æƒé‡é‡åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©å…·æœ‰ 1750 äº¿ä¸ªå‚æ•°çš„ GPT æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒç²¾åº¦ï¼Œå…è®¸å• GPU æ‰§è¡Œå¹¶æ¯” FP16 æ˜¾ç€åŠ é€Ÿæ¨ç†ã€‚
#### Instruction Tuning
åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼ŒæŒ‡ä»¤è°ƒä¼˜æ˜¯ä¸€ç§é€šè¿‡æ ¹æ®ç‰¹å®šè¾“å…¥æŒ‡ä»¤æˆ–ç¤ºä¾‹ä¼˜åŒ–å…¶å“åº”æ¥å¾®è°ƒæ¨¡å‹çš„æŠ€æœ¯ï¼Œä»è€Œæé«˜å…¶ä¸ºç»™å®šæç¤ºæˆ–ä¸Šä¸‹æ–‡ç”Ÿæˆç›¸å…³ä¸”å‡†ç¡®çš„è¾“å‡ºçš„æ€§èƒ½ã€‚
#### LoRA
LoRA é€šè¿‡å­¦ä¹ æ’åºåˆ†è§£çŸ©é˜µå¹¶ç»“åˆå†»ç»“åŸå§‹æƒé‡ï¼Œå®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å‚æ•°å‡å°‘ã€‚è¿™æ˜¾ç€å‡å°‘äº†ç‰¹å®šäºä»»åŠ¡çš„é€‚åº”çš„å­˜å‚¨éœ€æ±‚ï¼Œä¿ƒè¿›éƒ¨ç½²æœŸé—´çš„é«˜æ•ˆä»»åŠ¡åˆ‡æ¢ï¼Œè€Œä¸ä¼šå¼•å…¥æ¨ç†å»¶è¿Ÿï¼Œå¹¶ä¸”ä¸é€‚é…å™¨ã€å‰ç¼€è°ƒæ•´å’Œå¾®è°ƒç­‰å…¶ä»–é€‚åº”æ–¹æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚
#### Prefix tuning https://arxiv.org/abs/2101.00190
å‰ç¼€è°ƒä¼˜æ˜¯è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡å¾®è°ƒçš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚å®ƒä¿æŒè¯­è¨€æ¨¡å‹å‚æ•°å†»ç»“ï¼Œä½†ä¼˜åŒ–ä¸€ä¸ªå°çš„è¿ç»­ç‰¹å®šäºä»»åŠ¡çš„å‘é‡ï¼ˆç§°ä¸ºå‰ç¼€ï¼‰ã€‚å‰ç¼€è°ƒæ•´ä»æç¤ºä¸­æ±²å–çµæ„Ÿï¼Œå…è®¸åç»­ä»¤ç‰Œå…³æ³¨æ­¤å‰ç¼€ï¼Œå°±å¥½åƒå®ƒæ˜¯â€œè™šæ‹Ÿä»¤ç‰Œâ€ä¸€æ ·ã€‚
#### Prompt Tuning
Promp tuningæ˜¯ä¸€ç§ç»æµæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶æ›´æ–°å…¶æƒé‡çš„æƒ…å†µä¸‹ä½¿äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹é€‚åº”æ–°çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚
#### QLoRA
QLoRA æ˜¯ä¸€ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯æ˜¾ç€é™ä½å†…å­˜éœ€æ±‚ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å•ä¸ª 48GB GPU ä¸Šå¾®è°ƒ 650 äº¿ä¸ªå‚æ•°æ¨¡å‹ï¼Œè€Œä¸ä¼šå½±å“ 16 ä½å¾®è°ƒä»»åŠ¡çš„æ€§èƒ½ã€‚
#### RLHF
æ ¹æ®äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œä¹Ÿç§°ä¸ºæ ¹æ®äººç±»åå¥½è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¯ä¸€ç§åˆ©ç”¨äººç±»åé¦ˆæ¥è®­ç»ƒâ€œå¥–åŠ±æ¨¡å‹â€çš„æŠ€æœ¯ã€‚ç„¶åå°†è¯¥æ¨¡å‹ç”¨ä½œå¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹  (RL) ä¼˜åŒ–ä»£ç†çš„ç­–ç•¥ã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨è¯¸å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ä¹‹ç±»çš„ä¼˜åŒ–ç®—æ³•æ¥å®ç°çš„ã€‚
#### Self-instruction
Self-instructæ˜¯ä¸€ç§ç»“åˆè‡ªç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒæ¶‰åŠä½¿ç”¨æ¨¡å‹è‡ªèº«çš„é¢„æµ‹ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œä½¿å…¶èƒ½å¤Ÿä»ç”Ÿæˆçš„æ•°æ®ä¸­å­¦ä¹ å¹¶æé«˜å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
#### semantic search
è¯­ä¹‰æœç´¢æ˜¯ä¸€ç§æœç´¢æŠ€æœ¯ï¼Œæ—¨åœ¨ç†è§£ç”¨æˆ·æŸ¥è¯¢å’Œæœç´¢å†…å®¹èƒŒåçš„å«ä¹‰å’Œæ„å›¾ã€‚å®ƒè¶…è¶Šäº†ç®€å•çš„å…³é”®å­—åŒ¹é…ï¼Œå¹¶è€ƒè™‘äº†æŸ¥è¯¢å’Œæ–‡æ¡£çš„ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ï¼Œä»è€Œäº§ç”Ÿæ›´å‡†ç¡®å’Œç›¸å…³çš„æœç´¢ç»“æœã€‚
#### Tokenization
Tokenizationæ˜¯å°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å•å…ƒä»¥ä¾› LLM æ¨¡å‹å¤„ç†çš„è¿‡ç¨‹ã€‚Byte Pair Encoding (BPE) æˆ– WordPiece ç­‰å­å­—ç®—æ³•ç”¨äºå°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å•å…ƒï¼Œä»è€Œæ•è·é¢‘ç¹å’Œç½•è§çš„å•è¯ã€‚
#### Topic Modeling
ä¸»é¢˜å»ºæ¨¡æ˜¯ä¸€ç§ç»Ÿè®¡æŠ€æœ¯ï¼Œç”¨äºå‘ç°æ–‡æ¡£é›†åˆä¸­çš„æ½œåœ¨ä¸»é¢˜æˆ–ä¸»é¢˜ã€‚å®ƒä¸ºå•è¯åˆ†é…æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶æ ¹æ®å•è¯å…±ç°æ¨¡å¼è¯†åˆ«ä¸»é¢˜ï¼Œæœ‰åŠ©äºç†è§£æ–‡æœ¬è¯­æ–™åº“ä¸­å­˜åœ¨çš„ä¸»è¦ä¸»é¢˜å’Œç»“æ„ã€‚
#### Transformer
Transformer æ˜¯ã€ŠAttention Is All You Needã€‹è®ºæ–‡ä¸­ä»‹ç»çš„ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚å®ƒç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè‡ªæ³¨æ„åŠ›å±‚å–ä»£äº†ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)ï¼Œç”¨äºæ•è·ä¾èµ–æ€§å¹¶æ”¹è¿›é¡ºåºæ•°æ®ä»»åŠ¡ä¸­çš„å¹¶è¡Œå¤„ç†ã€‚


## LLMs Surveys
A Survey of Large Language Models
- Renmin University of China
- https://arxiv.org/pdf/2303.18223.pdf
- é€šè¿‡ä»‹ç»èƒŒæ™¯ã€ä¸»è¦å‘ç°å’Œä¸»æµæŠ€æœ¯æ¥å›é¡¾LLMsçš„æœ€æ–°è¿›å±•ã€‚é‡ç‚¹å…³æ³¨LLMçš„å››ä¸ªä¸»è¦æ–¹é¢ï¼Œå³é¢„è®­ç»ƒã€é€‚åº”è°ƒä¼˜ã€åˆ©ç”¨å’Œèƒ½åŠ›è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ€»ç»“äº†å‘å±•LLMsçš„å¯ç”¨èµ„æºï¼Œå¹¶è®¨è®ºäº†æœªæ¥æ–¹å‘çš„å‰©ä½™é—®é¢˜ã€‚

Augmented Language Models: a Survey
- Meta AI
- https://arxiv.org/abs/2302.07842
- å›é¡¾äº†é€šè¿‡æ¨ç†æŠ€èƒ½å’Œä½¿ç”¨å·¥å…·çš„èƒ½åŠ›å¢å¼ºè¯­è¨€æ¨¡å‹ (LM) çš„å·¥ä½œ

Large Language Models Meet NL2Code: A Survey (2023 ACL)
- Chinese Academy of Sciences
- https://aclanthology.org/2023.acl-long.411/
- å¯¹ NL2Code çš„ 27 ä¸ªç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œå¹¶å›é¡¾äº†åŸºå‡†å’ŒæŒ‡æ ‡ã€‚åœ¨ HumanEval åŸºå‡†ä¸Šå¯¹æ‰€æœ‰ç°æœ‰æ¨¡å‹è¿›è¡Œç›´è§‚æ¯”è¾ƒã€‚é€šè¿‡æ·±å…¥è§‚å¯Ÿå’Œåˆ†æï¼Œå¾—å‡ºä¸€äº›è§è§£å¹¶å¾—å‡ºç»“è®ºï¼šNL2Code å¤§å‹è¯­è¨€æ¨¡å‹æˆåŠŸçš„å…³é”®å› ç´ æ˜¯â€œå¤§å°ºå¯¸ã€ä¼˜è´¨æ•°æ®ã€ä¸“å®¶è°ƒä¼˜â€ã€‚

A Survey on Model Compression and Acceleration for Pretrained Language Models (2023 AAAI)
- University of California
- https://ojs.aaai.org/index.php/AAAI/article/view/26255
- é‡ç‚¹å…³æ³¨æ¨ç†é˜¶æ®µï¼Œå›é¡¾é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„å½“å‰çŠ¶æ€ï¼ŒåŒ…æ‹¬åŸºå‡†ã€æŒ‡æ ‡å’Œæ–¹æ³•ã€‚

A Survey on Evaluation of Large Language Models
- Jilin University
- https://arxiv.org/pdf/2307.03109.pdf
- å¯¹å¤§æ¨¡å‹çš„è¿™äº›è¯„ä¼°æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„å›é¡¾ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šè¯„ä¼°ä»€ä¹ˆã€è¯„ä¼°åœ¨å“ªé‡Œä»¥åŠå¦‚ä½•è¯„ä¼°ã€‚

A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT
- Michigan State University
- https://arxiv.org/pdf/2302.09419.pdf
- é¢å›é¡¾äº† PFM (Pretrained Foundation Models) åœ¨æ–‡æœ¬ã€å›¾åƒã€å›¾è¡¨ä»¥åŠå…¶ä»–æ•°æ®æ¨¡å¼æ–¹é¢çš„æœ€æ–°ç ”ç©¶è¿›å±•ã€æŒ‘æˆ˜å’Œæœºé‡ã€‚è¯¥ç»¼è¿°æ¶µç›–äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¹ ä¸­ä½¿ç”¨çš„åŸºæœ¬ç»„ä»¶å’Œç°æœ‰é¢„è®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ¢ç´¢äº†ç”¨äºä¸åŒæ•°æ®æ¨¡å¼çš„é«˜çº§ PFM ä»¥åŠè€ƒè™‘æ•°æ®è´¨é‡å’Œæ•°é‡çš„ç»Ÿä¸€ PFMã€‚è¯¥è¯„è®ºè¿˜è®¨è®ºäº†ä¸ PFM åŸºç¡€ç›¸å…³çš„ç ”ç©¶ï¼Œä¾‹å¦‚æ¨¡å‹æ•ˆç‡å’Œå‹ç¼©ã€å®‰å…¨æ€§å’Œéšç§ã€‚

ChatGPT is not all you need. A State of the Art Review of large Generative AI models
- https://arxiv.org/pdf/2301.04655.pdf
- Universidad Pontificia Comillas


