# all-about-llm
å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒå’ŒæœåŠ¡è°ƒç ”
## LLM
### 1. [ç™¾å·å¤§æ¨¡å‹](https://github.com/baichuan-inc/baichuan-7B)
	
å®Œå…¨æ”¯æŒå•†ç”¨çš„æ¨¡å‹
å‘å¸ƒå›¢é˜Ÿï¼šç™¾å·æ™ºèƒ½
å®¶æ—ï¼šllama

æ³¨æ„ä»“åº“ä¸­çš„è¯„æµ‹ç»“æœæ˜¯åŸºäºsftå’ŒRLHF ä½†æ˜¯ç›®å‰å¼€æºçš„modelæ˜¯pretrainç‰ˆæœ¬	

é¢„è®­ç»ƒæ¨¡å‹ç»­å†™çš„å¥å­æœ‰æ¯”è¾ƒå¤§çš„é—®é¢˜ï¼ˆå¯èƒ½æ¥è‡ªäºè¯­æ–™é—®é¢˜ï¼Ÿï¼‰
{'text': 'ä»Šå¤©å¤©æ°”æ˜¯çœŸçš„æœ‰ç‚¹çƒ­,æˆ‘èµ°åœ¨è¡—ä¸Šçš„æ—¶å€™,å‘ç°äº†å¾ˆå¤šäººçš„è„¸ä¸Šæ³›äº†çº¢è‰²çš„......\næˆ‘èµ°åœ¨è¡—ä¸Šçš„æ—¶å€™,å‘ç°äº†å¾ˆå¤šäººçš„è„¸ä¸Šæ³›äº†çº¢è‰²çš„,æˆ‘é—®è€çˆ¸:ä¸ºä»€ä¹ˆç°åœ¨æœ‰è¿™ä¹ˆå¤šäººçš„è„¸ä¸Šéƒ½é•¿äº†æ–‘å‘¢?è€çˆ¸è¯´:å› ä¸ºå¤©æ°”å¤ªçƒ­äº†,è„¸ä¸Šæ²¡æœ‰æ±—æ°´çš„æ»‹æ¶¦,çš®è‚¤æ²¡æœ‰å…‰æ³½äº†ã€‚ä½ åˆ«é—®äº†,æˆ‘ç°åœ¨æ­£åœ¨å»å¼€ä¼šçš„åœ°æ–¹çš„è·¯ä¸Šã€‚'}

**7bé¢„è®¡åœ¨A100å ç”¨æ˜¾å­˜ ï¼š27689MB  ï¼› 4090å ç”¨æ˜¾å­˜23.6G æ¨ç†é€Ÿåº¦å¾ˆæ…¢**

####  æ¦‚è¿°

åŸºäºTransformerç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿tokensä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º**4096**

**é’ˆå¯¹chatçš„å¾®è°ƒæ–¹æ¡ˆï¼ˆéå®˜æ–¹ï¼‰**ï¼š
- MedicalGPTï¼šhttps://github.com/shibing624/MedicalGPT/blob/main/README.md
- LLaMA-Efficient-Tuning ï¼š
https://github.com/hiyouga/LLaMA-Efficient-Tuning

**é—®é¢˜ï¼šè¯„æµ‹å°é—­æ€§ä¸å¥½**ï¼Ÿâ¬‡ï¸

	è¾“å…¥ï¼š
	åœ¨Unixä¸­ï¼Œpasswdå‘½ä»¤ä½äº____ç›®å½•ä¸­çš„ã€‚
	è¾“å‡ºï¼š
	åœ¨Unixä¸­ï¼Œpasswdå‘½ä»¤ä½äº____ç›®å½•ä¸­çš„ã€‚ A. /etc/ B. /usr/ C. /bin/ D. /usr/bin/ ç­”æ¡ˆï¼šA</s>
**åˆ†è¯**
ä½¿ç”¨SentencePieceä¸­çš„Byte-PairEncoding(BPE)ä½œä¸ºåˆ†è¯ç®—æ³•ï¼Œå¹¶ä¸”è¿›è¡Œäº†ä»¥ä¸‹çš„ä¼˜åŒ–ï¼š
* ä½¿ç”¨2000ä¸‡æ¡ä»¥ä¸­è‹±ä¸ºä¸»çš„å¤šè¯­è¨€è¯­æ–™è®­ç»ƒåˆ†è¯æ¨¡å‹ï¼Œæå‡å¯¹äºä¸­æ–‡çš„å‹ç¼©ç‡ã€‚
* å¯¹äºæ•°å­¦é¢†åŸŸï¼Œå‚è€ƒäº†LLaMAå’ŒGalacticaä¸­çš„æ–¹æ¡ˆï¼Œå¯¹æ•°å­—çš„æ¯ä¸€ä½å•ç‹¬åˆ†å¼€ï¼Œé¿å…å‡ºç°æ•°å­—ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¹äºæå‡æ•°å­¦èƒ½åŠ›æœ‰é‡è¦å¸®åŠ©ã€‚
* å¯¹äºç½•è§å­—è¯ï¼ˆå¦‚ç‰¹æ®Šç¬¦å·ç­‰ï¼‰ï¼Œæ”¯æŒUTF-8charactersçš„byteç¼–ç ï¼Œå› æ­¤åšåˆ°æœªçŸ¥å­—è¯çš„å…¨è¦†ç›–ï¼Œè¯è¡¨å¤§å°è¾¾åˆ°6.4ä¸‡ã€‚
* å¯¹ä¸­æ–‡çš„å‹ç¼©ç‡æ¯”è¾ƒé«˜ï¼ˆï¼Ÿå‹ç¼©ç‡å’‹ç®—çš„ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼‰

| Model         | baichuan\-7B | LLaMA  | Falcon | mpt\-7B | ChatGLM | moss\-moon\-003 |
|---------------|--------------|--------|--------|---------|---------|-----------------|
| Compress Rate | 0\.737       | 1\.312 | 1\.049 | 1\.206  | 0\.631  | 0\.659          |
| Vocab Size    | 64,000       | 32,000 | 65,024 | 50,254  | 130,344 | 106,029         |

#### æ•°æ®
![æ•°æ®å¤„ç†æµç¨‹][def2]

#### æ¨¡å‹ç»“æ„
æ•´ä½“æ¨¡å‹åŸºäºæ ‡å‡†çš„ Transformer ç»“æ„ï¼Œé‡‡ç”¨äº†å’Œ LLaMA ä¸€æ ·çš„æ¨¡å‹è®¾è®¡
![æ¨¡å‹ç»“æ„][def]
- **ä½ç½®ç¼–ç ** rotary-embedding æ˜¯ç°é˜¶æ®µè¢«å¤§å¤šæ¨¡å‹é‡‡ç”¨çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå…·æœ‰æ›´å¥½çš„å¤–å»¶æ•ˆæœã€‚è™½ç„¶è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§é•¿åº¦ä¸º4096ï¼Œä½†æ˜¯å®é™…æµ‹è¯•ä¸­æ¨¡å‹å¯ä»¥å¾ˆå¥½çš„æ‰©å±•åˆ° 5000 tokens ä¸Šï¼Œå¦‚ä¸‹å›¾ï¼š
  ![Alt text][def3]
- **å…·ä½“å‚æ•°**
	| **Hyperparameter** | **Value**  |
	|--------------------|------------|
	| n_parameters       | 7000559616 |
	| n_layers           | 32         |
	| n_heads            | 32         |
	| d_model            | 4096       |
	| vocab size         | 64000      |
	| sequence length    | 4096       |


  

- æ¿€æ´»å±‚ï¼šSwiGLU, Feedforward å˜åŒ–ä¸º(8/3)å€çš„éšå«å±‚å¤§å°ï¼Œå³11008
- Layer-Normalization: åŸºäº RMSNorm çš„ Pre-Normalization
#### è®­ç»ƒç®—åŠ›
åƒå¡A800æœºå™¨ä¸Šè¾¾åˆ°äº†7Bæ¨¡å‹182Tflopsçš„ååï¼ŒGPUå³°å€¼ç®—åŠ›åˆ©ç”¨ç‡é«˜è¾¾58.3% ã€‚è®­ç»ƒç¨³å®šæ€§å’Œåå
#### æ¨¡å‹æ¨ç†
[huggingface](https://huggingface.co/baichuan-inc/baichuan-7B)


### 2. [Aquila æ‚Ÿé“å¤©é¹°ç³»åˆ—å•†ç”¨å¼€æºæ¨¡å‹](https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README.md)

å‘å¸ƒå›¢é˜Ÿï¼šæ™ºæºç¤¾åŒº
è¦ç”¨flagAIæ¡†æ¶ï¼ˆè¿™ä¸ªæ¡†æ¶æºç &ä¾èµ–çš„å…¼å®¹æ€§è¿˜æœ‰ä¸€äº›é”™è¯¯ï¼‰
å…³æ³¨issueï¼šhttps://github.com/FlagAI-Open/FlagAI/issues/334
è¿™ä¸ªé—®é¢˜å®˜æ–¹æ­£åœ¨ä¿®å¤
#### æ¦‚è¿°
**å®˜æ–¹ç®€ä»‹**
Aquilaè¯­è¨€å¤§æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº†GPT-3ã€LLaMAç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„tokenizerï¼Œå‡çº§äº†BMTrainå¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œåœ¨Aquilaçš„è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†æ¯”Magtron+DeepSpeed zero-2å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚Aquilaè¯­è¨€å¤§æ¨¡å‹æ˜¯åœ¨ä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™åŸºç¡€ä¸Šä»ï¼å¼€å§‹è®­ç»ƒçš„ï¼Œé€šè¿‡æ•°æ®è´¨é‡çš„æ§åˆ¶ã€å¤šç§è®­ç»ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œå®ç°åœ¨æ›´å°çš„æ•°æ®é›†ã€æ›´çŸ­çš„è®­ç»ƒæ—¶é—´ï¼Œè·å¾—æ¯”å…¶å®ƒå¼€æºæ¨¡å‹æ›´ä¼˜çš„æ€§èƒ½ã€‚ä¹Ÿæ˜¯é¦–ä¸ªæ”¯æŒä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€ç¬¦åˆå›½å†…æ•°æ®åˆè§„éœ€è¦çš„å¤§è§„æ¨¡å¼€æºè¯­è¨€æ¨¡å‹ã€‚

**æœ€ä½ç¡¬ä»¶éœ€æ±‚**ï¼š **è¿è¡ŒAquila-7Bç³»åˆ—éœ€è¦å†…å­˜30G, æ˜¾å­˜18Gï¼Œç”Ÿæˆæœ€å¤§é•¿åº¦ 2048 tokensã€‚**

Aquilaæ¨¡å‹æ‰€é‡‡ç”¨çš„tokenizeræ˜¯ç”±ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚æˆ‘ä»¬åœ¨å¤„ç†è‹±æ–‡ã€ä¸­æ–‡ä»¥åŠä»£ç æ•°æ®æ—¶ï¼Œé‡‡ç”¨äº†ä¸åŒçš„åˆ†è¯å™¨å¯¹ä¸€ä¸‡ä¸ªæ ·æœ¬è¿›è¡Œäº†æŠ½å–ã€‚Aquila tokenizerä¸å…¶ä»–tokenizerçš„å‚æ•°å¯¹æ¯”è§ä¸‹è¡¨:
| **æ¨¡å‹/Model** | **è¯è¡¨å¤§å°/Vocab size** | **è¯´æ˜/Note** | **è‹±æ–‡å¹³å‡tokensé‡/Avg tokens \(English \)** | **ä¸­æ–‡å¹³å‡tokensé‡/Avg tokens \(Chinesse \)** | **ä»£ç å¹³å‡tokensé‡/Avg tokens \(code\)** |
|--------------|---------------------|-------------|---------------------------------------|----------------------------------------|------------------------------------|
| GPT2         | 50527               | bpe         | 1717                                  | 1764                                   | 2323                               |
| LlaMA        | 32000               | sp\(bpe\)   | 1805                                  | 1257                                   | 1970                               |
| Aquila       | 100000              | bpe         | 1575                                  | 477                                    | 1679                               |

#### æ¨¡å‹ä¸‹è½½
- è„šæœ¬ä¸‹è½½
-  FlagOpen æ¨¡å‹ä»“åº“ä¸‹è½½ https://model.baai.ac.cn/models

å…¶ä»–ä¿¡æ¯ç­‰å¾…å…­æœˆåº•å®˜æ–¹æŠ€æœ¯æŠ¥å‘Š



### 3. [Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½èµ„æºçš„llama+loraæ–¹æ¡ˆ](https://github.com/Facico/Chinese-Vicuna)

è¿™ç§æ–¹æ¡ˆå¯èƒ½ä¼šé™ä½æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

gitä»“åº“æä¾›äº†ä¸€ä¸ªé’ˆå¯¹llamaç³»åˆ—å·¥ç¨‹å¾ˆå¥½çš„QAæ–‡æ¡£ï¼šhttps://github.com/Facico/Chinese-Vicuna/blob/master/docs/notes.md

æœ‰å…³æŠ€æœ¯
- LLaMA paper: https://arxiv.org/abs/2302.13971v1
- Self-Instruct paper: https://arxiv.org/abs/2212.10560
- data generation: https://github.com/LianjiaTech/BELLE and https://guanaco-model.github.io/
- the first work: https://github.com/tatsu-lab/stanford_alpaca
- Lora paperï¼šhttps://arxiv.org/pdf/2106.09685.pdf
#### æ¦‚è¿°
**ç‰¹ç‚¹**ï¼šllama+ä¸­æ–‡+ä½èµ„æº+å‚æ–™è®­ç»ƒçš„æ–¹æ¡ˆ 
  åŸºäºLLaMA+instructionæ•°æ®æ„å»ºä¸€ä¸ªä¸­æ–‡çš„ç¾Šé©¼æ¨¡å‹ï¼Œå¹¶å¸®åŠ©å¤§å®¶èƒ½å¿«é€Ÿå­¦ä¼šä½¿ç”¨å¼•å…¥è‡ªå·±çš„æ•°æ®ï¼Œå¹¶è®­ç»ƒå‡ºå±äºè‡ªå·±çš„å°ç¾Šé©¼ï¼ˆVicunaï¼‰
  ç±»ä¼¼äºstable diffusionæ¨¡å‹çš„çˆ†ç«ï¼Œå‡ºç°äº†åƒcivitaiç­‰å¹³å°ï¼Œç”±ä¸€ä¸ªåŸºç¡€çš„æ¨¡å‹+å„ç§LORAæ¨¡å‹çš„å¼€æºç¤¾åŒºã€‚è¿™ä¸ªé¡¹ç›®å°±æ˜¯å»è®­ç»ƒè¿™ä¸ªLORA
**Loraè®­ç»ƒæ–¹å¼**ï¼š
- ä»£ç åŸºäºalpaca-loraå¼€å‘ï¼Œhttps://github.com/tloen/alpaca-lora
- è¿™æ˜¯ä¸€å¥—æ¯”è¾ƒç®€å•çš„ä»£ç ï¼ŒåŸºæœ¬æ€è·¯å°±æ˜¯ç”¨PEFTçš„loraæ¥å£+transformerçš„trainer+instructionçš„æ•°æ®é…ç½®
**è®­ç»ƒç®—åŠ›é…ç½®ï¼š**ï¼ˆæ¥æºä¸ºé¡¹ç›®githubï¼‰
	4090çš„ç®—åŠ›çº¦ä¸º3090ä¸¤å€ï¼ŒA100-40Gçš„int8ç®—åŠ›ä¸4090ç›¸è¿‘
	| **Model** | **GPU** | **lora+fp16+512** |  | **lora+int8+256** |  | **lora+int8+512** |  | **lora+int8+2048** | |
	|-----------|---------|--------------------|------|--------------------|------|--------------------|------|---------------------|------|
	|           |         | speed              | size | speed              | size | speed              | size | speed               | size |
	| LLaMA-7B  | 2080Ti  |                    |      | 0.2h/w            | 11G  |                    |      |                     |      |
	|           | 3090    |                    |      |                    |      |                    |      |                     |      |
	|           | 4090    | 0.3h/w            | 20G  |                    |      | 0.8h/w            |      | 3.5h/w              | 20G  |
	| LLaMA-13B | 3090    |                    |      | 0.9h/w            |      |                    |      | 7.5h/w              | 24G  |
	|           | 4090    |


**æ³¨æ„ï¼š**
- int8 ä»…åŠ è½½æ¨¡å‹æ˜¾å­˜å ç”¨ï¼ˆVRAMï¼‰$ \approx $ ç¡¬ç›˜ç©ºé—´å¤§å°ï¼Œæ¯”å¦‚7Bå¤§æ¦‚8Gå·¦å³ï¼Œ13Bå¤§æ¦‚14Gå·¦å³ï¼›å¦‚æœæ˜¯fp16å’Œfp32åˆ™ç›¸åº”ä¹˜2å’Œä¹˜4
- è®­ç»ƒçš„æ—¶å€™ï¼Œæ˜¾å­˜å ç”¨å’Œè®­ç»ƒçš„é€Ÿåº¦å’Œåºåˆ—é•¿åº¦å¯†åˆ‡ç›¸å…³ï¼Œæ¯”å¦‚åºåˆ—é•¿åº¦256æ˜¾å­˜å ç”¨ä¸è¶…è¿‡11Gï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥åœ¨2080Tiä¸Šå¾®è°ƒ7Bï¼Œåºåˆ—é•¿åº¦å¦‚æœæ˜¯2048ï¼Œåˆ™æ˜¾å­˜å ç”¨ä¼šéª¤å¢åˆ°20Gï¼Œå°±è¦ä¸Š3090æˆ–è€…4090æ‰èƒ½å¾®è°ƒ7Bäº†ï¼›
- åŒç†ï¼Œ13Båœ¨3090/4090ä¸Šæ˜¯å¯ä»¥å¾®è°ƒçš„ï¼Œ2048çš„æ—¶å€™microbatché™åˆ°1ä¹Ÿæ˜¯å¯ä»¥è·‘çš„
- å¦å¤–ï¼Œæœ‰äººå‘ç°åœ¨A100-40Gä¸Šå¢å¤§batchæ²¡æœ‰æ˜æ˜¾çš„æé€Ÿï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºint8æ¯”è¾ƒåƒç®—åŠ›ï¼ˆæ¯”å¦‚ç›¸åŒçš„é…ç½®fp16å¿«äºint8ï¼‰ï¼Œç®—åŠ›åƒæ»¡åå¢åŠ batchä¹Ÿä¸èƒ½æé«˜ååé‡ï¼Œå¦ä¸€æ–¹é¢A100-40Gçš„int8ç®—åŠ›å…¶å®å’Œ4090å·®ä¸å¤šã€‚
#### æ•°æ®
åˆ©ç”¨äº†ç›®å‰å‡ ä»½é«˜è´¨é‡çš„å¼€æºæ•°æ®ï¼ˆè¿™äº›æ•°æ®å¾ˆå¤šéƒ½åƒalpacaé‚£æ ·ï¼Œä½¿ç”¨chatgptçš„æ¥å£ï¼Œç”Ÿæˆé«˜è´¨é‡çš„instructionæ•°æ®ã€‚ï¼‰
{"instruction": "ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\\n\n", "input": "", "output": "åœ°çƒä¸Šæœ‰é€‚å®œç”Ÿå‘½å­˜åœ¨çš„æ¡ä»¶å’Œå¤šæ ·åŒ–çš„ç”Ÿå‘½å½¢å¼ã€‚"}
- Belle https://github.com/LianjiaTech/BELLE
- guanaco https://huggingface.co/datasets/JosephusCheung/GuanacoDataset
æ•°æ®ä¸‹è½½é“¾æ¥
- æœ‰æ›´å¥½å¯¹è¯èƒ½åŠ›çš„chatv1çš„å¾®è°ƒæ•°æ®ï¼šhttps://huggingface.co/datasets/Chinese-Vicuna/instruct_chat_50k.jsonl ï¼šç”±3ä¸‡æ¡sharegptä¸­æ–‡æ•°æ®å’Œ2ä¸‡æ¡alpaca-instruction-Chinese-datasetæ•°æ®ç»„æˆ
- é“¾æ¥: https://pan.baidu.com/s/1WSxuhSAotl14ifaAiz5eKw?pwd=b4kb æå–ç : b4kb
- é“¾æ¥: https://drive.google.com/file/d/1tzXVhS74m-EtoFot7hEc005LDeZGPit_/view?usp=sharing
- é“¾æ¥: https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0 ï¼ˆ694K rows  409MBï¼‰
#### æ¨¡å‹
- ä¸Šæ¸¸æ¨¡å‹ï¼šLLAMA 7B
- é¡¹ç›®æä¾›äº†loraæ¨¡å‹
  - åŠ è½½æ–¹å¼å‚è€ƒgenerate.py
    - Chinese-Vicuna/Chinese-Vicuna-lora-7b-belle-and-guanaco
    - Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco
  - æ¨¡å‹ä½¿ç”¨çš„æ˜¯8bit+lora+256 tokens
  - æ›´å¤šæ¨¡å‹ï¼šhttps://huggingface.co/Chinese-Vicuna

#### 4.  [Chinese-LLaMA-Alpaca å¼€æºä¸­æ–‡LLaMAæ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

chinese-vicunaä¸­æåˆ°è¿™ä¸ªé¡¹ç›®ï¼š**åšäº†è¯è¡¨æ‰©å……ä½†æ˜¯æ•ˆæœä¸åŠæ²¡æœ‰æ‰©å……è¯è¡¨çš„fastchat-vicuna**

è¿™ä¸ªé¡¹ç›®ç»™äº†ä¸€ä¸ªæ‰©å……è¯è¡¨çš„æ€è·¯&å¼€æºäº†å¾ˆå¤šæœ‰ç”¨çš„å·¥å…·å’Œä»£ç ;é¡¹ç›®wikiæœ‰å¾ˆå¤šå¹²è´§;è®­ç»ƒä»£ä»·æ¯”è¾ƒå¤§

#### æ¦‚è¿°

**æŠ€æœ¯æŠ¥å‘Š**ï¼š[Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)

- ğŸš€ é’ˆå¯¹åŸç‰ˆLLaMAæ¨¡å‹æ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œæå‡äº†ä¸­æ–‡ç¼–è§£ç æ•ˆç‡
- ğŸš€ å¼€æºäº†ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬æ•°æ®é¢„è®­ç»ƒçš„ä¸­æ–‡LLaMAä»¥åŠç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ä¸­æ–‡Alpaca
- ğŸš€ å¼€æºäº†é¢„è®­ç»ƒè„šæœ¬ã€æŒ‡ä»¤ç²¾è°ƒè„šæœ¬ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€è¦è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹
- ğŸš€ å¿«é€Ÿä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆä¸ªäººPCï¼‰çš„CPU/GPUæœ¬åœ°é‡åŒ–å’Œéƒ¨ç½²ä½“éªŒå¤§æ¨¡å‹
- ğŸš€ æ”¯æŒğŸ¤—transformers, llama.cpp, text-generation-webui, LlamaChat, LangChain, privateGPTç­‰ç”Ÿæ€
- ç›®å‰å·²å¼€æºçš„æ¨¡å‹ç‰ˆæœ¬ï¼š7Bï¼ˆåŸºç¡€ç‰ˆã€Plusç‰ˆï¼‰ã€13Bï¼ˆåŸºç¡€ç‰ˆã€Plusç‰ˆï¼‰ã€33Bï¼ˆåŸºç¡€ç‰ˆï¼‰ ï¼ˆæ¨¡å‹å¤§å°ç›¸åŒ plusç‰ˆæœ¬ä½¿ç”¨çš„æ•°æ®æ›´å¤šï¼‰
  
**è®­ç»ƒé…ç½®ï¼š**
ä»¥ä¸‹æ˜¯è®­ç»ƒåŸºç¡€ç‰ˆ7Bæ¨¡å‹çš„è®­ç»ƒé…ç½®ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒæŠ€æœ¯æŠ¥å‘Šã€‚

| **å®éªŒè®¾ç½®**                   | **é¢„è®­ç»ƒ-ç¬¬ä¸€é˜¶æ®µ**     | **é¢„è®­ç»ƒ-ç¬¬äºŒé˜¶æ®µ**     | **æŒ‡ä»¤ç²¾è°ƒ**          |
|----------------------------|-------------------|-------------------|-------------------|
| Batch Size                 | 1024              | 1024              | 512               |
| Initial Learning Rate      | 2.00E-04        | 1.00E-04        | 1.00E-04        |
| Training Steps             | 3K                | 6K                | 6K-10K           |
| Max Length                 | 512               | 512               | 512               |
| Trainable Parameters (%) | 2.97%            | 6.06%            | 6.22%            |
| Training Device            | 8 Ã— A100          | 16 Ã— A100         | 16 Ã— A100         |
| Distributed Training       | DeepSpeed Zero-2 | DeepSpeed Zero-2 | DeepSpeed Zero-2 |

**è®­ç»ƒç»†èŠ‚**
- åœ¨é€šç”¨ä¸­æ–‡è¯­æ–™ä¸Šè®­ç»ƒäº†åŸºäºsentencepieceçš„20Kä¸­æ–‡è¯è¡¨å¹¶ä¸åŸç‰ˆLLaMAæ¨¡å‹çš„32Kè¯è¡¨è¿›è¡Œåˆå¹¶--æœ€ç»ˆè¯è¡¨å¤§å°49953&49954
- æ›´å¤šç»†èŠ‚è§ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82


#### æ¨¡å‹
å‘å¸ƒçš„æ˜¯LoRAæƒé‡ï¼Œéœ€è¦æ­é…[åŸç‰ˆLLaMAæ¨¡å‹](https://github.com/facebookresearch/llama)ï¼Œç„¶å[åˆå¹¶æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E5%90%88%E5%B9%B6%E6%A8%A1%E5%9E%8B)ã€‚
ä¸‹è½½ååŠ¡å¿…æ£€æŸ¥å‹ç¼©åŒ…ä¸­æ¨¡å‹æ–‡ä»¶çš„SHA256æ˜¯å¦ä¸€è‡´ï¼Œè¯·æŸ¥çœ‹[SHA256.md](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/SHA256.md)ã€‚

**æ¨¡å‹å¯¹æ¯”**

| **å¯¹æ¯”é¡¹**                  | **ä¸­æ–‡LLaMA**                 | **ä¸­æ–‡Alpaca**                            |
|--------------------------|-----------------------------|-----------------------------------------|
| è®­ç»ƒæ–¹å¼                     | ä¼ ç»ŸCLM                       | æŒ‡ä»¤ç²¾è°ƒ                                    |
| è®­ç»ƒè¯­æ–™                     | æ— æ ‡æ³¨é€šç”¨è¯­æ–™                     | æœ‰æ ‡æ³¨æŒ‡ä»¤æ•°æ®                                 |
| è¯è¡¨å¤§å°              | 49953                       | 49954=49953\+1ï¼ˆpad tokenï¼‰               |
| è¾“å…¥æ¨¡æ¿                     | ä¸éœ€è¦                         | éœ€è¦ç¬¦åˆæ¨¡æ¿è¦æ±‚                         |
| é€‚ç”¨åœºæ™¯ âœ”ï¸                  | æ–‡æœ¬ç»­å†™ï¼šç»™å®šä¸Šæ–‡å†…å®¹ï¼Œè®©æ¨¡å‹ç»§ç»­å†™ä¸‹å»ï¼Œç”Ÿæˆä¸‹æ–‡   | "1ã€æŒ‡ä»¤ç†è§£ï¼ˆé—®ç­”ã€å†™ä½œã€å»ºè®®ç­‰ï¼‰                      |
|å¤šè½®ä¸Šä¸‹æ–‡ç†è§£ï¼ˆèŠå¤©ç­‰ï¼‰          |
| ä¸é€‚ç”¨åœºæ™¯ âŒ                  | æŒ‡ä»¤ç†è§£ ã€å¤šè½®èŠå¤©ç­‰                 | æ–‡æœ¬æ— é™åˆ¶è‡ªç”±ç”Ÿæˆ                               |
| llama\.cpp               | ä½¿ç”¨\-på‚æ•°æŒ‡å®šä¸Šæ–‡                 | ä½¿ç”¨\-inså‚æ•°å¯åŠ¨æŒ‡ä»¤ç†è§£\+èŠå¤©æ¨¡å¼                   |
| text\-generation\-webui  | ä¸é€‚åˆchatæ¨¡å¼                   | ä½¿ç”¨\-\-cpuå¯åœ¨æ— æ˜¾å¡å½¢å¼ä¸‹è¿è¡Œï¼Œè‹¥ç”Ÿæˆå†…å®¹ä¸æ»¡æ„ï¼Œå»ºè®®ä¿®æ”¹prompt |
| LlamaChat                | åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©"LLaMA"              | åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©"Alpaca"                         |
| HFæ¨ç†ä»£ç                    | æ— éœ€æ·»åŠ é¢å¤–å¯åŠ¨å‚æ•°                  | å¯åŠ¨æ—¶æ·»åŠ å‚æ•° \-\-with\_prompt                |
| web\-demoä»£ç               | ä¸é€‚ç”¨                         | ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯ï¼›æ”¯æŒå¤šè½®å¯¹è¯                 |
| LangChainç¤ºä¾‹ / privateGPT | ä¸é€‚ç”¨                         | ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯                        |
| å·²çŸ¥é—®é¢˜                     | å¦‚æœä¸æ§åˆ¶ç»ˆæ­¢ï¼Œåˆ™ä¼šä¸€ç›´å†™ä¸‹å»ï¼Œç›´åˆ°è¾¾åˆ°è¾“å‡ºé•¿åº¦ä¸Šé™ã€‚ | ç›®å‰ç‰ˆæœ¬æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ç›¸å¯¹çŸ­ä¸€äº›ï¼Œæ¯”è¾ƒæƒœå­—å¦‚é‡‘ã€‚å¯åœ¨æŒ‡ä»¤ä¸­è¦æ±‚è¯¦ç»†å›ç­”ã€‚  |

**Loraä¸‹è½½ï¼š**
| **æ¨¡å‹åç§°**                     | **è®­ç»ƒæ•°æ®** | **é‡æ„æ¨¡å‹**     | **å¤§å°** | **LoRAä¸‹è½½** |
|------------------------------|----------|-------------------|-------------|-----------------|
| Chinese\-LLaMA\-7B           | é€šç”¨20G    | åŸç‰ˆLLaMA\-7B       | 770M        | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1xV1UXjh1EPrPtXg6WyG7XQ?pwd=923e) [googledrive](https://drive.google.com/file/d/1JvFhBpekYiueWiUL3AF1TtaWDb3clY5D/view?usp=sharing)    |          |
| Chinese\-LLaMA\-Plus\-7B â­ï¸  | é€šç”¨120G   | åŸç‰ˆLLaMA\-7B       | 790M        |[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/12tjjxmDWwLBM8Tj_7FAjHg?pwd=32hc) [googledrive](https://drive.google.com/file/d/1EDcTmq6tDmRxqarpapdyDGBE9opY0zrB/view?usp=share_link)     |         |
| Chinese\-LLaMA\-13B          | é€šç”¨20G    | åŸç‰ˆLLaMA\-13B      | 1\.0G       | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1wYoSF58SnU9k0Lndd5VEYg?pwd=mm8i) [googledrive](https://drive.google.com/file/d/1gzMc0xMCpXsXmU1uxFlgQ8VRnWNtDjD8/view?usp=share_link)       |          |
| Chinese\-LLaMA\-Plus\-13B â­ï¸ | é€šç”¨120G   | åŸç‰ˆLLaMA\-13B      | 1\.0G       | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1Mew4EjBlejWBBB6_WW6vig?pwd=mf5w) [googledrive](https://drive.google.com/file/d/1CcLJvY7XsFAOjfSIqCpDI7jf3EEPDcEF/view?usp=share_link)       |              |
| Chinese\-LLaMA\-33B          | é€šç”¨20G    | åŸç‰ˆLLaMA\-33B\ | 2\.7G       |  [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1fey7lGMMw3GT982l8uJYMg?pwd=2f2s) [googledrive](https://drive.google.com/file/d/1YeSgnZWaRkKdmYa-JHiIlcvqhrDd4-Y4/view?usp=share_link)          |

åŒæ—¶ï¼šå¯ä»¥åœ¨ğŸ¤—Model Hubä¸‹è½½ä»¥ä¸Šæ‰€æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”ä½¿ç”¨transformerså’ŒPEFTè°ƒç”¨ä¸­æ–‡LLaMAæˆ–Alpaca LoRAæ¨¡å‹ã€‚è¯¦ç»†è§é¡¹ç›®gitä»“åº“

#### é‡åŒ–æ¨ç†å’Œéƒ¨ç½²
[å‚è€ƒ](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%B8%8E%E9%83%A8%E7%BD%B2)

#### æ•°æ®

**é¢„è®­ç»ƒè¯­æ–™**
åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨çº¦20Gå·¦å³çš„é€šç”¨ä¸­æ–‡è¯­æ–™ï¼ˆä¸ä¸­æ–‡BERT-wwmã€MacBERTã€LERTã€PERTä¸­ä½¿ç”¨çš„è¯­æ–™ä¸€è‡´ï¼‰åœ¨åŸç‰ˆLLaMAæƒé‡çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è¿›è¡Œé¢„è®­ç»ƒ

**æŒ‡ä»¤ç²¾è°ƒè¯­æ–™**
| **æ•°æ®**              | **é‡çº§** | **æ¥æº** | **è¯´æ˜**                         |
|---------------------|--------|--------|--------------------------------|
| ä¸­è‹±ç¿»è¯‘æ•°æ®              | 500K   | [å¤–éƒ¨é“¾æ¥](https://github.com/brightmart/nlp_chinese_corpus#5%E7%BF%BB%E8%AF%91%E8%AF%AD%E6%96%99translation2019zh)   | åœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡‡æ ·\+è§„åˆ™ç­›é€‰           |
| pCLUEæ•°æ®             | 300K   | [å¤–éƒ¨é“¾æ¥](https://github.com/CLUEbenchmark/pCLUE)   | åœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡‡æ ·\+è§„åˆ™ç­›é€‰           |
| Alpacaæ•°æ®ï¼ˆè‹±ï¼‰         | 50K    | [å¤–éƒ¨é“¾æ¥](htthttps://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/dataps://github.com/tatsu-lab/stanford_alpaca)  | æ–¯å¦ç¦åŸç‰ˆAlpacaè®­ç»ƒæ•°æ®                |
| Alpacaæ•°æ®ï¼ˆä¸­ï¼‰         | 50K    | [å¤–éƒ¨é“¾æ¥]()  | æœ¬é¡¹ç›®ä½¿ç”¨ChatGPTæ¥å£å°†è‹±æ–‡ç‰ˆç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆç­›æ‰ä¸€éƒ¨åˆ†ï¼‰ |
| Self\-instructionæ•°æ® | 1~2M   | ï¼ˆæš‚æ— ï¼‰   | æœ¬é¡¹ç›®ä½¿ç”¨ChatGPTæ¥å£è¿›è¡Œçˆ¬å–ï¼Œå…·ä½“è§ä»¥ä¸‹è„šæœ¬æè¿°   |

æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªåŠ¨æ€ç”Ÿæˆä¸åŒé¢†åŸŸå’ŒæŒ‡ä»¤ç±»å‹çš„promptçˆ¬å–è„šæœ¬ `script/crawl_prompt.py`

### 5.  [BELLE: Be Everyone's Large Language Model Engine](https://github.com/LianjiaTech/BELLE)

ç›¸æ¯”å¦‚ä½•åšå¥½å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒï¼ŒBELLEæ›´å…³æ³¨å¦‚ä½•åœ¨å¼€æºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¸®åŠ©æ¯ä¸€ä¸ªäººéƒ½èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªå±äºè‡ªå·±çš„ã€æ•ˆæœå°½å¯èƒ½å¥½çš„å…·æœ‰æŒ‡ä»¤è¡¨ç°èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ï¼Œé™ä½å¤§è¯­è¨€æ¨¡å‹ã€ç‰¹åˆ«æ˜¯ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨é—¨æ§›.

### 6.[Tigerbot](https://github.com/TigerResearch/TigerBot)
  - [ ] ä¸ºä»€ä¹ˆtigerbotä½¿ç”¨100wæ•°æ®æŒ‡ä»¤å¾®è°ƒçš„ç»“æœæ¯”ä¸ä¸Švicunaä½¿ç”¨50kæ•°æ®å¾®è°ƒçš„ç»“æœï¼Œæ˜¯pretrainingæ¨¡å‹æœ‰é—®é¢˜è¿˜æ˜¯bloomåº•åº§å°±æ˜¯ä¸è¡Œå‘¢ï¼ˆç”¨basemodelæµ‹è¯•ä¸€ä¸‹ï¼‰
  
æ¨¡å‹å¯¹æ ‡instuctGPT-6B ç›®å‰7bæ¨¡å‹çš„æ•ˆæœä¸€èˆ¬ å¯èƒ½è®­ç»ƒè¿‡ç¨‹æœ‰å¾ˆå¤šè„æ•°æ® æ¯”è¾ƒæ˜æ˜¾çš„é—®é¢˜æ˜¯ä¼šè¾“å‡ºå„ç§å¤šä½™çš„ç¬¦å·

demoç½‘é¡µï¼šhttps://tigerbot.com/chat

#### æ¦‚è¿°

**å‘å¸ƒæœºæ„**ï¼šè™åšç§‘æŠ€
**å®¶æ—**ï¼šbloom  
å…³äºä¸ºä»€ä¹ˆä½¿ç”¨bloomåŸºåº§è€Œä¸æ˜¯æ›´ç«3çš„llamaï¼š1 bloomåŸºåº§å¯ä»¥å•†ç”¨ / 2 é¡¹ç›®å¼€å§‹æ—¶é—´æ—©äºllama
**æˆå‘˜**ï¼šTigerBot-7B, TigerBot-7B-baseï¼ŒTigerBot-180B (research version)
**åŠŸèƒ½**ï¼šæä¾›äº†ä¸‰ç§ APIï¼ŒåŒ…æ‹¬ Chat-APIï¼ŒPlug-insï¼Œ**Fine-Tunes**ã€‚
**æ•°æ®**ï¼šé¢„è®­ç»ƒ 100Gï¼Œä» 2TB è¿‡æ»¤åçš„æ•°æ®ä¸­ç»è¿‡å»å™ªå»é‡æ¸…æ´—è€Œå¾—ï¼›ç›‘ç£å¾®è°ƒ 1G æˆ– 100 ä¸‡æ¡æ•°æ®ï¼ŒæŒ‰æ¯”ä¾‹æ¶µç›–ç”¨æˆ·æŒ‡ä»¤å¸¸è§çš„ 10 å¤§ç±» 120 å°ç±»ä»»åŠ¡

**å®˜æ–¹ä»‹ç»:**
åœ¨ BLOOM åŸºç¡€ä¸Šï¼Œåœ¨æ¨¡å‹æ¶æ„å’Œç®—æ³•ä¸Šåšäº†å¦‚ä¸‹ä¼˜åŒ–ï¼š
- æŒ‡ä»¤å®Œæˆç›‘ç£å¾®è°ƒçš„åˆ›æ–°ç®—æ³•ä»¥è·å¾—æ›´å¥½çš„å¯å­¦ä¹ å‹(learnability)ï¼Œ
- è¿ç”¨ ensemble å’Œ probabilistic modeling çš„æ–¹æ³•å®ç°æ›´å¯æ§çš„äº‹å®æ€§(factuality)å’Œåˆ›é€ æ€§(generativeness)ï¼Œ
- åœ¨å¹¶è¡Œè®­ç»ƒä¸Šï¼Œçªç ´äº† deep-speed ç­‰ä¸»æµæ¡†æ¶ä¸­è‹¥å¹²å†…å­˜å’Œé€šä¿¡é—®é¢˜ï¼Œä½¿å¾—åœ¨åƒå¡ç¯å¢ƒä¸‹æ•°æœˆæ— é—´æ–­ï¼ˆè´Ÿè´£äººè¯´æ˜¯æ”¹äº†deepseedæºç å®ç°ï¼‰
- å¯¹ä¸­æ–‡è¯­è¨€çš„æ›´ä¸è§„åˆ™çš„åˆ†å¸ƒï¼Œä» tokenizer åˆ°è®­ç»ƒç®—æ³•ä¸Šåšäº†æ›´é€‚åˆçš„ç®—æ³•ä¼˜åŒ–ã€‚
é‡åŒ–ï¼šä½¿ç”¨GPTQç®—æ³•å’ŒGPTQ-for-LLaMaå®ç°é‡åŒ–ï¼š

**è®­ç»ƒç®—åŠ›**ï¼š
7bæ¨¡å‹  å•å¡a100 40Gå°±å¯ä»¥è®­ç»ƒ
180bæ¨¡å‹  
**æ¨ç†ç®—åŠ›**ï¼š
é‡åŒ–å‰ï¼š`tigerbot-7b-sft` æ¨ç†å¯åœ¨1å¼ RXT3090ä¸Šè¿›è¡Œï¼›`tigerbot-180b-sft` æ¨ç†å¯åœ¨5å¼ A100(80G)ä¸Šè¿›è¡Œ    
**é‡åŒ–å**ï¼š`tigerbot-7b-sft-4bit-128g` æ¨ç†å¯åœ¨ä¸€å¼ RTX3090ä¸Šè¿›è¡Œ `tigerbot-180b-research-4bit-128g` æ¨ç†å¯åœ¨ä¸¤å¼ A100(80G)ä¸Šè¿›è¡Œ

#### æ•°æ®

**é¢„è®­ç»ƒæ•°æ®**
åŸºäº GPT3 çš„ pretrain çš„æ•°æ®åˆ†å¸ƒï¼Œé‡‡é›†ä¸­æ–‡ä¹¦ç±ï¼Œäº’è”ç½‘ï¼Œå’Œç™¾ç§‘ç±»æ•°æ®ï¼Œå¹¶é€šè¿‡æ•°æ®æºè´¨é‡åˆ†è¿‡æ»¤å’Œ tf-idf soft dedupingï¼Œä» 20TB æ•°æ®è¿‡æ»¤åˆ° 2TBï¼Œä¿æŒè¯­è¨€å’Œç±»ç›®çš„æ¯”ä¾‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸ŠéšæœºæŠ½æ · 100G æ•°æ®å¼€æº
å¼€æºæ•°æ®é›†
- [ä¸­æ–‡å¼€æºé¢„è®­ç»ƒé›† - 55Gï¼ŒåŒ…å«ä¸­æ–‡ä¹¦ç±ã€ä¸­æ–‡äº’è”ç½‘ã€ä¸­æ–‡ç™¾ç§‘](https://huggingface.co/datasets/TigerResearch/pretrain_zh)
- [è‹±æ–‡å¼€æºé¢„è®­ç»ƒé›† - 51Gï¼ŒåŒ…å«è‹±æ–‡ä¹¦ç±ã€è‹±æ–‡äº’è”ç½‘ã€è‹±æ–‡ç™¾ç§‘](https://huggingface.co/datasets/TigerResearch/pretrain_en)
  
**å¾®è°ƒæ•°æ®**
æ•°æ®ä¸‹è½½é“¾æ¥å’Œæ¸…æ´—è§„åˆ™è§é¡¹ç›®github

å¼€æºæ•°æ®é›†
- æŒ‡ä»¤æ•°æ®é›†, å½“å‰å¼€æº 120W é—®ç­”å¯¹ï¼Œç£ç›˜ç©ºé—´ 1.1G ï¼ˆæ•°æ®é›†å¼€æ”¾åˆ° huggingfaceï¼‰
- é¢†åŸŸæ•°æ®å¼€æ”¾é‡‘èã€æ³•å¾‹ã€ç™¾ç§‘ç›¸å…³é¢†åŸŸæ•°æ®ï¼Œä½œä¸º rethink å¤–éƒ¨æ•°æ®
  

#### æ¨¡å‹ä¸‹è½½
| **Tigerbot\-7B**              | **Bits** | **memory\(GB\)** |
|-------------------------------|----------|------------------|
| tigerbot\-7b\-base            | 16       | 17\.2            |
| tigerbot\-7b\-sft             | 16       | 17\.2            |
| tigerbot\-7b\-sft\-4bit\-128g | 4        | 8\.5             |

| **Tigerbot\-180B\-Research**    | **Bits** | **memory\(GB\)** |
|---------------------------------|----------|------------------|
| tigerbot\-180b\-sft             | 16       | 347\.6           |
| tigerbot\-180b\-sft\-4bit\-128g | 4        | 108\.5           |

### 7. [Fastchat](https://github.com/lm-sys/FastChat)

FastChat æ˜¯ä¸€ä¸ªå¼€æ”¾å¹³å°ï¼Œç”¨äºè®­ç»ƒã€æœåŠ¡å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººã€‚æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

æœ€å…ˆè¿›æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒVicunaã€FastChat-T5ï¼‰çš„æƒé‡ã€è®­ç»ƒä»£ç å’Œè¯„ä¼°ä»£ç ã€‚
å…·æœ‰ Web UI å’Œä¸ OpenAI å…¼å®¹çš„ RESTful API çš„åˆ†å¸ƒå¼å¤šæ¨¡å‹æœåŠ¡ç³»ç»Ÿã€‚


### 8. Vicuna

ç”¨ ShareGPT æ”¶é›†çš„å¯¹è¯æ•°æ®å¾®è°ƒ LLaMAå®ç°   ç”¨GPT-4è¯„ä»·æ•ˆæœ
ä¸æ“…é•¿æ•°å­¦æ¨ç†ã€ç¼–ç ä»»åŠ¡ï¼Œè¯­è¨€é€»è¾‘ä¸Šæ•´ä½“æ˜¯è‹±æ–‡é€»è¾‘ï¼ˆæ¯”å¦‚ä¸­æ–‡ä»»åŠ¡ä¼šå¯¼è‡´çŠ¶è¯­åç½®&å¤¹æ‚è‹±æ–‡å•è¯ç­‰ï¼‰

![Alt text][def4]
ä½¿ç”¨openai [moderation](https://platform.openai.com/docs/guides/moderation/overview) apiæ¥è¿›è¡Œå®¡æ ¸ 

#### æ¦‚è¿°


**æ•°æ®**ï¼š70K å¯¹è¯å¯¹
**è®­ç»ƒ**ï¼šå¢å¼ºäº† Alpaca æä¾›çš„è®­ç»ƒè„šæœ¬ï¼Œä»¥æ›´å¥½åœ°å¤„ç†å¤šè½®å¯¹è¯å’Œé•¿åºåˆ—ã€‚ä¸€å¤©å†…åœ¨ 8 ä¸ª A100 GPU ä¸Šä½¿ç”¨ PyTorch FSDP å®Œæˆ
**ç®—åŠ›éœ€æ±‚**ï¼šè®­ç»ƒ ï¼š8 ä¸ª A100 GPU

å®˜æ–¹[blog](https://lmsys.org/blog/2023-03-30-vicuna/#how-good-is-vicuna)æä¾›äº†ä¸€ä¸ªå¯¹æ¯”è¡¨æ ¼ï¼š
 **Model Name**          | **LLaMA**                                | **Alpaca**                                           | **Vicuna**                                 | **Bard/ChatGPT** |
|-------------------------|------------------------------------------|------------------------------------------------------|--------------------------------------------|------------------|
| Dataset                 | Publicly available datasets\(1T token\) | Self\-instruct from davinci\-003 API\(52K samples\) | User\-shared conversations\(70K samples\) | N/A              |
| Training code           | N/A                                      | Available                                            | Available                                  | N/A              |
| Evaluation metrics      | Academic benchmark                       | Author evaluation                                    | GPT\-4 assessment                          | Mixed            |
| Training cost(7B)   | 82K GPU\-hours                           | \$ 500 (data) + $100 (training)                   | $140(training)                          | N/A              |
| Training cost (13B) | 135K GPU\-hours                          | N/A                                                  | $300 (training)                          | N/A              |

#### æ•°æ®
ä» ShareGPT.com ä½¿ç”¨å…¬å…± API æ”¶é›†çš„å¤§çº¦ 70K ç”¨æˆ·å…±äº«å¯¹è¯
ä¸ºäº†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå°† HTML è½¬æ¢å› markdown å¹¶è¿‡æ»¤æ‰ä¸€äº›ä¸åˆé€‚æˆ–ä½è´¨é‡çš„æ ·æœ¬ï¼Œå°†å†—é•¿çš„å¯¹è¯åˆ†æˆé€‚åˆæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„è¾ƒå°ç‰‡æ®µ


#### è®­ç»ƒè¿‡ç¨‹ 
è®­ç»ƒè„šæœ¬åœ¨ Stanfordâ€™s alpaca çš„åŸºç¡€ä¸Šåšäº†å¦‚ä¸‹æ”¹è¿›ï¼š
- å†…å­˜ä¼˜åŒ–ï¼šä¸ºäº†ä½¿ Vicuna èƒ½å¤Ÿç†è§£é•¿ä¸Šä¸‹æ–‡ï¼Œå°†æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä»ç¾Šé©¼ä¸­çš„ 512 æ‰©å±•åˆ° 2048ï¼Œè¿™å¤§å¤§å¢åŠ äº† GPU å†…å­˜éœ€æ±‚ã€‚é€šè¿‡åˆ©ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œé—ªå­˜æ³¨æ„åŠ›æ¥è§£å†³å†…å­˜å‹åŠ›ã€‚ 
- å¤šè½®å¯¹è¯ï¼šè°ƒæ•´è®­ç»ƒæŸå¤±ä»¥è€ƒè™‘å¤šè½®å¯¹è¯ï¼Œå¹¶ä»…æ ¹æ®èŠå¤©æœºå™¨äººçš„è¾“å‡ºè®¡ç®—å¾®è°ƒæŸå¤±ã€‚ 
- é€šè¿‡ Spot Instanceé™ä½æˆæœ¬ï¼š40 å€å¤§çš„æ•°æ®é›†å’Œ 4 å€çš„è®­ç»ƒåºåˆ—é•¿åº¦å¯¹è®­ç»ƒè´¹ç”¨æå‡ºäº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ vicunaä½¿ç”¨ **SkyPilot managed spot**æ¥é™ä½æˆæœ¬ï¼ˆåˆ©ç”¨æ›´ä¾¿å®œçš„ç‚¹å®ä¾‹ä»¥åŠè‡ªåŠ¨æ¢å¤æŠ¢å å’Œè‡ªåŠ¨åŒºåŸŸåˆ‡æ¢ï¼‰ã€‚ è¯¥è§£å†³æ–¹æ¡ˆå°† 7B æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä» 500 ç¾å…ƒå‰Šå‡è‡³ 140 ç¾å…ƒå·¦å³ï¼Œå°† 13B æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä» 1000 ç¾å…ƒå·¦å³å‰Šå‡è‡³ 300 ç¾å…ƒã€‚ ï¼ˆAlpaServe ï¼‰

### æ¨¡å‹ 
**åœ°å€**: https://huggingface.co/lmsys

è¿˜æ˜¯deltaçš„æ ¼å¼å‘å¸ƒæƒé‡ ç±»ä¼¼è¿™æ ·è½¬æ¢ â†“
éœ€è¦èµ„æºï¼š30GB of CPU RAMï¼ˆ7bï¼‰ 60GB of CPU RAMï¼ˆ13bï¼‰æä¾›äº† Low CPU Memory `Conversion æ–¹å¼ï¼š--low-cpu-mem 
python3 -m fastchat.model.apply_delta \
    --base-model-path /path/to/llama-7b \
    --target-model-path /path/to/output/vicuna-7b \
    --delta-path lmsys/vicuna-7b-delta-v1.1  
--low-cpu-mem`

### 9. [Koala: A Dialogue Model for Academic Research è€ƒæ‹‰å¯¹è¯æ¨¡å‹](https://bair.berkeley.edu/blog/2023/04/03/koala/)

![Alt text](./figure/image3.png)

### 10.[Fireflyï¼ˆæµè¤ï¼‰: ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹](https://github.com/yangjianxin1/Firefly)

**å®¶æ—** : bloom 
- å¼€æºQLoRAçš„è®­ç»ƒä»£ç ï¼Œä½¿ç”¨ä¸€å¼ æ˜¾å¡å¯¹bloom-7b1è¿›è¡Œå¾®è°ƒï¼Œå¼€æºfirefly-7b1-qlora-v0.1æ¨¡å‹ ã€‚
https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M
æœ¬æ•°æ®åº”ç”¨äºé¡¹ç›®ï¼šFireflyï¼ˆæµè¤ï¼‰: ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹ ï¼Œè®­ç»ƒåå¾—åˆ°çš„æ¨¡å‹[firefly-1b4](https://huggingface.co/YeungNLP/firefly-bloom-1b4-sft)

### 11. Stanford Alpaca: An Instruction-following LLaMA Model 

### 12. [GPT4all](https://github.com/nomic-ai/gpt4all)

## å¤šæ¨¡æ€ Model & å¤šæ¨¡æ€ä»»åŠ¡å®ç°æ–¹æ¡ˆ
#### 1. [I-JEPA (the Image-based Joint-Embedding Predictive Architecture)](https://arxiv.org/pdf/2301.08243.pdf)
metaå¼€æº
é¦–ä¸ªä¸–ç•ŒèƒŒæ™¯çŸ¥è¯†å­¦ä¹ æ¨¡å‹ 
å›¾åƒè¡¥å…¨ä»»åŠ¡ https://github.com/facebookresearch/ijepa

### 2. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding
https://arxiv.org/abs/2306.02858

### 3. MM-React 

### 4. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models



## å¤§æ¨¡å‹æœ‰å…³é¡¹ç›®å®ç° å·¥å…·&å‚è€ƒé¡¹ç›®

### ä¼˜åŒ–æ–¹å‘
1. ä¼˜åŒ– text_split ç®—æ³•ï¼Œä½¿åŒ¹é…å‡ºçš„ç»“æœä½œä¸ºä¸Šä¸‹æ–‡æ—¶èƒ½å¤Ÿæä¾›æ›´åˆç†çš„æ¨ç†/å›ç­”ä¾æ®ï¼›
2. ä¼˜åŒ– embedding æ¨¡å‹ï¼Œæå‡è¯­ä¹‰å‘é‡åŒ–çš„æ•ˆæœï¼Œä½¿å¾—è¯­ä¹‰åŒ¹é…è¿‡ç¨‹ä¸­èƒ½å¤ŸåŒ¹é…å‡ºæœ€æ»¡è¶³è¦æ±‚çš„æ–‡æœ¬æ®µè½ä½œä¸ºä¸Šä¸‹æ–‡ï¼›
3. ä¼˜åŒ– LLM æ¨¡å‹ï¼Œä½¿å¾—ç»™å®šæé—®ç›¸åŒæƒ…å†µä¸‹ï¼Œå¾—åˆ°æ›´ç†æƒ³çš„æ¨ç†/å›ç­”ç»“æœã€‚

### æ•°æ®å‡†å¤‡
- åˆ†è¯å·¥å…· GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation. åœ¨chinese-llama&aplaceé¡¹ç›®ä¸­ç”¨äºè¯è¡¨æ‰©å……
- ç½‘ç«™çˆ¬è™«å·¥å…· https://apify.com/apify/website-content-crawler
- PDFç­‰æ–‡æœ¬è§£æç»„ä»¶ï¼š
  1ã€PDFminer
  PDFMineræ˜¯ä¸€ä¸ªPythonçš„PDFè§£æå™¨ï¼Œå¯ä»¥ä»PDFæ–‡æ¡£ä¸­æå–ä¿¡æ¯ã€‚ä¸å…¶ä»–PDFç›¸å…³çš„å·¥å…·ä¸åŒï¼Œå®ƒä¾§é‡çš„æ˜¯è·å–å’Œåˆ†ææ–‡æœ¬æ•°æ®ã€‚PDFMinerå…è®¸è·å–æŸä¸€é¡µä¸­æ–‡æœ¬çš„å‡†ç¡®ä½ç½®å’Œä¸€äº›è¯¸å¦‚å­—ä½“ã€è¡Œæ•°çš„ä¿¡æ¯ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªPDFè½¬æ¢å™¨ï¼Œå¯ä»¥æŠŠPDFæ–‡ä»¶è½¬æ¢æˆHTMLç­‰æ ¼å¼ã€‚è¿˜æœ‰ä¸€ä¸ªæ‰©å±•çš„PDFè§£æå™¨ï¼Œå¯ä»¥ç”¨äºé™¤æ–‡æœ¬åˆ†æä»¥å¤–çš„å…¶ä»–ç”¨é€”ã€‚
  PDFMinerå†…ç½®ä¸¤ä¸ªå·¥å…·ï¼špdf2txt.pyå’Œdumppdf.pyï¼š
  pdf2txt.pyä»PDFæ–‡ä»¶ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ã€‚ä½†ä¸èƒ½è¯†åˆ«ç”»æˆå›¾ç‰‡çš„æ–‡æœ¬ï¼Œè¿™éœ€è¦ç‰¹å¾è¯†åˆ«ã€‚å¯¹äºåŠ å¯†çš„PDFä½ éœ€è¦æä¾›ä¸€ä¸ªå¯†ç æ‰èƒ½è§£æï¼Œå¯¹äºæ²¡æœ‰æå–æƒé™çš„PDFæ–‡æ¡£ä½ å¾—ä¸åˆ°ä»»ä½•æ–‡æœ¬ã€‚
  åœ°å€ï¼šhttps://pdfminersix.readthedocs.io
  2ã€pdfplumber
  pdfplumberåº“æŒ‰é¡µå¤„ç† pdf ï¼Œè·å–é¡µé¢æ–‡å­—ï¼Œæå–è¡¨æ ¼ç­‰æ“ä½œã€‚
  åœ°å€ï¼šhttps://github.com/jsvine/pdfplumber
  3ã€pypdf2
  PyPDF2æ˜¯ä¸€ä¸ªçº¯Python PDFåº“ï¼Œå¯ä»¥è¯»å–æ–‡æ¡£ä¿¡æ¯ï¼ˆæ ‡é¢˜ï¼Œä½œè€…ç­‰ï¼‰ã€å†™å…¥ã€åˆ†å‰²ã€åˆå¹¶PDFæ–‡æ¡£ï¼Œå®ƒè¿˜å¯ä»¥å¯¹pdfæ–‡æ¡£è¿›è¡Œæ·»åŠ æ°´å°ã€åŠ å¯†è§£å¯†ç­‰ã€‚
  åœ°å€ï¼šhttps://pythonhosted.org/PyPDF2
  4ã€pymupdf
  PyMuPDFæ˜¯æ”¯æŒMuPDFçš„Pythonç»‘å®šã€‚
  ä½¿ç”¨PyMuPDFï¼Œå¯ä»¥è®¿é—®æ‰©å±•åä¸ºâ€œ.pdfâ€ã€â€œ.xpsâ€ã€â€œ.oxpsâ€ã€â€œ.cbzâ€ã€â€œ.fb2â€æˆ–â€œ.epubâ€ã€‚æ­¤å¤–ï¼Œå¤§çº¦10ç§æµè¡Œçš„å›¾åƒæ ¼å¼ä¹Ÿå¯ä»¥åƒæ–‡æ¡£ä¸€æ ·å¤„ç†:â€œ.pngâ€ï¼Œâ€œ.jpgâ€ï¼Œâ€œ.bmpâ€ï¼Œâ€œ.tiffâ€ç­‰ã€‚
  åœ°å€ï¼šhttps://pypi.org/project/PyMuPDF/
  5ã€ppstructure
  PP-StructureV2æ”¯æŒå¯¹å›¾ç‰‡/pdfå½¢å¼çš„æ–‡æ¡£è¿›è¡Œç‰ˆé¢åˆ†æï¼Œå¯ä»¥åˆ’åˆ†æ–‡å­—ã€æ ‡é¢˜ã€è¡¨æ ¼ã€å›¾ç‰‡ã€å…¬å¼ç­‰åŒºåŸŸï¼›æ”¯æŒé€šç”¨çš„ä¸­è‹±æ–‡è¡¨æ ¼æ£€æµ‹ä»»åŠ¡ï¼›æ”¯æŒè¡¨æ ¼åŒºåŸŸè¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œæœ€ç»ˆç»“æœè¾“å‡ºExcelæ–‡ä»¶ï¼›
  PP-Structureæ˜¯PaddleOCRå›¢é˜Ÿè‡ªç ”çš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ›´å¥½çš„å®Œæˆç‰ˆé¢åˆ†æã€è¡¨æ ¼è¯†åˆ«ç­‰æ–‡æ¡£ç†è§£ç›¸å…³ä»»åŠ¡ã€‚
  ç‰ˆé¢åˆ†æä»»åŠ¡ä¸­ï¼Œå›¾åƒé¦–å…ˆç»è¿‡ç‰ˆé¢åˆ†ææ¨¡å‹ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºæ–‡æœ¬ã€è¡¨æ ¼ã€å›¾åƒç­‰ä¸åŒåŒºåŸŸï¼Œéšåå¯¹è¿™äº›åŒºåŸŸåˆ†åˆ«è¿›è¡Œè¯†åˆ«ï¼Œå¦‚ï¼Œå°†è¡¨æ ¼åŒºåŸŸé€å…¥è¡¨æ ¼è¯†åˆ«æ¨¡å—è¿›è¡Œç»“æ„åŒ–è¯†åˆ«ï¼Œå°†æ–‡æœ¬åŒºåŸŸé€å…¥OCRå¼•æ“è¿›è¡Œæ–‡å­—è¯†åˆ«ï¼Œæœ€åä½¿ç”¨ç‰ˆé¢æ¢å¤æ¨¡å—å°†å…¶æ¢å¤ä¸ºä¸åŸå§‹å›¾åƒå¸ƒå±€ä¸€è‡´çš„wordæˆ–è€…pdfæ ¼å¼çš„æ–‡ä»¶ã€‚
  åœ°å€ï¼šhttps://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppstructure

- æ–‡æœ¬å‘é‡åŒ–ç»„ä»¶
  1ã€text2vec
  å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚
  åœ°å€ï¼šhttps://github.com/shibing624/text2vec
     2ã€SGPT
  SGPTï¼šGPT Sentence Embeddings for Semantic Searchï¼Œæ˜¯ä¸€ä¸ªä½¿ç”¨GPTæ¶æ„ç”Ÿæˆembeddingçš„æ–¹æ³•ï¼Œä¸BERTæ¨¡å¼ä¸åŒã€‚
  åœ°å€ï¼šhttps://arxiv.org/abs/2202.08904
   3ã€M3E
  Moka Massive Mixed Embeddingçš„ç¼©å†™ï¼Œç”±MokaAIè®­ç»ƒï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniemï¼Œè¯„æµ‹BenchMarkä½¿ç”¨MTEB-zhï¼Œé€šè¿‡åƒä¸‡çº§ (2200w+) çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚


### æ•°æ®å¤„ç†
- Vectara  æ•°æ®ç­›é€‰å’ŒåŒ¹é…å·¥å…· github:https://github.com/vectara/vectara-answer
æ•ˆæœï¼š åŒ¹é…æ•ˆæœé’ˆå¯¹äºä¸­æ–‡ä¸å¤ªå¥½ï¼Œä½†æ˜¯è¯†åˆ«æ–‡å­—çš„èƒ½åŠ›ï¼Œç•Œé¢çš„è®¾ç½®ï¼Œæ–‡å­—çš„ç« èŠ‚è¯†åˆ«ç­‰å¾ˆå¥½ï¼Œæ•°æ®åº“å¼ç®¡ç†ï¼Œé€‚åˆæ‰¹é‡å¤„ç†æ–‡æ¡£
	![Alt text](./figure/image4.png)

### æ¨¡å‹ä¸‹è½½&è½¬æ¢ï¼Œè®­ç»ƒ&å¾®è°ƒ
- Training Open Instruction-following Language Models https://github.com/allenai/open-instruct
- å„ç§æ¨¡å‹çš„ä½èµ„æºé‡åŒ–å’Œéƒ¨ç½² https://github.com/jianzhnie/Efficient-Tuning-LLMs
- Chinese-vicuna é¡¹ç›®æä¾›  Chinese-Vicuna/readme_zh.md at master Â· Facico/Chinese-Vicuna
- Chinese-Llama-Aplaceé¡¹ç›®æä¾›:
  
| **æ–¹å¼** | **é€‚ç”¨åœºæ™¯**                            | **æ•™ç¨‹** |
|--------|-------------------------------------|--------|
| åœ¨çº¿è½¬æ¢   | Colabç”¨æˆ·å¯åˆ©ç”¨æœ¬é¡¹ç›®æä¾›çš„notebookè¿›è¡Œåœ¨çº¿è½¬æ¢å¹¶é‡åŒ–æ¨¡å‹ | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E5%9C%A8%E7%BA%BF%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2)     |
| æ‰‹åŠ¨è½¬æ¢   | ç¦»çº¿æ–¹å¼è½¬æ¢ï¼Œç”Ÿæˆä¸åŒæ ¼å¼çš„æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œé‡åŒ–æˆ–è¿›ä¸€æ­¥ç²¾è°ƒ       | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2)     |
- [MeZO: Fine-Tuning Language Models with Just Forward Passes](https://security.feishu.cn/link/safety?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.17333.pdf&scene=ccm&logParams=%7B%22location%22%3A%22ccm_default%22%7D&lang=zh)
  å†…å­˜é«˜æ•ˆçš„é›¶é˜¶ä¼˜åŒ–å™¨ï¼Œå®ç°ä¸æ¨ç†é˜¶æ®µç›¸åŒçš„å†…å­˜å ç”¨å¤§å°
- Fairseq https://github.com/facebookresearch/fairseq
- Deepseed

### æ¨¡å‹é‡åŒ–æ¨ç†å’Œéƒ¨ç½²

- æ¨¡å‹é‡åŒ– https://github.com/qwopqwop200/GPTQ-for-LLaMa
- Chinese-Llama-Aplaceé¡¹ç›®æ•´ç†   å…·ä½“å†…å®¹è¯·å‚è€ƒæœ¬é¡¹ç›® >>> ğŸ“š https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E4%B8%8E%E9%83%A8%E7%BD%B2


| **æ¨ç†å’Œéƒ¨ç½²æ–¹å¼**             | **ç‰¹ç‚¹**                            | **å¹³å°** | **CPU** | **GPU** | **é‡åŒ–åŠ è½½** | **å›¾å½¢ç•Œé¢** | **æ•™ç¨‹** |
|-------------------------|-----------------------------------|--------|---------|---------|----------|----------|--------|
| [llama\.cpp](https://github.com/ggerganov/llama.cpp)              | ä¸°å¯Œçš„é‡åŒ–é€‰é¡¹å’Œé«˜æ•ˆæœ¬åœ°æ¨ç†                    | é€šç”¨     | âœ…       | âœ…       | âœ…        | âŒ        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2)     |
| ğŸ¤—[Transformers](https://github.com/huggingface/transformers)         | åŸç”Ÿtransformersæ¨ç†æ¥å£                | é€šç”¨     | âœ…       | âœ…       | âœ…        | âœ…        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8Transformers%E6%8E%A8%E7%90%86)     |
| [text\-generation\-webui](https://github.com/oobabooga/text-generation-webui) | å‰ç«¯Web UIç•Œé¢çš„éƒ¨ç½²æ–¹å¼                   | é€šç”¨     | âœ…       | âœ…       | âœ…        | âœ…        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8text-generation-webui%E6%90%AD%E5%BB%BA%E7%95%8C%E9%9D%A2)     |
| [LlamaChat](https://github.com/alexrozanski/LlamaChat)               | macOSä¸‹çš„å›¾å½¢äº¤äº’ç•Œé¢ï¼ˆéœ€æ­é…llama\.cppæ¨¡å‹ï¼‰    | MacOS  | âœ…       | âŒ       | âœ…        | âœ…        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8LlamaChat%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%EF%BC%88macOS%EF%BC%89)     |
| [LangChain](https://github.com/hwchase17/langchain)               | LLMåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œé€‚ç”¨äºè¿›è¡ŒäºŒæ¬¡å¼€å‘               | é€šç”¨     | âœ…â€       | âœ…       | âœ…â€        | âŒ        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%B8%8ELangChain%E8%BF%9B%E8%A1%8C%E9%9B%86%E6%88%90)     |
| [privateGPT](https://github.com/imartinez/privateGPT)              | åŸºäºLangChainçš„å¤šæ–‡æ¡£æœ¬åœ°é—®ç­”æ¡†æ¶             | é€šç”¨     | âœ…       | âœ…       | âœ…        | âŒ        | [é“¾æ¥](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8privateGPT%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%96%87%E6%A1%A3%E9%97%AE%E7%AD%94)     |
| [Colab Gradio Dem](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb)       | åœ¨Colabä¸­å¯åŠ¨åŸºäºGradioçš„äº¤äº’å¼WebæœåŠ¡ï¼Œä½“éªŒæ¨¡å‹æ•ˆæœ | é€šç”¨     | âœ…       | âœ…       | âœ…        | âŒ        | [é“¾æ¥](https://colab.research.google.com/github/ymcui/Chinese-LLaMA-Alpaca/blob/main/notebooks/gradio_web_demo.ipynb)     |


### æ¡†æ¶
- Langchain
- llama-index
- YuLan-IR/RETA-LLM at main Â· RUC-GSAI/YuLan-IR  YuLan-RETA-LLMï¼šåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨æ£€ç´¢
  

### æ•ˆæœè¯„ä¼°
* Evaluating instruction following on more user-oriented data
* C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models  

### å¸¸è§bugè§£å†³
åœ¨python=3.9ç¯å¢ƒä¸­è¿è¡Œlangchianæ—¶å€™é‡åˆ°not a classé”™è¯¯ï¼š

`pip install typing-inspect==0.8.0 typing_extensions==4.5.0`


## å‚è€ƒèµ„æ–™

### Paper
- A Survey of Large Language Models
- Transformer models: an introduction and catalogâ€Šâ€”â€Š2023 Edition

### å·¥å…·ç½‘å€
- https://civitai.com/
  
### ä¸€äº›å€¼å¾—å…³æ³¨çš„issues
- Retrieval-Augmented Generation æ£€é”™å¢å¼ºæŠ€æœ¯ 
RAGâ€”â€”ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆæ„å»ºç‰¹å®šè¡Œä¸šçš„å¤§å‹è¯­è¨€æ¨¡å‹
- æ˜¯å¦è¦è¿›è¡Œè¯è¡¨æ‰©å……  [å¸¸è§é—®é¢˜](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#%E9%97%AE%E9%A2%984%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%89%A9%E5%85%85%E8%AF%8D%E8%A1%A8%E7%9B%B4%E6%8E%A5%E5%9C%A8%E5%8E%9F%E7%89%88llama%E4%B8%8A%E7%94%A8%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8D%E8%A1%8C%E5%90%97)
â€œLLaMAè¯è¡¨ä¸­ä»…åŒ…å«å¾ˆå°‘çš„ä¸­æ–‡å­—ç¬¦ï¼Œæ‰€ä»¥åœ¨åˆ‡è¯æ—¶ä¼šæŠŠä¸­æ–‡åˆ‡åœ°æ›´ç¢ï¼Œéœ€è¦å¤šä¸ªbyte tokenæ‰èƒ½æ‹¼æˆä¸€ä¸ªå®Œæ•´çš„æ±‰å­—ï¼Œè¿›è€Œå¯¼è‡´ä¿¡æ¯å¯†åº¦é™ä½ã€‚æ¯”å¦‚ï¼Œåœ¨æ‰©å±•è¯è¡¨åçš„æ¨¡å‹ä¸­ï¼Œå•ä¸ªæ±‰å­—å€¾å‘äºè¢«åˆ‡æˆ1ä¸ªtokenï¼Œè€Œåœ¨åŸç‰ˆLLaMAä¸­å¯èƒ½å°±éœ€è¦2-3ä¸ªæ‰èƒ½ç»„åˆæˆä¸€ä¸ªæ±‰å­—ï¼Œæ˜¾è‘—é™ä½ç¼–è§£ç çš„æ•ˆç‡ã€‚â€  ï¼ˆè¿™å¯èƒ½å°±æ˜¯åœ¨ç”¨vicunaçš„æ—¶å€™è™½ç„¶æ˜¯2048toknsä½†æ˜¯å®é™…ç”¨èµ·æ¥ä¸Šä¸‹æ–‡å¾ˆçŸ­çš„åŸå› ï¼‰

- â€œå‚ç›´é¢†åŸŸçš„æ•°æ®é‡è¿‡å¤§ä¼šå¯¼è‡´æ¨¡å‹å¤±å»æ³›ç”¨èƒ½åŠ›ï¼Œç”šè‡³å¤±å»è¯­è¨€èƒ½åŠ›ï¼Œå³è¯´äººè¯çš„èƒ½åŠ›â€ï¼šhttps://github.com/Facico/Chinese-Vicuna/issues/68
  
- åŒä¸Š ptuning loraç­‰å°å‚æ•°å¾®è°ƒæ–¹æ³•å®¹æ˜“äº§ç”Ÿçš„ç¾éš¾æ€§é—å¿˜é—®é¢˜
â€œæœ€å¥½èƒ½å¤Ÿåœ¨å¾®è°ƒè¯­æ–™ä¸­ä¹ŸåŠ å…¥é€šç”¨å­¦ä¹ è¯­æ–™ä¸€èµ·å¾®è°ƒï¼Œé¿å…äº§ç”Ÿå¯¹å¾®è°ƒè¯­æ–™æå¤§çš„åå‘ï¼Œåœ¨instruct gptè®ºæ–‡ä¸­ä¹Ÿæåˆ°åœ¨å¼ºåŒ–å­¦ä¹ ppoçš„æ—¶å€™æ¨¡å‹ä¹Ÿä¼šå¾ˆå®¹æ˜“å¯¹äºppoæ•°æ®æ‹Ÿåˆï¼Œé™ä½æ¨¡å‹é€šç”¨è‡ªç„¶è¯­è¨€ä»»åŠ¡èƒ½åŠ›ï¼Œæ‰€ä»¥åœ¨ppo lossä¸­åŠ å…¥äº†SFTæ¢¯åº¦å’Œé¢„è®­ç»ƒæ¢¯åº¦æ¥ç¼“è§£è¿™ç§é—å¿˜é—®é¢˜ã€‚â€

- ä¼˜åŒ–æ–¹å‘ https://github.com/imClumsyPanda/langchain-ChatGLM/issues/14
  
- fintuningæ•°æ®å‡†å¤‡é—®é¢˜  https://github.com/THUDM/ChatGLM-6B/issues/364
  doc2query/msmarco-chinese-mt5-base-v1ï¼Œ æ ¹æ®docç”Ÿæˆé—®é¢˜

### LLMæˆé•¿è·¯çº¿å›¾

ç¤ºä¾‹ï¼šhttps://github.com/LAION-AI/Open-Assistant#the-plan

[def]: ./figure/image0.png
[def2]: ./figure/image.png
[def3]: ./figure/image1.png
[def4]: ./figure/image2.png