---
description: ä¼¦æ•¦å¤§å­¦ã€MetaAIã€StabilityAIè”åˆå‘å¸ƒ70é¡µç»¼è¿°ï¼Œç›˜ç‚¹å¤§æ¨¡å‹çš„16å¤§æŒ‘æˆ˜
---

# ğŸ‘ Challenges and Applications of Large Language Models

è®ºæ–‡é“¾æ¥ï¼š[https://arxiv.org/abs/2307.10169](https://arxiv.org/abs/2307.10169)

æœ¬æ–‡å…³æ³¨ä¸¤ä¸ªé—®é¢˜ï¼š

(1) Challenges: What problems remain unresolved?  -- ä»è®¾è®¡ã€è¡Œä¸ºã€ç§‘å­¦ä¸‰ä¸ªæ–¹é¢å±•å¼€

(2) Applications: Where are LLMs currently being applied, and how are the challenges constraining them? -- ä»‹ç»äº†èŠå¤©æœºå™¨äººã€è®¡ç®—ç”Ÿç‰©å­¦ã€è®¡ç®—æœºç¼–ç¨‹ã€åˆ›é€ æ€§å·¥ä½œã€çŸ¥è¯†å·¥ä½œã€æ³•å¾‹ã€åŒ»å­¦ã€æ¨ç†ã€æœºå™¨äººå’Œç¤¾ä¼šç§‘å­¦ç­‰é¢†åŸŸã€‚

## Challenges

<figure><img src="../../.gitbook/assets/image (3).png" alt="" width="563"><figcaption></figcaption></figure>

### Challenge 1ï¼šUnfathomable Datasets æ·±ä¸å¯æµ‹çš„æ•°æ®é›†ï¼šThe size of modern pre-training datasets renders it impractical for any individual to read or conduct quality assessments on the encompassed documents thoroughly. æ•°æ®è§„æ¨¡å¤ªå¤§ï¼Œä»»ä½•ä¸ªäººéƒ½æ— æ³•å½»åº•é˜…è¯»æ‰€åŒ…å«çš„æ–‡æ¡£æˆ–å¯¹å…¶è¿›è¡Œè´¨é‡è¯„ä¼°

æ‰©å¤§é¢„è®­ç»ƒæ•°æ®é‡ä¸€ç›´æ˜¯ä¸ºLLMé…å¤‡é€šç”¨åŠŸèƒ½çš„ä¸»è¦é©±åŠ¨åŠ›ä¹‹ä¸€ã€‚ é¢„è®­ç»ƒæ•°æ®é›†çš„å¤§å°å¾ˆå¿«å°±è¶…è¿‡äº†å¤§å¤šæ•°äººç±»å›¢é˜Ÿå¯ä»¥æ‰‹åŠ¨è¿›è¡Œè´¨é‡æ£€æŸ¥çš„æ–‡æ¡£æ•°é‡ã€‚ ç›¸åï¼Œå¤§å¤šæ•°æ•°æ®æ”¶é›†è¿‡ç¨‹ä¾èµ–äºæœ‰å…³æ•°æ®æºå’Œè¿‡æ»¤çš„å¯å‘å¼ï¼ˆheuristicsï¼‰æ–¹æ³•ã€‚

è¿™ä¸€éƒ¨åˆ†æ¢è®¨äº†è¿™äº›å¯å‘æ³•çš„ä¸åˆ©åæœï¼Œä»¥åŠè®¸å¤šæ¨¡å‹å®è·µè€…å¯¹å…¶æ¨¡å‹è®­ç»ƒæ‰€ç”¨çš„æ•°æ®åªæœ‰æ¨¡ç³Šç†è§£çš„ç°å®ã€‚&#x20;

#### **Near-Duplicates**&#x20;

**è¿‘ä¼¼é‡å¤æ•°æ®**ä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼Œè€Œè¿‡æ»¤è¿™äº›æ•°æ®å¾ˆå›°éš¾ã€‚ç°æœ‰æ–¹æ³•: MinHash; NearDup;  SemDeDup; Kaddour æå‡ºçš„ [The MiniPile Challenge for Data-Efficient Language Models](https://arxiv.org/abs/2304.0844)...

#### **Benchmark Data Contamination**

å½“è®­ç»ƒæ•°æ®é›†åŒ…å«æ¥è‡ªæˆ–ç±»ä¼¼äºè¯„ä¼°æµ‹è¯•é›†çš„æ•°æ®æ—¶ï¼Œå°±ä¼šå‘ç”Ÿ**åŸºå‡†æ•°æ®æ±¡æŸ“**ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½æŒ‡æ ‡å¤¸å¤§ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥è®°ä½æµ‹è¯•æ•°æ®å¹¶åœ¨æµ‹è¯•æœŸé—´ç®€å•åœ°å°†å…¶regurgitateå›æ¥ã€‚åœ¨å®è·µä¸­æŸ¥æ‰¾å¹¶åˆ é™¤æ‰€æœ‰è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é‡å æ˜¯å¾ˆå›°éš¾çš„ã€‚

å‚è€ƒæ–‡çŒ®ï¼š

[Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) GPT-3 ä½œè€… Brown ç­‰äººï¼Œå°†é‡å ç¤ºä¾‹å®šä¹‰ä¸ºä¸é¢„è®­ç»ƒé›†ä¸­çš„ä»»ä½•å…¶ä»–ç¤ºä¾‹å…±äº«è‡³å°‘ 13 ä¸ªè¿ç»­å•è¯çš„ç¤ºä¾‹ã€‚å¦‚æœä¸€ä¸ªç¤ºä¾‹å°‘äº 13 ä¸ªå•è¯ï¼Œå¹¶ä¸”ä¸å¦ä¸€ä¸ªç¤ºä¾‹å…±äº«æ‰€æœ‰å•è¯ï¼Œåˆ™ä»–ä»¬ä¼šè®¤ä¸ºè¯¥ç¤ºä¾‹é‡å ã€‚

[Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus](https://arxiv.org/abs/2104.08758) ä¸­åœ¨ç½‘ç»œçˆ¬å–çš„ C4 è¯­æ–™åº“ä¸­æœç´¢æµ‹è¯•æ•°æ®ï¼Œä½†æµ‹é‡ç²¾ç¡®åŒ¹é…ï¼Œå¹¶å¯¹å¤§å†™å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œæ ‡å‡†åŒ–ã€‚ä»–ä»¬å‘ç°æ–‡æœ¬ç”Ÿæˆå’ŒçŸ¥è¯†å®Œæˆä»»åŠ¡å­˜åœ¨å„ç§è¾“å…¥å’Œæ ‡ç­¾æ±¡æŸ“ï¼›ä»¥åŠ GLUE åŸºå‡†çš„ä»…è¾“å…¥æ±¡æŸ“ã€‚ä»–ä»¬è®¤ä¸ºï¼Œæµ‹è¯•æ•°æ®å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼æœ€ç»ˆå‡ºç°åœ¨ Common Crawlï¼ˆC4 çš„åŸå§‹è½¬å‚¨æºï¼‰å¿«ç…§ä¸­ï¼šç»™å®šçš„æµ‹è¯•é›†æ˜¯ä»ç½‘ç»œæ–‡æœ¬æ„å»ºçš„ï¼Œæˆ–è€…æ˜¯åœ¨åˆ›å»ºåä¸Šä¼ çš„ã€‚

[Did ChatGPT cheat on your test?](https://hitz-zentroa.github.io/lm-contamination/blog/) è¦æ±‚ ChatGPT ç”Ÿæˆå­¦æœ¯åŸºå‡†å®ä¾‹ï¼Œå‘ç°å®ƒå·²ç»è®°ä½äº†å¤šä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬ä¸€äº›æµ‹è¯•åˆ†å‰²ã€‚

[Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks](https://arxiv.org/abs/2305.10160) æå‡ºä¸‰ç§å‡è½»æ±¡æŸ“çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬encryptionå’Œtraining exclusion controlsã€‚

#### **Personally Identifiable Information (PII)**

åœ¨é¢„è®­ç»ƒè¯­æ–™åº“ä¸­å‘ç°äº†è¯¸å¦‚ç”µè¯å·ç å’Œç”µå­é‚®ä»¶åœ°å€ä¹‹ç±»çš„ä¿¡æ¯ï¼Œå¯¼è‡´æç¤ºè¿‡ç¨‹ä¸­çš„éšç§æ³„éœ²ã€‚

#### Pre-Training Domain Mixtures

ä¸€äº›ç ”ç©¶ä¸»å¼ é¢„è®­ç»ƒè¯­æ–™åº“çš„å¤šæ ·æ€§ï¼Œè®¸å¤šæµè¡Œçš„è¯­æ–™åº“éƒ½éµå¾ªè¿™ä¸€ç‚¹ï¼Œå°†æ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®é›†è¿æ¥èµ·æ¥ï¼Œç„¶è€Œï¼Œå¯¹äºå¼ºå¤§çš„ä¸‹æ¸¸æ€§èƒ½éœ€è¦å¤šå°‘ä¸åŒæ¥æºçš„æ•°æ®é‡ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

<figure><img src="../../.gitbook/assets/image (2).png" alt=""><figcaption><p>Table 1: <strong>Overview of Selected Pre-Training Datasets.</strong> Over the years, pre-training datasets have become more unfathomable: they grew rapidly in size and diversity, and not all datasets are publicly available (we do not include datasets that have very little or no information available about them). Unless stated otherwise, the natural language is in English. âˆ— We report the number of tokens as provided by the respective paper based on their proposed tokenization scheme.é¢„è®­ç»ƒæ•°æ®é›†å˜å¾—æ›´åŠ æ·±ä¸å¯æµ‹ï¼šå®ƒä»¬çš„å¤§å°å’Œå¤šæ ·æ€§è¿…é€Ÿå¢é•¿ï¼Œå¹¶ä¸”å¹¶éæ‰€æœ‰æ•°æ®é›†éƒ½æ˜¯å…¬å¼€å¯ç”¨çš„.</p></figcaption></figure>

#### **Fine-Tuning Task Mixtures**

ç¡®å®šå¾®è°ƒä»»åŠ¡æ··åˆä»¥ä¾¿åœ¨è®¸å¤šä¸åŒçš„ä»»åŠ¡ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œé€šå¸¸æ¯ä¸ªä»»åŠ¡çš„ç¤ºä¾‹ç›¸å¯¹è¾ƒå°‘ã€‚è¿™ç§æŠ€æœ¯ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºmultitask-prompted fine-tuned(MTLM å¤šä»»åŠ¡æç¤ºå¾®è°ƒ)ï¼Œå·²ç»è¯æ˜äº†åœ¨åªéœ€å¾ˆå°‘çš„é¢å¤–è®­ç»ƒè®¡ç®—çš„æƒ…å†µä¸‹æ˜¾ç€çš„æ³›åŒ–æ”¹è¿›ã€‚

å¦‚ä½•å¾ˆå¥½åœ°å¹³è¡¡ä»»åŠ¡æ•°æ®é›†ä»ç„¶ä¸æ¸…æ¥šã€‚ç°åœ¨çš„ä¸€äº›åšæ³•æ˜¯å»æ¨¡ä»¿é—­æºmodelçš„æ•°æ®é…æ¯”ï¼Œä½†æ˜¯ç»è¿‡å¾®è°ƒçš„å¼€æºæ¨¡å‹å’Œé—­æºæ¨¡å‹ä¹‹é—´ä»ç„¶å­˜åœ¨å·¨å¤§çš„èƒ½åŠ›å·®è·ï¼Œè¿™æ¿€åŠ±äº†æœªæ¥ç ”ç©¶æ›´å¥½çš„æ¨¡ä»¿ï¼ˆé—­æºæ¨¡å‹ï¼‰æ•°æ®çš„å·¥ä½œã€‚

### Challenge2ï¼šTokenizer-Reliance å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œè¿è¡Œé€šå¸¸ä¾èµ–äºç‰¹å®šçš„åˆ†è¯å™¨ï¼Œè¿™å¯èƒ½å¯¹å…¶æ€§èƒ½å’Œé€‚åº”æ€§äº§ç”Ÿå½±å“ã€‚ --åˆ†è¯å™¨å¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚è®¡ç®—å¼€é”€ã€è¯­è¨€ä¾èµ–æ€§ã€æ–°è¯çš„å¤„ç†ã€å›ºå®šè¯æ±‡é‡ã€ä¿¡æ¯ä¸¢å¤±å’Œä½äººç±»å¯è§£é‡Šæ€§ã€‚

#### The number of tokens necessary to convey the same information varies significantly across languages ä¸åŒè¯­è¨€ä¼ é€’å‡ºç›¸åŒçš„ä¿¡æ¯æ‰€éœ€è¦çš„tokenæ•°å·®è·å¾ˆå¤§&#x20;

API è¯­è¨€æ¨¡å‹çš„å®šä»·æ”¿ç­–ï¼ˆæ ¹æ®å¤„ç†æˆ–ç”Ÿæˆçš„ä»£å¸æ•°é‡å‘ç”¨æˆ·æ”¶è´¹ï¼‰å¯èƒ½ä¸å…¬å¹³ã€‚ä»–ä»¬å‘ç°ï¼Œè®¸å¤šå—æ”¯æŒè¯­è¨€çš„ç”¨æˆ·åœ¨æ”¶åˆ°ä½äºæ ‡å‡†çš„ç»“æœçš„åŒæ—¶å´è¢«æ”¶å–äº†è¿‡é«˜çš„è´¹ç”¨ã€‚

#### **Discrepancies between the data that a tokenizer and a model have been trained on can lead to glitch tokens  åˆ†è¯å™¨å’Œé¢„è®­ç»ƒè¯­æ–™åº“ä¹‹é—´çš„ä¸ä¸€è‡´æ€§å¯èƒ½å¯¼è‡´é”™è¯¯ token**ï¼Œè¿›è€Œå¯¼è‡´æ¨¡å‹è¡Œä¸ºå¼‚å¸¸

è¿™å¯èƒ½ä¼šå¯¼è‡´æ„å¤–çš„æ¨¡å‹è¡Œä¸ºï¼Œå› ä¸ºå®ƒä»¬ç›¸åº”çš„embeddingsåŸºæœ¬ä¸Šæœªç»è®­ç»ƒã€‚æ¯æ¬¡ä¿®æ”¹é¢„è®­ç»ƒè¯­æ–™åº“æ—¶ï¼Œåˆ†è¯å™¨å’Œé¢„è®­ç»ƒè¯­æ–™åº“ä¹‹é—´çš„è¿™ç§couplingï¼ˆè€¦åˆï¼‰éƒ½ä¼šäº§ç”Ÿtokenizeræ–°è®­ç»ƒè¿è¡Œçš„è´Ÿæ‹…ã€‚

#### åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¿è¡Œè‰¯å¥½çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯å¯¹äºéç©ºæ ¼åˆ†éš”çš„è¯­è¨€ï¼ˆä¾‹å¦‚ä¸­æ–‡æˆ–æ—¥è¯­ï¼‰ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§

ç°æœ‰çš„å­è¯æ ‡è®°åŒ–æ–¹æ¡ˆä¸»è¦æ˜¯è´ªå©ªç®—æ³•ï¼Œè¯•å›¾æ ¹æ®æ‰€ä½¿ç”¨çš„æ ‡è®°æ•°é‡å°½å¯èƒ½æœ‰æ•ˆåœ°å¯¹è¯­è¨€è¿›è¡Œç¼–ç ã€‚è¿™äº›æ–¹æ³•æœ‰åˆ©äºåŒ…å«å¤§éƒ¨åˆ†è®­ç»ƒæ•°æ®çš„subwordsï¼Œå› æ­¤ä¹Ÿæœ‰åˆ©äºåœ¨å¤šç§è¯­è¨€ä¹‹é—´å…±äº«çš„subwordsã€‚è¿™æœ‰åˆ©äºå…·æœ‰shared scriptsçš„è¯­è¨€ï¼Œä¾‹å¦‚æ‹‰ä¸è¯­å’Œè¥¿é‡Œå°”è¯­ï¼Œå¯¼è‡´ä½èµ„æºè¯­è¨€çš„æ ‡è®°åŒ–æ•ˆæœä¸ä½³ã€‚

#### æ­¤å¤–ï¼Œåˆ†è¯å™¨ä¼šå¸¦æ¥**è®¡ç®—è´Ÿæ‹…ã€è¯­è¨€ä¾èµ–æ€§ã€å¤„ç†æ–°è¯ã€å›ºå®šè¯æ±‡è¡¨å¤§å°ã€ä¿¡æ¯ä¸¢å¤±å’Œäººç±»å¯è§£é‡Šæ€§**ç­‰å¤šä¸ªæŒ‘æˆ˜ã€‚

<figure><img src="../../.gitbook/assets/image (7).png" alt=""><figcaption><p><strong>Figure 2:</strong> Exemplary Drawbacks of relying on Tokenization. (1) The tokenizer training step involves non-trivial computations, e.g., multiple passes over the entire pre-training dataset, and introduces a dependency on it, which can become especially problematic in multilingual settings. (2) The embedding layer E and output layer W of LLMs involve the vocabulary size; e.g., making up â‰ˆ 66% of the modelâ€™s parameter count in T5 models .</p></figcaption></figure>

å¦‚å›¾ 2 æ‰€ç¤ºä¸ºä¾èµ–åˆ†è¯çš„å…¸å‹ç¼ºç‚¹ï¼Œå…¶ä¸­ï¼Œ**åˆ†è¯å™¨è®­ç»ƒæ­¥éª¤æ¶‰åŠåˆ°å¤æ‚çš„è®¡ç®—**ï¼Œå¦‚å¯¹æ•´ä¸ªé¢„è®­ç»ƒæ•°æ®é›†è¿›è¡Œå¤šæ¬¡æ‰«æï¼Œå¹¶å¼•å…¥äº†å¯¹å…¶çš„ä¾èµ–ï¼Œè¿™åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹å¯èƒ½ä¼šå˜å¾—ç‰¹åˆ«æ£˜æ‰‹ã€‚æ­¤å¤–ï¼Œ**è¯­è¨€æ¨¡å‹çš„åµŒå…¥å±‚ E å’Œè¾“å‡ºå±‚ W æ¶‰åŠè¯æ±‡è¡¨å¤§å°**ï¼Œæ¯”å¦‚åœ¨ T5 æ¨¡å‹ä¸­çº¦å æ¨¡å‹å‚æ•°æ€»æ•°çš„ 66% å·¦å³ã€‚

é’ˆå¯¹è¿™ä¸ªæŒ‘æˆ˜ï¼Œå­è¯çº§åˆ«çš„è¾“å…¥åˆ™æä¾›äº†è¯æ±‡å¤§å°å’Œåºåˆ—é•¿åº¦ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚æ­¤å¤–ï¼ŒByte-Pair Encoding (BPE)å’Œ WordPiece æ˜¯å¸¸ç”¨çš„å­è¯åˆ†è¯ç®—æ³•ã€‚å­—èŠ‚çº§è¾“å…¥æ˜¯å­è¯åˆ†è¯çš„ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œå¯ä»¥ä¸å­è¯åˆ†è¯å™¨ç»“åˆä½¿ç”¨æˆ–å®šä¹‰ä¸€ä¸ªæœ‰é™çš„è¯æ±‡è¡¨æ¥ç¼–ç æ‰€æœ‰å¯èƒ½çš„åºåˆ—ã€‚è¿˜æœ‰ä¸€äº›ç ”ç©¶æå‡ºäº†åŸºäºå­—èŠ‚çº§è¾“å…¥çš„åˆ†è¯æ–¹æ³•ï¼Œåœ¨æ€§èƒ½æ–¹é¢ä¸åŸºäºå­è¯çš„æ¨¡å‹ç›¸åª²ç¾ã€‚

### Challenge3ï¼šHigh Pre-Training Costs

å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œè¿™å¯èƒ½ä¼šå¯¹å…¶å¹¿æ³›åº”ç”¨äº§ç”Ÿé™åˆ¶--ä¸å¯æŒç»­

è®­ç»ƒ LLM çš„ä¸»è¦æ¶ˆè€—æ˜¯åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéœ€è¦**æ•°åä¸‡ä¸ªè®¡ç®—å°æ—¶ã€æ•°ç™¾ä¸‡å…ƒçš„æˆæœ¬ï¼Œä»¥åŠç›¸å½“äºæ•°ä¸ªæ™®é€šç¾å›½å®¶åº­å¹´åº¦èƒ½æºæ¶ˆè€—é‡çš„èƒ½é‡**ã€‚è€Œè¿‘æœŸæå‡ºçš„ç¼©æ”¾å®šå¾‹è®¤ä¸ºï¼Œ**æ¨¡å‹æ€§èƒ½éšç€æ¨¡å‹å¤§å°ã€æ•°æ®é›†å¤§å°å’Œè®­ç»ƒä¸­ä½¿ç”¨çš„è®¡ç®—é‡å‘ˆå¹‚å¾‹å…³ç³»ï¼Œ**å°†è¯¯å·®ä» 3% å‡å°‘åˆ° 2% å¯èƒ½éœ€è¦ä¸€ä¸ªæ•°é‡çº§çš„æ•°æ®æˆ–è®¡ç®—ã€‚

Unsustainable Loss Power-Lawï¼šé€šè¿‡å¢åŠ è®¡ç®—é¢„ç®—ï¼Œæ€§èƒ½ä¼šæé«˜ï¼Œä½†å¦‚æœæ¨¡å‹æˆ–æ•°æ®é›†å¤§å°å›ºå®šï¼Œæ€§èƒ½ä¼šé™ä½ï¼Œè¿™åæ˜ äº†æ”¶ç›Šé€’å‡çš„å¹‚å¾‹ã€‚

<mark style="background-color:green;">**ç°æœ‰çš„ä¸¤æ¡è§£å†³è·¯çº¿**</mark>ï¼š

**Compute-Optimal Training Recipes**--**è®¡ç®—æœ€ä¼˜è®­ç»ƒæ–¹æ³•**

_Given a particular budget, how large should the pretraining corpus and model be to maximize training efficiency?_

å­¦ä¹ ç»éªŒæ€§çš„â€œç¼©æ”¾å®šå¾‹â€œscaling lawsâ€ï¼Œä»¥å®ç°åœ¨ç»™å®šè®¡ç®—é¢„ç®—ä¸‹æœ€å¤§åŒ–è®­ç»ƒæ•ˆç‡

ä¾‹å¦‚ï¼ŒOpenAI GPT4çš„æŠ¥å‘Šç§°ï¼Œä»–ä»¬èƒ½å¤Ÿæ ¹æ®ä¸€ç³»åˆ—è¾ƒå°æ¨¡å‹çš„æ€§èƒ½å‡†ç¡®é¢„æµ‹å…¨å°ºå¯¸ GPT-4 æ¨¡å‹çš„æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨çš„è®¡ç®—é‡æœ€å¤šæ¯”å®Œæ•´æ¨¡å‹å°‘ 10,000 å€ã€‚

æ€§èƒ½é¢„æµ‹çš„ä¸€ä¸ªå‰©ä½™éšœç¢æ˜¯**inverse scaling**ï¼Œç”±äºç¼©æ”¾æ³•åˆ™é€šå¸¸æ˜¯åœ¨é¢„è®­ç»ƒçš„èƒŒæ™¯ä¸‹æ„å»ºçš„ï¼Œä»è€Œä¸ä¸‹æ¸¸ä»»åŠ¡è§£è€¦ï¼Œå› æ­¤å¦‚ä½•é¢„æµ‹é€†ç¼©æ”¾å±æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚å‘ç°ä¸Šæ¸¸å’Œä¸‹æ¸¸è®¾ç½®ä¸­çš„ç¼©æ”¾æ³•åˆ™å¯èƒ½ä¸åŒï¼›é™¤äº†æ¨¡å‹å¤§å°ä¹‹å¤–ï¼Œmodel shapeå¯¹äºä¸‹æ¸¸å¾®è°ƒä¹Ÿå¾ˆé‡è¦ã€‚

**Pre-Training Objectives--é¢„è®­ç»ƒç›®æ ‡**

å„ç§é¢„è®­ç»ƒç›®æ ‡ï¼ˆPTOï¼‰é€‚åˆå¯¹LLMè¿›è¡Œè‡ªæˆ‘ç›‘ç£åŸ¹è®­ã€‚ PTO çš„å‡†ç¡®é€‰æ‹©ä¼šä¸¥é‡å½±å“æ¨¡å‹åœ¨é¢„è®­ç»ƒæœŸé—´çš„æ•°æ®æ•ˆç‡ï¼Œä»è€Œå‡å°‘æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ã€‚

A PTO typically is a <mark style="color:green;">function</mark> of the <mark style="background-color:yellow;">(i) architecture,</mark> <mark style="background-color:yellow;">(ii) input/targets construction</mark> (e.g., t<mark style="background-color:yellow;">arget span length, low/high corruption, see Fig. 4,</mark> and (<mark style="background-color:yellow;">iii) masking strategy (Fig. 3).</mark> While (i) and (ii) can be disentangled and should not be conflated conceptually, in practice, there exist popular combinations that achieve good performances.

PTOé€šå¸¸æ˜¯(i)æ¶æ„ï¼Œ(ii)è¾“å…¥/ç›®æ ‡ç»“æ„(ä¾‹å¦‚ï¼Œç›®æ ‡è·¨åº¦é•¿åº¦ï¼Œä½/é«˜corruption (fig 4 )å’Œ(iii)æ©è”½ç­–ç•¥( fig 3)çš„å‡½æ•°ã€‚

<figure><img src="../../.gitbook/assets/Screenshot 2023-07-24 at 8.36.23 PM.png" alt="" width="563"><figcaption><p>Figure 3: Masking Strategies. Each row denotes to which inputs xi (columns) a particular output yi (row) can attend to (uni- or bi-directional). å±è”½ç­–ç•¥ã€‚æ¯ä¸€è¡Œè¡¨ç¤ºå¯ä»¥attend to çš„è¾“å…¥(åˆ—)çš„ä¸€ä¸ªç‰¹å®šçš„è¾“å‡º(è¡Œ)(uni -æˆ–bi-directional)</p></figcaption></figure>

åŠ å…¥æ‰€æœ‰çš„token,å¦‚fig3æ‰€ç¤º(å·¦),æ˜¯æœ€data-efficientç­–ç•¥,å› ä¸ºå®ƒä½¿ç”¨ä¸Šä¸‹æ–‡tokenä¹‹å‰å’Œä¹‹åçš„é¢„æµ‹ã€‚ç„¶è€Œ,ç”±äºè¿™ä¸ªåŸå› ,å®ƒä¸é€‚åˆæ–‡æœ¬ç”Ÿæˆ,å› ä¸ºå®ƒåœ¨é¢„æµ‹ä¸­è€ƒè™‘äº†future contextã€‚æˆ‘ä»¬é€šå¸¸ç”¨å®ƒæ¥å®Œæˆè‡ªç„¶è¯­è¨€ç†è§£(NLU)ä»»åŠ¡ã€‚Next token prediction objectiveé¢„æµ‹ç›®æ ‡æ˜¯æœ€é€‚åˆè‡ªç„¶è¯­è¨€ç”Ÿæˆ(NLG),ä½†ä¹Ÿdata-efficientä¹Ÿæ˜¯æœ€ä½çš„,å› ä¸ºå®ƒåªå…³æ³¨è¿‡å»çš„ä¸Šä¸‹æ–‡(fig3(ä¸­))ã€‚æœ€è¿‘çš„ç ”ç©¶ç›®æ ‡æ˜¯æ˜¯æ‰¾åˆ°ä¸€ä¸ªmiddle-groundæ¥æé«˜æ•ˆç‡çš„æ•°æ®æä¾›æ›´å¼ºå¤§å’Œæ›´å¤šæ ·åŒ–çš„è®­ç»ƒä¿¡å·,ä¾‹å¦‚,Prefix LM,è¿™ä¸­ç­–ç•¥éƒ¨åˆ†çš„å…³æ³¨past tokens,å¦‚fig3ä¸­æ‰€ç¤º(å³)å’Œä¸‹é¢è®¨è®ºåˆ°çš„éƒ¨åˆ†ã€‚

**Masked Language Modeling (MLM; or Cloze)**ï¼šé€šè¿‡ç”¨ç‰¹æ®Šçš„ \[MASK] æ ‡è®°æ›¿æ¢éšè—sequenceä¸­ä¸€å®šæ¯”ä¾‹çš„æ ‡è®°ã€‚ä¸€èˆ¬å°† MLM ç›®æ ‡ç”¨äºnon-autoregressiveï¼Œå³non-generativeã€bidirectional context modelsã€‚å…¶ä¸­æ¨¡å‹ä½¿ç”¨ç›®æ ‡tokenä¹‹å‰å’Œä¹‹åçš„tokenè¿›è¡Œé¢„æµ‹ï¼Œæœ‰ç€å¯¹å…¶ä¸Šä¸‹æ–‡æ¯” NTP ç›®æ ‡æ›´å…¨é¢çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¯ä¸ªè¾“å…¥å¥å­åœ¨ä¸€æ¬¡ä¼ é€’ä¸­é¢„æµ‹å¤šä¸ªå±è”½æ ‡è®°ï¼Œè€Œ NTP ç›®æ ‡é€šå¸¸é€šè¿‡ä¸€æ¬¡é¢„æµ‹ä¸€ä¸ªæ ‡è®°æ¥å­¦ä¹ ã€‚

[Bidirectional Language Models Are Also Few-shot Learners](https://arxiv.org/abs/2209.14500)ä¸­è¡¨æ˜æ­¤ç±»æ¨¡å‹äº§ç”Ÿçš„è¡¨ç¤ºæ›´é€‚åˆè¿ç§»å­¦ä¹ ï¼›ç„¶è€Œï¼Œå®ƒä»¬åœ¨è¿›è¡Œin-context learningæ—¶é‡åˆ°å›°éš¾ã€‚

ä¸€ç§æå‡æ–¹å¼:[METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals](https://arxiv.org/abs/2204.06644) --(i) ä½¿ç”¨ MLM ç›®æ ‡è®­ç»ƒ ALM(auxiliary language model)ï¼Œ(ii) ç»™å®šä¸€äº›å¸¦æœ‰masked positionsçš„è¾“å…¥ï¼Œé¢„æµ‹tokensï¼ˆä½¿ç”¨ ALMï¼‰ï¼Œ(iii) è®­ç»ƒä¸»æ¨¡å‹ä»¥çº æ­£æ’å…¥masked positionsçš„è¿™äº›tokensï¼Œå³ 1) é¢„æµ‹ ALM æ˜¯å¦æ›¿æ¢äº†æ ‡è®°ï¼Œå¦‚æœæ˜¯ï¼Œ2) é¢„æµ‹åŸå§‹æ ‡è®°ã€‚train the auxiliary and main model **jointly**.

**Prefix Language Modeling**&#x20;

Generalizes language modeling by allowing prefix tokens with a <mark style="background-color:yellow;">bidirectional receptive field</mark> to be added to the input (without prefix, it is equivalent to standard LM). å¯¹æ¯”fig3å·¦å’Œå³

**Span Corruption** or span denoising

å°† MLM æ¨å¹¿åˆ°å¯¹ç»™å®šæ–‡æœ¬ä¸­çš„è¿ç»­æ ‡è®°åºåˆ—ï¼ˆç§°ä¸ºspansï¼‰è¿›è¡Œå»å™ªã€‚å»å™ªç›®æ ‡é€šå¸¸ç”¨å•ä¸ªå”¯ä¸€çš„æ©è”½æ ‡è®°æ›¿æ¢é‡‡æ ·çš„spanï¼Œå¹¶è®­ç»ƒæ¨¡å‹æ¥å¡«å……å®ƒã€‚å¯ä»¥æå‡è®­ç»ƒçš„é€Ÿåº¦ï¼Œå› ä¸ºï¼šä¸ç ´å i.i.d. ä¸­çš„å•ä¸ªtokenç›¸æ¯”ï¼Œspan corruptionå¹³å‡ä¼šäº§ç”Ÿæ›´çŸ­çš„åºåˆ—ã€‚

**Mixture of Denoisersï¼ˆMoDï¼‰**

é€šè¿‡æ··åˆå¤šä¸ªå»å™ªç›®æ ‡æ¥æ³¨å…¥ç›®æ ‡å¤šæ ·æ€§

**Fill In the Middle**

é€šè¿‡åœ¨æ–‡æ¡£ä¸­æ‰“ä¹±tokenæ¥å¢å¼ºä¸‹ä¸€ä¸ªtokené¢„æµ‹ç›®æ ‡ï¼Œä»¥ä¾¿æˆ‘ä»¬æ ¹æ®å‰ç¼€å’Œåç¼€å¡«å……ä¸­é—´ï¼ˆFIMï¼‰

**Meet in the Middle**

é€šè¿‡å¯ç”¨åŒå‘ä¸Šä¸‹æ–‡æ¥æ„å»ºæ›´å¯†é›†ã€æ•°æ®æ•ˆç‡æ›´é«˜çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶ä¿æŒè‡ªå›å½’ï¼Œä»è€Œæ‰©å±• FIM ç›®æ ‡

**Parallelism Strategies**

**Miscellaneous**

<figure><img src="../../.gitbook/assets/image (4).png" alt=""><figcaption><p>Figure4ï¼šSelf-Supervised Data Construction by Pre-Training Objectives. é€šè¿‡é¢„è®­ç»ƒç›®æ ‡è¿›è¡Œè‡ªæˆ‘ç›‘ç£çš„æ•°æ®æ„å»ºï¼Œç”¨ç°è‰²çŸ©å½¢è¡¨ç¤ºå±è”½æ ‡è®°ï¼Œè¿™äº›æ ‡è®°æˆä¸ºç›®æ ‡ï¼Œçœç•¥äº†ç‰¹æ®Šçš„æ ‡è®°ã€‚</p></figcaption></figure>

### Challenge:4ï¼šFine-Tuning Overhead

å¯¹æµ·é‡ä¸”å¤šæ ·åŒ–çš„æ–‡æœ¬æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒLLMçš„ä¸€ä¸ªæ½œåœ¨ç¼ºç‚¹æ˜¯ï¼Œç”Ÿæˆçš„æ¨¡å‹å¯èƒ½éš¾ä»¥æ˜ç¡®æ•è·ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†çš„åˆ†å¸ƒå±æ€§ã€‚ä½¿ç”¨å¾®è°ƒè§£å†³ä¸Šè¿°é—®é¢˜-->å¾®è°ƒæ˜¯æŒ‡åœ¨ç‰¹å®šäºå•ä¸ªé¢†åŸŸæˆ–ä»»åŠ¡çš„ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šè°ƒæ•´é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å‚æ•°ã€‚ LLM fine-tuningå¯¹äºä½¿ LLM é€‚åº”ä¸‹æ¸¸ä»»åŠ¡éå¸¸æœ‰æ•ˆ ã€‚

ä»æŠ€æœ¯ä¸Šè®²ï¼Œå¯ä»¥é€šè¿‡åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹æ¥å®ç°å¾®è°ƒã€‚æ ¹æ®æ¨¡å‹æ¶æ„ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®Œæˆçš„ï¼š(i) ä½¿ç”¨æ ‡å‡†è¯­è¨€å»ºæ¨¡ç›®æ ‡ç›´æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œ(ii) å°†å•ç‹¬çš„å¯å­¦ä¹ å±‚æ·»åŠ åˆ°é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¾“å‡ºè¡¨ç¤ºä¸­ï¼Œè¿™äº›å±‚æ—¨åœ¨åœ¨æ¨¡å‹çš„è¾“å‡ºè¡¨ç¤ºå’Œå„ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„è¾“å‡ºæ ¼å¼ï¼ˆä¾‹å¦‚ï¼Œç”¨äºæ–‡æœ¬åˆ†ç±»æˆ–åºåˆ—æ ‡è®°ï¼‰ä¹‹é—´åˆ›å»ºå…¼å®¹æ€§ã€‚ç„¶è€Œï¼Œ<mark style="background-color:red;">æ‹¥æœ‰æ•°åäº¿å‚æ•°çš„LLMéœ€è¦å¤§é‡å†…å­˜æ¥å­˜å‚¨ï¼ˆiï¼‰the model parametersï¼Œï¼ˆiiï¼‰the model activationsï¼Œä»¥åŠï¼ˆiiiï¼‰gradients and corresponding statisticsã€‚ç”±äºè®¾å¤‡å†…å­˜ï¼ˆä¾‹å¦‚ GPU æˆ– TPUï¼‰æœ‰é™ï¼Œéœ€è¦è®¿é—®å…·æœ‰è®¸å¤šè®¾å¤‡çš„å¤§å‹é›†ç¾¤æ¥å¾®è°ƒå®Œæ•´çš„ LLMã€‚</mark>

#### Large Memory Requirements

<mark style="background-color:red;">å¾®è°ƒæ•´ä¸ª LLM éœ€è¦ä¸é¢„è®­ç»ƒç›¸åŒæ•°é‡çš„å†…å­˜</mark>ï¼Œè¿™å¯¹äºè®¸å¤šä»ä¸šè€…æ¥è¯´æ˜¯ä¸å¯è¡Œçš„ã€‚

æ­¤å¤–ï¼Œè™½ç„¶å®Œæ•´çš„æ¨¡å‹å¾®è°ƒå¯ä»¥æœ‰æ•ˆåœ°ä½¿ LLM åœ¨ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†éœ€è¦ä¸ºå„ä¸ªä»»åŠ¡å­˜å‚¨å’ŒåŠ è½½å¾®è°ƒåçš„ LLM çš„å„ä¸ªå‰¯æœ¬ï¼Œè¿™åœ¨è®¡ç®—ä¸Šæ•ˆç‡ä½ä¸‹ï¼Œï¼ˆè§fig5ï¼‰

<figure><img src="../../.gitbook/assets/image (5).png" alt="" width="365"><figcaption><p>Figure 5: Fine-tuning an LLM for a specific downstream task. (a) illustrates vanilla fine-tuning, which requires updating the entire model, resulting in a new model for each task. In (b), PEFT instead learns a small subset of model parameters for each task with a fixed base LLM. The same base model can be re-used during inference for different tasks. <strong>å°†ä¼ ç»Ÿfinetuningå’Œpeftåšäº†å¯¹æ¯”ã€‚</strong></p></figcaption></figure>

#### **Parameter-efficient fine-tuning**

å‚æ•°é«˜æ•ˆå¾®è°ƒä½¿LLMé€‚åº”ç‰¹å®šæ•°æ®é›†/é¢†åŸŸçš„å¦ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ã€‚ PEFT æ˜¯æŒ‡é€šè¿‡ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†æ¨¡å‹å‚æ•°æ¥é€‚åº” LLM çš„ä¸€ç±»æ–¹æ³•ã€‚ [Adapters](https://arxiv.org/abs/1902.00751)æ˜¯ PEFT æœ€æ—©çš„å·¥ä½œä¹‹ä¸€ã€‚è¯¥æ–¹æ³•å°†é¢å¤–çš„å¯å­¦ä¹ å±‚åˆå¹¶åˆ° Transformer æ¶æ„ä¸­ï¼Œè¿™äº›å±‚åœ¨å¾®è°ƒæœŸé—´è¿›è¡Œæ›´æ–°ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œçš„å…¶ä½™éƒ¨åˆ†ä¸å˜ã€‚ 26 ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆåŒ…æ‹¬ GLUE åŸºå‡† ï¼‰çš„å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ Adapter è®­ç»ƒçš„æ¨¡å‹åœ¨ä»…æ›´æ–° 3% çš„æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹å¯ä»¥ä¸full fine-tuning ç«äº‰ã€‚åŒæ—¶ï¼Œ[BIt-Fit](https://aclanthology.org/2022.acl-short.1/)å»ºè®®ä»…æ›´æ–°æ¨¡å‹çš„åå·®é¡¹è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›åå·®é¡¹å æ¨¡å‹å‚æ•°çš„æ¯”ä¾‹ä¸åˆ° 1%ã€‚

ç›®å‰Adapteråˆå¹¶åˆ°è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„ä¸‰ä¸ªé€šç”¨æ¡†æ¶AdapterHubï¼š[https://adapterhub.ml/](https://adapterhub.ml/)

* LLM-Adaptersï¼š[https://github.com/AGI-Edgerunners/LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)
* HuggingFace  PEFT  ï¼š[https://github.com/huggingface/peft](https://github.com/huggingface/peft)

é’ˆå¯¹è¾ƒå¤§æ¨¡å‹å¼•å…¥çš„ PEFT æ–¹æ³•åŒ…æ‹¬prefix-tuningå’Œprompt-tuningï¼Œå®ƒä»¬éƒ½æ˜¯é€šè¿‡åœ¨è¾“å…¥å‰é¢æ·»åŠ ä¸€ç»„å¯å­¦ä¹ çš„token embeddingsæ¥è¿›è¡Œæ“ä½œã€‚è¿™äº›token embeddingsï¼ˆä¹Ÿç§°ä¸ºsoft promptï¼‰æ˜¯åœ¨å¾®è°ƒé˜¶æ®µå­¦ä¹ çš„ï¼Œè€Œæ¨¡å‹å‚æ•°çš„å…¶ä½™éƒ¨åˆ†ä¿æŒå›ºå®šã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§è½¯æç¤ºåŒ…å«æ•°åƒä¸ªè€Œä¸æ˜¯æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œå¹¶ä¸”å­˜å‚¨æ•ˆç‡æ›´é«˜ã€‚åœ¨å¾®è°ƒä»¤ç‰Œçš„åŒæ—¶ä»ç„¶éœ€è¦é€šè¿‡ç½‘ç»œè¿›è¡Œåå‘ä¼ æ’­

ä»…å…·æœ‰API è®¿é—®æƒé™çš„é»‘ç›’æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼š[Black-Box Prompt Learning for Pre-trained Language Models](https://arxiv.org/abs/2201.08531) & [Black-Box Tuning for Language-Model-as-a-Service](https://proceedings.mlr.press/v162/sun22e.html)

[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638) å¼•å…¥ $$(IA)^3$$ ï¼Œå®ƒä½¿ç”¨å¯å­¦ä¹ å‘é‡æ¥ç¼©æ”¾å„ä¸ª Transformer å±‚ä¸­çš„æ¿€æ´»ã€‚ä½œè€…é€šè¿‡è¯æ˜ä½¿ç”¨$$(IA)^3$$ è®­ç»ƒçš„æ¨¡å‹ä¼˜äºå„ç§æ•°æ®é›†ä¸Šçš„å®Œæ•´æ¨¡å‹å¾®è°ƒï¼ŒåŒæ—¶ä»…æ›´æ–° 0.01% çš„æ¨¡å‹å‚æ•°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

[Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)æå‡ºäº†ä¸€ç§å†…å­˜é«˜æ•ˆçš„é›¶é˜¶ï¼ˆMeZOï¼‰ä¼˜åŒ–å™¨ï¼Œå®ƒåªéœ€è¦ä¸æ¨ç†æœŸé—´ç›¸åŒçš„å†…å­˜å ç”¨ï¼ˆè€Œä¸æ˜¯å­˜å‚¨æ¢¯åº¦æˆ–ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ä¼˜åŒ–ä¸å¯å¾®åˆ†çš„ç›®æ ‡ï¼Œä¾‹å¦‚å‡†ç¡®æ€§æˆ– F1 åˆ†æ•°ï¼Œä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„è°ƒæ•´æ–¹æ³•åˆ™æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚

[LoRA: Low-Rank Adaptation of Large Language Models](./)æå‡ºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œå®ƒå°†å„ä¸ª Transformer å±‚æƒé‡çŸ©é˜µçš„å‚æ•°æ›´æ–°ä½œä¸ºåŠ æ³•ä½ç§©åˆ†è§£ã€‚è¿™ç§é‡æ–°å‚æ•°åŒ–é¿å…äº†è®¡ç®—å¯†é›†çŸ©é˜µä¹˜æ³•çš„éœ€è¦ã€‚QL0RAå°† LoRA æ‰©å±•åˆ°é‡åŒ–çš„ LLMï¼Œå¤§å¤§å‡å°‘äº†å†…å­˜ä½¿ç”¨é‡ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿåœ¨å•ä¸ª 48GB GPU ä¸Šå¾®è°ƒ 65B æ¨¡å‹ï¼ŒåŒä¸€æ¨¡å‹çš„å¸¸è§„è®­ç»ƒéœ€è¦è¶…è¿‡ 780 GB çš„ GPU å†…å­˜ã€‚

**Compute Requirements**  é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒ LLM æ‰€éœ€çš„å†…å­˜å¤æ‚åº¦æœ‰äº†æ˜¾ç€æé«˜ï¼Œå‰©ä¸‹çš„æŒ‘æˆ˜æ˜¯æ—¶é—´å¤æ‚åº¦ã€‚ä½¿ç”¨ PEFT æ–¹æ³•å¯¹ LLM è¿›è¡Œå¾®è°ƒï¼Œ<mark style="background-color:red;">ä»ç„¶éœ€è¦å®Œæ•´çš„æ¢¯åº¦è®¡ç®—</mark>ã€‚é€‚åº”LLMæ‰€éœ€çš„è®¡ç®—åŸºç¡€è®¾æ–½é˜»ç¢äº†å°å‹è®¾å¤‡ä¸Šçš„ä¸ªæ€§åŒ–ç­‰æ½œåœ¨åº”ç”¨ã€‚



**Reference**

[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)

[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://aclanthology.org/2022.acl-short.1.pdf)

[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353.pdf)

[The Power of Scale for Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243.pdf)

[Black-Box Tuning for Language-Model-as-a-Service](https://proceedings.mlr.press/v162/sun22e.html)

[Black-Box Prompt Learning for Pre-trained Language Models](https://arxiv.org/abs/2201.08531)

[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)

[Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)

[LoRA: Low-Rank Adaptation of Large Language Models](./)



### Challenge5ï¼šHigh Inference Latency æ¨ç†å»¶è¿Ÿ

ç›®å‰æ ¹æ®ç ”ç©¶ï¼ŒLLM è¡¨ç°å‡ºé«˜æ¨ç†å»¶è¿Ÿçš„ä¸¤ä¸ªåŸå› ï¼š

(1) **low parallelizability** å¹¶è¡Œæ€§ä½ï¼Œå› ä¸ºæ¨ç†è¿‡ç¨‹ä¸€æ¬¡åªè¿›è¡Œä¸€ä¸ªtokenï¼›

(2) **large memory footprints** å†…å­˜å ç”¨è¾ƒå¤§--due to the model size and the transient states needed during decoding (e.g., attention key and value tensors).

æ­¤å¤–ï¼Œ Transformers ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡ç¼©æ”¾ä¹Ÿæ˜¯å€¼å¾—è®¨è®ºçš„ï¼Œè§challlenge6ã€‚

#### **Techniques used to address these challenges eg.** å‡å°‘å†…å­˜å ç”¨ï¼ˆå¤§å°å’Œ/æˆ–å¸¦å®½&åŠ é€Ÿç‰¹å®šçš„è®¡ç®—æ“ä½œ

<mark style="background-color:blue;">**Efficient Attention   --åŠ é€Ÿattentionæœºåˆ¶çš„è®¡ç®—**</mark>

<mark style="color:green;">Two lines of workï¼š (i) lower-level hardware-aware modifications è¾ƒä½çº§åˆ«çš„ç¡¬ä»¶æ„ŸçŸ¥modifications (ii) higher-level sub-quadratic approximations of the attention mechanism æ³¨æ„åŠ›æœºåˆ¶æ›´é«˜å±‚æ¬¡çš„æ¬¡äºŒæ¬¡æ–¹è¿‘ä¼¼</mark>

**Lower-level hardware-aware modifications**

1. [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150) æå‡º <mark style="color:blue;">multi-query attention</mark> æ—¨åœ¨é€šè¿‡ä¸ºé”®å’Œå€¼å¼ é‡ä»…ä¿ç•™ä¸€ä¸ªæ³¨æ„åŠ›å¤´ï¼Œåœ¨ä½¿ç”¨ Transformer è§£ç å™¨å±‚é¡ºåºç”Ÿæˆtokenåºåˆ—æ—¶å‡å°‘å†…å­˜å¸¦å®½ç“¶é¢ˆã€‚
2.  [FlashAttention](https://arxiv.org/abs/2205.14135) çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æ›¿ä»£è®¡ç®—æ–¹æ³•æ¥å‡å°‘å†…å­˜å¸¦å®½ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘ I/O æ“ä½œçš„æ•°é‡ï¼Œä»è€ŒåŠ å¿«ç°ä»£ GPU ä¸Šçš„è®¡ç®—é€Ÿåº¦ã€‚ä½œä¸ºä¸€ç§ä¼˜åŒ–çš„æ³¨æ„åŠ›ï¼Œ FlashAttention åˆ©ç”¨ç®—å­èåˆæ¥å‡å°‘å†…å­˜å¸¦å®½ç“¶é¢ˆã€‚&#x20;

    [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135) æå‡º`FlashAttention`

    [Faster Causal Attention Over Large Sequences Through Sparse Flash Attention](../lian-dan-gong-ju-xiang/paramters-and-definations.md) æ„å»ºåœ¨ FlashAttention ä¹‹ä¸Šï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›ç¨€ç–æ¨¡å¼--key/query dropping and hashing-based attentionã€‚

    [Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102) å®æ–½ä¸åŒçš„shardingæŠ€æœ¯ï¼Œä»¥æœ‰æ•ˆåœ°è·¨è®¾å¤‡ä¼ æ’­å‰é¦ˆå’Œæ³¨æ„åŠ›è®¡ç®—ï¼ŒåŒæ—¶ä¼˜åŒ–è®¾å¤‡é—´é€šä¿¡æˆæœ¬ï¼Œ**ä½¿ç”¨muli-query attentionå®ç°é«˜è¾¾ 43,000 ä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚**

**Higher-level sub-quadratic approximations of the attention mechanism**

æé«˜æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æˆ–å†…å­˜å¤æ‚åº¦çš„ä¸€ä¸ªå…±åŒä¸»é¢˜æ˜¯ç¨€ç–æ³¨æ„åŠ›çŸ©é˜µæˆ–å¼•å…¥ï¼ˆçº¿æ€§ï¼‰è¿‘ä¼¼ã€‚

ç„¶è€Œï¼Œä¸€äº›æœ‰æ•ˆçš„æ³¨æ„åŠ›è¿‘ä¼¼çš„å¯æ‰©å±•æ€§å—åˆ°äº†è´¨ç–‘ã€‚æ¯”å¦‚ä¾‹å¦‚ï¼Œæœ‰ç ”ç©¶å‘ç°å‘ç°Performer attention approximation è¿‘ä¼¼çš„è¡¨ç°ä¸¥é‡ä½äºæ™®é€šçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯å½“æ‰©å±•åˆ°å¤§å‹æ¨¡å‹æ—¶ã€‚

<mark style="background-color:blue;">**Quantization**</mark>

é‡åŒ–æ˜¯ä¸€ç§post-trainingæŠ€æœ¯ï¼Œå¯é€šè¿‡é™ä½æƒé‡å’Œæ¿€æ´»çš„è®¡ç®—ç²¾åº¦æ¥å‡å°‘å†…å­˜å ç”¨å’Œ/æˆ–å¢åŠ æ¨¡å‹çš„ååé‡ã€‚[ nuQmm](https://arxiv.org/abs/2206.09557) å’Œ [ZeroQuant ](https://arxiv.org/abs/2206.01861)ä½¿ç”¨éå‡åŒ€é‡åŒ–æ–¹æ³•æ¥é‡åŒ–æƒé‡å¹¶åº”ç”¨è‡ªå®šä¹‰ CUDA å†…æ ¸ä»¥è·å¾—è®¡ç®—ä¼˜åŠ¿ã€‚ [LLM.int8() ](https://arxiv.org/abs/2208.07339) æ˜¯ä¸€ç§æ— é™çº§é‡åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡åˆ©ç”¨ Int8 é‡åŒ–æ¥æœ‰æ•ˆæ¨æ–­æ•°åäº¿å‚æ•° LLMï¼Œå¹¶é’ˆå¯¹æŸäº›outlierç‰¹å¾å›é€€åˆ°æ›´é«˜çš„ç²¾åº¦ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

ç±»ä¼¼åœ°ï¼Œ[GLM-130B](https://arxiv.org/abs/2210.02414) ä½¿ç”¨æ— é™çº§8ä½é‡åŒ–æ–¹æ¡ˆï¼Œä»¥8ä½å­˜å‚¨æƒé‡å¹¶ä»¥16ä½ç²¾åº¦æ‰§è¡ŒçŸ©é˜µä¹˜æ³•ã€‚[GPTQ](https://arxiv.org/abs/2210.17323)æ˜¯ä¸€ç§é«˜æ•ˆçš„ä¸€æ¬¡æ€§é‡åŒ–æŠ€æœ¯ï¼Œå°† LLM æƒé‡å‹ç¼©è‡³æ¯ä¸ªæƒé‡ 3 åˆ° 4 ä½ï¼Œä»è€Œä½¿ 175B å‚æ•°æ¨¡å‹èƒ½å¤Ÿåœ¨å• ä¸ª GPU ä¸Šè¿è¡Œã€‚[SpQR](https://arxiv.org/abs/2306.03078) é€šè¿‡ç»“åˆå¼‚å¸¸å€¼æƒé‡çš„æ›´é«˜ç²¾åº¦è¡¨ç¤ºå’Œåˆ†ç»„é‡åŒ–æ¥è¿›ä¸€æ­¥æ”¹è¿›è¿™ä¸€ç‚¹ã€‚

<mark style="background-color:blue;">**Pruning**</mark>

Pruningæ˜¯ç”¨äºé‡åŒ–çš„complementary post-training æŠ€æœ¯ï¼Œè¦åˆ é™¤ç»™å®šæ¨¡å‹çš„éƒ¨åˆ†æƒé‡ï¼ˆä¸ä¼šé™ä½å…¶æ€§èƒ½ï¼‰ã€‚ä¸€ä¸ªé‡è¦çš„åŒºåˆ«æ˜¯ä¿®å‰ªæ˜¯éµå¾ªç»“æ„åŒ–æ¨¡å¼è¿˜æ˜¯éç»“æ„åŒ–æ¨¡å¼ã€‚ç»“æ„åŒ–ç¨€ç–æ¨¡å‹ç”¨æ˜æ˜¾æ›´å°ä½†ä»ç„¶å¯†é›†çš„ç»„ä»¶çš„é›†åˆæ¥æ›¿ä»£æ¨¡å‹çš„å¯†é›†éƒ¨åˆ†ã€‚éç»“æ„åŒ–ç¨€ç–æ¨¡å‹åŒ…å«çš„æƒé‡ä¸ºé›¶ï¼Œè¿™ä¸ä¼šå½±å“ç½‘ç»œçš„è¡Œä¸ºï¼Œå› æ­¤ç†è®ºä¸Šå¯ä»¥æ¥å—ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œåœ¨å½“å‰ç¡¬ä»¶ä¸Šå°†ç†è®ºè½¬åŒ–ä¸ºå®é™…è®¡ç®—èŠ‚çœæ›´å…·æŒ‘æˆ˜æ€§ã€‚



**Reference**

[Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102)

[Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)



### ç›¸å…³è§£è¯»ï¼š

* [https://mp.weixin.qq.com/s/JsCoUcuCg4ylKkPMvNouEw](https://mp.weixin.qq.com/s/JsCoUcuCg4ylKkPMvNouEw)



