# ğŸ¥³ Quictour

æœ¬å¿«é€Ÿæµè§ˆå°†å‘æ‚¨å±•ç¤º ğŸ¤— PEFT çš„ä¸»è¦åŠŸèƒ½ï¼Œå¹¶å¸®åŠ©æ‚¨è®­ç»ƒé€šå¸¸åœ¨æ¶ˆè´¹è®¾å¤‡ä¸Šæ— æ³•è®¿é—®çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ã€‚æ‚¨å°†äº†è§£å¦‚ä½•ä½¿ç”¨[`bigscience/mt0-large`](https://huggingface.co/bigscience/mt0-large)LoRA è®­ç»ƒ 1.2B å‚æ•°æ¨¡å‹ä»¥ç”Ÿæˆåˆ†ç±»æ ‡ç­¾å¹¶å°†å…¶ç”¨äºæ¨ç†ã€‚

### PeftConfig

æ¯ä¸ª ğŸ¤— PEFT æ–¹æ³•éƒ½ç”± PeftConfig ç±»å®šä¹‰ï¼Œè¯¥ç±»å­˜å‚¨æ„å»º PeftModel çš„æ‰€æœ‰é‡è¦å‚æ•°ã€‚

ä½¿ç”¨ LoRAï¼Œæ‚¨éœ€è¦åŠ è½½å¹¶åˆ›å»ºä¸€ä¸ª LoraConfig ç±»ã€‚åœ¨ LoraConfig ä¸­ï¼ŒæŒ‡å®šä»¥ä¸‹å‚æ•°ï¼š\


* the `task_type`, or sequence-to-sequence language modeling in this case ä»»åŠ¡ç±»å‹
* `inference_mode`, whether youâ€™re using the model for inference or not æ˜¯å¦ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
* `r`, the dimension of the low-rank matrices ä½ç§©çŸ©é˜µçš„ç»´åº¦
* `lora_alpha`, the scaling factor for the low-rank matrice ä½ç§©ç¼©æ”¾å› å­
* `lora_dropout`, the dropout probability of the LoRA layers lora å±‚çš„ä¸¢å¤±æ¦‚ç‡

```python
from peft import LoraConfig, TaskType
peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)
```

æ›´å¤šæŸ¥çœ‹ï¼š[LoraConfigé…ç½®è¯¦æƒ…](https://huggingface.co/docs/peft/main/en/package\_reference/tuners#peft.LoraConfig)

PeftModel ç”±`get_peft_model()` å‡½æ•°åˆ›å»ºï¼Œ [PeftConfig](https://huggingface.co/docs/peft/main/en/package\_reference/config#peft.PeftConfig) ä¸­åŒ…å«äº†å¦‚ä½•ä¸ºç‰¹å®šçš„PEFTæ–¹æ³•é…ç½®æ¨¡å‹çš„instruction.

**åŠ è½½é¢„è®­ç»ƒæ¨¡å‹**

<pre class="language-python"><code class="lang-python">from transformers import AutoModelForSeq2SeqLM
model_name_or_path = "bigscience/mt0-large" 
tokenizer_name_or_path = "bigscience/mt0-large" 
<strong>model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
</strong></code></pre>

