---
description: >-
  Akari Asai, Sewon Min, Zexuan Zhong, Danqi Chen
  https://acl2023-retrieval-lm.github.io/
---

# ğŸ§â™€ ACL 2023 Tutorial: Retrieval-based Language Models and Applications

è¿™é‡Œä¸»è¦ä¸­æ–‡æ€»ç»“æœ¬æ•™ç¨‹ä¸­çš„ä¸€äº›é‡ç‚¹å†…å®¹

**è®²è€…è¯´æ˜**ï¼š æœ¬æ•™ç¨‹æ˜¯æœ€å‰æ²¿çš„ï¼Œä¸å‚æ•°åŒ–llmç›¸æ¯”ï¼Œæˆ‘ä»¬è¿˜è¿œè¿œä¸èƒ½ç†è§£å¦‚ä½•æœ€å¥½åœ°å¼€å‘åŸºäºæ£€ç´¢çš„lmï¼Œè¿™ä¸ªæ•™ç¨‹ä¸»è¦åˆ†äº«ï¼š

* ç°æœ‰ç ”ç©¶çš„åˆ†ç±»å’Œå…³é”®è§è§£
* æˆ‘ä»¬å¯¹å½“å‰æŒ‘æˆ˜å’Œå¼€æ”¾é—®é¢˜çš„çœ‹æ³•

## 1. Introduction

**1. ä»€ä¹ˆæ˜¯Retrieval-based language models (LMs)ï¼Ÿ**

Retrieval-based LMs = Retrieval + LMs è¯­è¨€æ¨¡å‹ä»å¤–éƒ¨æ•°æ®å­˜å‚¨ä¸­è¿›è¡Œæ£€ç´¢ï¼ˆè‡³å°‘åœ¨æ¨ç†æœŸé—´ï¼‰

<figure><img src="figure/image16.png" alt="" width="563"><figcaption></figcaption></figure>

è¿™æ ·çš„æ¨¡å‹ä¹Ÿè¢«ç§°ä¸ºåŠå‚æ•°æ¨¡å‹å’Œéå‚æ•°æ¨¡å‹ï¼ˆsemiparametric and non-parametric modelsï¼‰

**2. The age of large language models (LLMs)ï¼šä¸»è¦ä»‹ç»ç›®å‰å¤§è¯­è¨€æ¨¡å‹çš„ä¸€äº›ç‰¹ç‚¹**

* Transformers-based, fully parametric
* Trained on next-token prediction tasks (+ RLHF;)
* Model size â†‘, data sizeâ†‘

**3. Retrieval for knowledge-intensive NLP tasks** å¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„æ£€ç´¢

**Representative tasks**: open-domain QA, fact-checking, entity linking...

LMæ¨åŠ¨äº†å¤§é‡å…³äºå¯†é›†æ£€ç´¢çš„æ›´å¥½ç®—æ³•çš„ç ”ç©¶ï¼Œä¾‹å¦‚ï¼ŒDPRï¼ŒColBERT,ANCE,Contrieverï¼Œ..

**4. Why** retrieval-based LMs?

* LLMs canâ€™t memorize all (long-tail) knowledge in their parameters å¤§æ¨¡å‹çš„å‚æ•°å¯¹çŸ¥è¯†çš„è®°å¿†æœ‰é™
* LLMsâ€™ knowledge is easily outdated and hard to update å¤§æ¨¡å‹çš„çŸ¥è¯†å®¹æ˜“è¿‡æ—¶ï¼Œéš¾ä»¥æ›´æ–°----ç°æœ‰çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•ä»ç„¶æ˜¯**ä¸å¯æ‰©å±•**çš„ï¼ˆç ”ç©¶æ–¹å‘ï¼ï¼‰è€Œæ•°æ®å­˜å‚¨å¯ä»¥å¾ˆå®¹æ˜“åœ°æ›´æ–°å’Œæ‰©å±•â€”â€”ç”šè‡³ä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹
* LLMsâ€™ output is challenging to interpret and verify å¤§æ¨¡å‹çš„è¾“å‡ºéš¾ä»¥éªŒè¯å’Œè§£é‡Š--ä»æ£€ç´¢ç»“æœä¸­æ›´æ–°çŸ¥è¯†æ¥æºå¯ä»¥è·å¾—æ›´å¥½çš„è§£é‡Šæ€§å’Œæ§åˆ¶æ€§ï¼ˆGenerating text with citationsï¼Œlike newbingï¼‰
* LLMs are shown to easily leak private training data **å¤§æ¨¡å‹å®¹æ˜“æ³„æ¼ç§æœ‰è®­ç»ƒæ•°æ®** ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡å°†ç§äººæ•°æ®å­˜å‚¨åœ¨æ•°æ®å­˜å‚¨å™¨ä¸­ï¼Œä»è€Œå¯¹å…¶è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ï¼ˆè€Œä¸æ˜¯ç›´æ¥å‚ä¸æ¨¡å‹å‚æ•°è®­ç»ƒï¼Ÿï¼‰
* LLMs are large and expensive to train and run å¤§æ¨¡å‹è®­ç»ƒå’Œè¿è¡Œæˆæœ¬é«˜ï¼Œè€Œæ•°æ®å­˜å‚¨å™¨å¯ä»¥åœ¨æ¨ç†æœŸé—´è¿›è¡Œæ£€ç´¢ï¼Œå› æ­¤å¯ä»¥**å‡å°‘æ¨¡å‹çš„å¤§å°å’Œæˆæœ¬** --Long-term goal: can we possibly reduce the training and inference costs, and scale down the size of LLMs?

## 2. Definition & Preliminaries

**1. A Retrieval-based LM: Definition** - A language model (LM) that usesan external datastore at test time åœ¨æµ‹è¯•æœŸé—´ä½¿ç”¨å¤–éƒ¨æ•°æ®å­˜å‚¨çš„è¯­è¨€æ¨¡å‹&#x20;

**2. A language model (LM): Categories**

<figure><img src="../../figure/image17.png" alt="" width="563"><figcaption></figcaption></figure>

è¿™é‡Œæœ‰ä¸€ä¸ªé—®é¢˜æ˜¯**ä¸ºä»€ä¹ˆDecoder-onlyæ¨¡å‹å‡ ä¹æˆä¸ºäº†ç°åœ¨LLMçš„ä¸»æµæ¶æ„**ï¼Ÿ&#x20;

å‚è€ƒåšå®¢ï¼š

[https://kexue.fm/archives/9529](https://kexue.fm/archives/9529)

[https://www.zhihu.com/question/588325646](https://www.zhihu.com/question/588325646)

ä¸»è¦è§‚ç‚¹: ä»»ä½•NLPä»»åŠ¡éƒ½å¯ä»¥åˆ†è§£ä¸ºâ€œè¾“å…¥â€è·Ÿâ€œè¾“å‡ºâ€ä¸¤éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¤„ç†â€œè¾“å…¥â€çš„æ¨¡å‹å«åšEncoderï¼Œç”Ÿæˆâ€œè¾“å‡ºâ€çš„æ¨¡å‹å«åšDecoderï¼Œé‚£ä¹ˆæ‰€æœ‰ä»»åŠ¡éƒ½å¯ä»¥ä»â€œEncoder-Decoderâ€çš„è§†è§’æ¥ç†è§£ï¼Œè€Œä¸åŒæ¨¡å‹ä¹‹é—´çš„å·®è·åœ¨äºEncoderã€Decoderçš„æ³¨æ„åŠ›æ¨¡å¼ä»¥åŠæ˜¯å¦å…±äº«å‚æ•°,æ¯”å¦‚:

| Model  | Encoder æ³¨æ„åŠ›  | Dncoder æ³¨æ„åŠ› | æ˜¯å¦å…±äº«å‚æ•°  |
| ------ | ------------ | ----------- | ------- |
| GPT    | å•å‘           | å•å‘          | æ˜¯       |
| UniLM  | åŒå‘           | å•å‘          | æ˜¯       |
| T5     | åŒå‘           | å•å‘          | å¦       |

{% code overflow="wrap" %}
```markdown

è¿™é‡Œçš„GPTå°±æ˜¯Decoder-onlyçš„ä»£è¡¨ä½œï¼›UniLMåˆ™æ˜¯è·ŸGPTç›¸ä¼¼çš„Decoderæ¶æ„ï¼Œä½†å®ƒæ˜¯æ··åˆçš„æ³¨æ„åŠ›æ¨¡å¼ï¼›T5åˆ™æ˜¯Encoder-Decoderæ¶æ„çš„ä»£è¡¨ä½œï¼Œä¸»è¦æ˜¯Googleæ¯”è¾ƒæ„Ÿå…´è¶£ã€‚

1. GPTå’ŒUniLMçš„å¯¹æ¯”å®éªŒï¼Œç»“æœè¯´æ˜ï¼šâ€œè¾“å…¥éƒ¨åˆ†çš„æ³¨æ„åŠ›æ”¹ä¸ºåŒå‘ä¸ä¼šå¸¦æ¥æ”¶ç›Šï¼ŒEncoder-Decoderæ¶æ„çš„ä¼˜åŠ¿å¾ˆå¯èƒ½åªæ˜¯æºäºå‚æ•°ç¿»å€ã€‚â€

2. ä½ç§©é—®é¢˜ï¼šAttentionçŸ©é˜µä¸€èˆ¬æ˜¯ç”±ä¸€ä¸ªä½ç§©åˆ†è§£çš„çŸ©é˜µåŠ softmaxè€Œæ¥ï¼Œå…·ä½“æ¥è¯´æ˜¯ä¸€ä¸ªnÃ—dçš„çŸ©é˜µä¸dÃ—nçš„çŸ©é˜µç›¸ä¹˜åå†åŠ softmaxï¼ˆnâ‰«dï¼‰ï¼Œè¿™ç§å½¢å¼çš„Attentionçš„çŸ©é˜µå› ä¸ºä½ç§©é—®é¢˜è€Œå¸¦æ¥è¡¨è¾¾èƒ½åŠ›çš„ä¸‹é™ï¼Œå…·ä½“åˆ†æå¯ä»¥å‚è€ƒã€ŠAttention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depthã€‹ã€‚è€ŒDecoder-onlyæ¶æ„çš„AttentionçŸ©é˜µæ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’é˜µï¼Œæ³¨æ„ä¸‰è§’é˜µçš„è¡Œåˆ—å¼ç­‰äºå®ƒå¯¹è§’çº¿å…ƒç´ ä¹‹ç§¯ï¼Œç”±äºsoftmaxçš„å­˜åœ¨ï¼Œå¯¹è§’çº¿å¿…ç„¶éƒ½æ˜¯æ­£æ•°ï¼Œæ‰€ä»¥å®ƒçš„è¡Œåˆ—å¼å¿…ç„¶æ˜¯æ­£æ•°ï¼Œå³Decoder-onlyæ¶æ„çš„AttentionçŸ©é˜µä¸€å®šæ˜¯æ»¡ç§©çš„ï¼Œæ»¡ç§©æ„å‘³ç€ç†è®ºä¸Šæœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ Decoder-onlyæ¶æ„çš„AttentionçŸ©é˜µåœ¨ç†è®ºä¸Šå…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ”¹ä¸ºåŒå‘æ³¨æ„åŠ›åè€Œä¼šå˜å¾—ä¸è¶³ã€‚ï¼ˆå¯ä»¥å‚è€ƒçº¿æ€§attentionç›¸å…³å†…å®¹ï¼šã€ŠAttention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depthã€‹&ã€ŠTransformerå‡çº§ä¹‹è·¯ã€‹ï¼‰

3. ã€ŠAttention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depthã€‹
è®ºæ–‡æœ€ä¸»è¦çš„ä¸€ä¸ªç»“è®ºæ˜¯decoder-onlyæ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•tuningæ•°æ®çš„æƒ…å†µä¸‹ã€zero-shotè¡¨ç°æœ€å¥½ï¼Œè€Œencoder-decoderåˆ™éœ€è¦åœ¨ä¸€å®šé‡çš„æ ‡æ³¨æ•°æ®ä¸Šåšmultitask finetuningæ‰èƒ½æ¿€å‘æœ€ä½³æ€§èƒ½ã€‚è€Œç›®å‰çš„Large LMçš„è®­ç»ƒèŒƒå¼è¿˜æ˜¯åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šåšè‡ªç›‘ç£å­¦ä¹ ï¼Œå¾ˆæ˜¾ç„¶ï¼ŒZero-Shotæ€§èƒ½æ›´å¥½çš„decoder-onlyæ¶æ„æ‰èƒ½æ›´å¥½åœ°åˆ©ç”¨è¿™äº›æ— æ ‡æ³¨æ•°æ®ã€‚æ­¤å¤–ï¼ŒInstruct GPTåœ¨è‡ªç›‘ç£å­¦ä¹ å¤–è¿˜å¼•å…¥äº†RLHFä½œè¾…åŠ©å­¦ä¹ ã€‚RLHFæœ¬èº«ä¹Ÿä¸éœ€è¦äººå·¥æä¾›ä»»åŠ¡ç‰¹å®šçš„æ ‡æ³¨æ•°æ®ï¼Œä»…éœ€è¦åœ¨LLMç”Ÿæˆçš„ç»“æœä¸Šä½œæ’åºã€‚è™½ç„¶ç›®å‰æ²¡æœ‰å¤ªå¤šæœ‰å…³RLHF + encoder-decoderçš„ç›¸å…³å®éªŒï¼Œç›´è§‰ä¸ŠRLHFå¸¦æ¥çš„æå‡å¯èƒ½è¿˜æ˜¯ä¸å¦‚multitask finetuningï¼Œæ¯•ç«Ÿå‰è€…æœ¬è´¨åªæ˜¯rankingã€å¼•å…¥ç›‘ç£ä¿¡å·æ²¡æœ‰åè€…å¼ºã€‚å³ï¼Œencoder-decoderåœ¨multitask finetuningä¸Šçš„ä¼˜åŠ¿åœ¨å¤§å‚æ•°é‡æ—¶è¢«LLMçš„æ¨ç†èƒ½åŠ›ç»™æ‹‰å¹³äº†ã€‚

```
{% endcode %}

é“¾æ¥é˜…è¯»ï¼š

[Transformerå‡çº§ä¹‹è·¯](https://spaces.ac.cn/archives/8338)

[Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth](https://arxiv.org/abs/2103.03404)

&#x20;**3. A language model (LM): Prompting**

å³é€šè¿‡ä¸åŒçš„promptè®©llmå®Œæˆä¸åŒçš„ä»»åŠ¡

**4. A language model (LM): Often evaluated with**&#x20;

ç»™å‡ºè¯„ä»·æŒ‡æ ‡ï¼š1. Perplexity  2. Downstream accuracy (Zero-shot or few-shot in-context learning,or fine-tuning) ä¼šåœ¨ç¬¬äº”èŠ‚è¯¦ç»†ä»‹ç»

ä¸€ä¸ªé—®é¢˜ï¼š**ä¸ºä»€ä¹ˆè¦ç”¨perplexityæ¥ä½œä¸ºæœ¬è¯¾ç¨‹çš„ä¸»è¦æŒ‡æ ‡**

â€œåœ¨æ¯”è¾ƒå‚æ•°åŒ–çš„è¯­è¨€æ¨¡å‹æ—¶ï¼Œå›°æƒ‘åº¦ï¼ˆPPLï¼‰ç»å¸¸è¢«ç”¨åˆ°ã€‚ä½†å›°æƒ‘åº¦çš„æ”¹å–„èƒ½å¦è½¬åŒ–ä¸ºä¸‹æ¸¸åº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼Œç°å·²æœ‰ç ”ç©¶è¡¨æ˜ï¼Œ<mark style="background-color:yellow;">å›°æƒ‘åº¦ä¸ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯ç”Ÿæˆä»»åŠ¡ï¼‰æœ‰å¾ˆå¥½çš„ç›¸å…³æ€§ï¼Œå¹¶ä¸”å›°æƒ‘åº¦é€šå¸¸å¯æä¾›éå¸¸ç¨³å®šçš„ç»“æœï¼Œå®ƒå¯ä»¥åœ¨å¤§è§„æ¨¡è¯„ä¼°æ•°æ®ä¸Šè¿›è¡Œè¯„ä¼°</mark>**ã€‚**(ç›¸å¯¹äºä¸‹æ¸¸ä»»åŠ¡æ¥è¯´ï¼Œè¯„ä¼°æ•°æ®æ˜¯æ²¡æœ‰æ ‡ç­¾çš„ï¼Œè€Œä¸‹æ¸¸ä»»åŠ¡å¯èƒ½ä¼šå—åˆ°æç¤ºçš„æ•æ„Ÿæ€§å’Œç¼ºä¹å¤§è§„æ¨¡æ ‡è®°æ•°æ®çš„å½±å“ï¼Œä»è€Œå¯¼è‡´ç»“æœä¸ç¨³å®šï¼‰ã€‚â€

**5. Inference: Datastore**

<figure><img src="../../figure/image18.png" alt="" width="375"><figcaption></figcaption></figure>

**6. Inference: Index**

ç›®æ ‡ï¼šåœ¨æ•°æ®å­˜å‚¨ä¸­æ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ä¸€å°éƒ¨åˆ†å…ƒç´ 

**sim**ï¼ša similarity score between two pieces of text ä¸‹é¢æ˜¯similarity scoreçš„ä¸€äº›ä¾‹å­

<figure><img src="../.gitbook/assets/image (1) (1) (1).png" alt="" width="375"><figcaption></figcaption></figure>

indexï¼šç»™å®šqueryï¼Œé€šè¿‡fast nearest neighbor searchï¼ˆè¿™ä¹Ÿæ˜¯ä¸€ä¸ªç ”ç©¶æ–¹å‘-å¦‚ä½•æ›´åŠ å¿«å¿«é€Ÿå’Œå‡†ç¡®ï¼‰ï¼Œè¾“å‡ºsimæœ€å¤§çš„kä¸ªå…ƒç´ 

ç›¸å…³software: FAISS, Distributed FAISS, SCaNN, etcâ€¦

å‚è€ƒï¼š[Faiss](https://github.com/facebookresearch/faiss/wiki)

## 3. Retrieval-based LM: Architecture

**1. Categorization of retrieval-based LMs**

![Alt text](../../figure/image20.png)

**2. Roadmap**

æ ¹æ® æ£€ç´¢ä»€ä¹ˆï¼Œå¦‚ä½•ä½¿ç”¨æ£€ç´¢ï¼Œåœ¨ä»€ä¹ˆæ—¶å€™æ£€ç´¢å°†æœ€è¿‘çš„ç ”ç©¶æ€»ç»“å±•ç¤ºåœ¨ä¸‹é¢çš„è·¯çº¿å›¾ï¼š

![Alt text](../../figure/image21.png)

### [REALM (Guu et al 2020)](https://arxiv.org/abs/2002.08909)--10 Feb 2020

æœ¬æ®µå¼€å§‹ä»‹ç»ç¬¬ä¸€ä¸ªç»“æ„ REALMï¼šRetrieval-Augmented Language Model Pre-Training--æ£€ç´¢å¢å¼ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹

<figure><img src="../../figure/image22.png" alt="" width="563"><figcaption></figcaption></figure>

çŸ¥ä¹ä¸Šä¸€äº›é˜…è¯»ç¬”è®°ï¼š

* [https://zhuanlan.zhihu.com/p/111255083](https://zhuanlan.zhihu.com/p/111255083)
* [https://zhuanlan.zhihu.com/p/360635601](https://zhuanlan.zhihu.com/p/360635601)

**åŠ¨æœº**ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹èƒ½å¤Ÿä»æ— ç›‘ç£æ–‡æœ¬è¯­æ–™ä¸­å­¦ä¹ åˆ°å¾ˆå¤šå…¬å…±çŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›çŸ¥è¯†å­˜å‚¨åœ¨å‚æ•°ä¸­ï¼Œæœ‰ä»¥ä¸‹ä¸¤ä¸ªç¼ºç‚¹ï¼š1. è¿™äº›çŸ¥è¯†æ˜¯éšå¼çš„ï¼Œä½¿ç”¨æ—¶éš¾ä»¥è§£é‡Šæ¨¡å‹å‚¨å­˜ã€ä½¿ç”¨çš„çŸ¥è¯†ï¼›2. æ¨¡å‹å­¦ä¹ åˆ°çš„çŸ¥è¯†çš„é‡çº§å’Œæ¨¡å‹å¤§å°ï¼ˆå‚æ•°é‡ï¼‰ç›¸å…³ï¼Œå› æ­¤ä¸ºäº†å­¦ä¹ åˆ°æ›´å¤šçš„çŸ¥è¯†ï¼Œéœ€è¦æ‰©å……æ¨¡å‹å¤§å°ã€‚

**é¢„è®­ç»ƒé˜¶æ®µçš„æµç¨‹**ï¼š1. ä»é¢„è®­ç»ƒè¯­æ–™ä¸­é‡‡æ · ï¼Œå¹¶å°†éƒ¨åˆ†token maskï¼ˆthe \[MASK] at the top of the pyramidï¼‰ï¼›2. é€šè¿‡æ£€ç´¢æ¨¡å—ï¼Œæ ¹æ®æ ·æœ¬ å»å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆå¦‚ç»´åŸºç™¾ç§‘æ–‡æ¡£ï¼‰ä¸­æ£€ç´¢èƒ½å¤Ÿå¸®åŠ©æ¢å¤mask tokençš„æ–‡æ¡£ ï¼ˆThe pyramidion on top allows for lessmaterial higher up the pyramidï¼‰ï¼›3. ä½¿ç”¨æ ·æœ¬ x å†…éƒ¨çš„ä¿¡æ¯ï¼Œä»¥åŠæ£€ç´¢åˆ°çš„æ–‡æ¡£ ä¸­çš„ä¿¡æ¯ï¼Œå…±åŒé¢„æµ‹è¢«maskæ‰çš„tokenï¼ˆpyramidionï¼‰ï¼›

**æ¨¡å‹ç»“æ„**ï¼šæ¨¡å‹çš„pre-trainingå’Œfine-tuningéƒ½å»ºæ¨¡ä¸ºretrieve-then-predictçš„è¿‡ç¨‹ï¼Œä½œè€…å°†$z$ è§†ä¸ºä¸€ä¸ªéšå˜é‡ï¼Œå°†æœ€åçš„ä»»åŠ¡ç›®æ ‡$y|x)$å»ºæ¨¡ä¸ºå¯¹äºæ‰€æœ‰æ½œåœ¨æ–‡æ¡£ $$z$$ çš„è¾¹ç¼˜æ¦‚ç‡ï¼š&#x20;

$$
p(y|x)=\sum_{z\in\mathcal{Z}}p(y|z,x)p(z|x)
$$

**ä¸¤ä¸ªéƒ¨åˆ†**ï¼šthe neural knowledge retriever(ç¥ç»çŸ¥è¯†æ£€ç´¢å™¨), -> $$p(z | x)$$, and the knowledge-augmented encoder(çŸ¥è¯†å¢å¼ºçš„encoder), -> $$p(y | z, x)$$.&#x20;

<figure><img src="../../figure/image23.png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src="../../figure/image24.png" alt="" width="563"><figcaption></figcaption></figure>

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä»»åŠ¡ä¸ºMLMï¼›åœ¨fine-tuneé˜¶æ®µï¼Œä»»åŠ¡ä¸ºOpen-domain QA

**è®­ç»ƒç»†èŠ‚**ï¼šé’ˆå¯¹æ•°æ®é‡è¾ƒå¤§çš„è§£å†³åŠæ³•--**pretrainingé˜¶æ®µä½¿ç”¨Maximum Inner Product Searchï¼ˆæœ€å¤§å†…ç§¯æœç´¢--å³å†…ç§¯ç©ºé—´ä¸‹çš„KNNï¼ŒMIPSï¼‰çš„ç®—æ³•æ¥æ‰¾åˆ°top-kä¸ªæœ€ç›¸å…³æ–‡æ¡£**ï¼Œä¸ºäº†é¿å…ä¸€ç›´åˆ·æ–°MIPSç´¢å¼•é€ æˆè€—æ—¶ä¸¥é‡ï¼Œæ¯éš”è‹¥å¹²stepæ‰åˆ·æ–°ä¸€æ¬¡MIPSç´¢å¼•ï¼ˆè¯¥ç´¢å¼•ä»…ç”¨æ¥é€‰æ‹©top-kä¸ªæ–‡æ¡£ï¼Œè€Œåœ¨æ¯ä¸€æ­¥è®­ç»ƒ[æ¢¯åº¦åä¼ ](acl2023-retrieval-lm.md#æ¢¯åº¦åä¼ )çš„æ—¶å€™ï¼Œä»ç„¶ä½¿ç”¨çš„æ˜¯æœ€æ–°çš„retreiverçš„å‚æ•°ï¼‰ã€‚**åœ¨fine-tuneé˜¶æ®µï¼ŒMIPSç´¢å¼•ä»…åœ¨ä¸€å¼€å§‹å»ºç«‹ä¸€æ¬¡**ï¼ˆä½¿ç”¨é¢„è®­ç»ƒçš„retrieverå‚æ•°ï¼‰ï¼Œä¹‹åä¾¿ä¸å†æ›´æ–°ã€‚ä½œè€…è®¤ä¸ºåœ¨é¢„è®­ç»ƒé˜¶æ®µæ£€ç´¢å™¨å°±å·²ç»å­¦ä¹ åˆ°äº†è¶³å¤Ÿå¥½çš„æ–‡æ¡£ç›¸å…³æ€§è¡¨å¾ï¼Œä½†ä½œè€…è®¤ä¸ºå¦‚æœåŒæ ·åœ¨fine-tuneé˜¶æ®µè¿­ä»£æ›´æ–°MIPSç´¢å¼•çš„è¯ï¼Œæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚

**trick**ï¼š1. Salient span maskingï¼ˆSSMï¼‰ï¼šå³åœ¨MLMé¢„è®­ç»ƒé˜¶æ®µï¼Œé®ç›–å…³é”®çš„å®ä½“/æ•°å­—ï¼Œè€Œä¸æ˜¯éšæœºtokenï¼›2. null documentï¼šéƒ¨åˆ†MLMæ ·æœ¬ä¸éœ€è¦å¤–éƒ¨æ–‡æ¡£æ”¯æŒï¼›3. é¿å…ä¿¡æ¯æ³„æ¼ï¼šå½“MLMçš„è®­ç»ƒè¯­æ–™å’Œæ£€ç´¢è¯­æ–™æœ‰é‡å æ—¶ï¼Œé¿å…ç›´æ¥æœç´¢åˆ°æ ·æœ¬xçš„åŸæ–‡ï¼›4. æ£€ç´¢å™¨çš„åˆå§‹åŒ–ã€å†·å¯åŠ¨é—®é¢˜ï¼šå¦‚æœä¸€å¼€å§‹éšæœºåˆå§‹åŒ–æ£€ç´¢å™¨ï¼Œé‚£ä¹ˆæ–‡æ¡£å°†ä¼šå¤§æ¦‚ç‡æ˜¯å®Œå…¨æ— å…³çš„ï¼Œæ¨¡å‹å¾—ä¸åˆ°æœ‰æ•ˆçš„æ¢¯åº¦ï¼›ä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…ä½¿ç”¨Inverse Cloze Testï¼ˆICTé€†å®Œå½¢å¡«ç©ºï¼‰ä»»åŠ¡æ¥åˆå§‹åŒ–è®­ç»ƒæ£€ç´¢å™¨ã€‚

**ç›¸å…³å·¥ä½œæ€»ç»“**ï¼š REALM and subsequent work

* **REALM (Guu et al 2020)**: MLM followed by fine-tuning, focusing on open-domain QA
* **DPR (Karpukhin et al 2020)**: Pipeline training instead of joint training, focusing on open-domain QA (no explicit language modeling)
* **RAG (Lewis et al 2020)**: â€œGenerativeâ€ instead of â€œmasked language modelingâ€, focusing on open-domain QA & knowledge intensive tasks (no explicit language modeling)
* **Atlas (Izcard et al 2022)**: Combine RAG with retrieval-based language model pre-training based on the encoder-decoder architecture (more to come in Section 4), focusing on open-domain QA & knowledge intensive tasks
* Papers that follow this approach focusing on LM perplexity have come out quite recently (Shi et al. 2023, Ram et al. 2023) ï¼š**Ram et al. 2023**. â€œIn-Context Retrieval-Augmented Language Modelsâ€&**Shi et al. 2023**.â€œREPLUG: Retrieval-Augmented Black-Box Language Models

### Retrieval-in-context LM

**ç›¸å…³è®ºæ–‡ï¼š**

[In-Context Retrieval-Augmented Language Models](https://arxiv.org/abs/2302.00083)

åœ¨ä¸Šé¢è¿™ç¯‡è®ºæ–‡ä¸­æœ‰ä¸€äº›å®éªŒç»“è®º:1. Retrieval helps overall sizes of LMs 2. A shorter prefix (more recent tokens) as a query helps 3. Retrieving more frequently helps(ä½†æ˜¯ä¼šæ¶ˆè€—æ›´å¤šçš„æ¨ç†æ—¶é—´æˆæœ¬)

[REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652)

![Alt text](../../figure/image25.png)

### [RETRO (Borgeaud et al. 2021)-ä»¥å°25å€å‚æ•°é‡åª²ç¾GPT-3çš„æ£€ç´¢å¢å¼ºè‡ªå›å½’è¯­è¨€æ¨¡å‹](https://arxiv.org/abs/2112.04426)

â€œIncorporation in the â€œintermediate layerâ€ instead of the â€œinputâ€ layer â†’ designed for many chunks, frequently, more efficientlyâ€

RETRO(Retrieval-Enhanced Transformer )-- improving language models through **explicit memory** at unprecedented scale

åˆå¹¶åˆ°ä¸­é—´å±‚è€Œä¸æ˜¯è¾“å…¥å±‚ + æ•°æ®è§„æ¨¡çš„å¢åŠ 

ç›¸å…³ç¬”è®°ï¼š

[https://zhuanlan.zhihu.com/p/475346411 ](https://zhuanlan.zhihu.com/p/475346411)

[https://www.cnblogs.com/Matrix\_Yao/p/16480698.html](https://www.cnblogs.com/Matrix\_Yao/p/16480698.html)

![Alt text](../../figure/image28.png)

**åŠ¨æœº**ï¼šæ¨¡å‹å‚æ•°â†‘ æ¨¡å‹æ•°æ®é‡â†‘ å®¹æ˜“å‘ç”Ÿæ•°æ®é›†éš¾ç†è§£ã€å¢åŠ æ¨¡å‹åå·®ç­‰ä¸€ç³»åˆ—é—®é¢˜ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒDeepMindå›¢é˜Ÿç ”å‘ä¸€ç§<mark style="background-color:yellow;">å¸¦æœ‰äº’è”ç½‘è§„æ¨¡æ£€ç´¢çš„é«˜æ•ˆé¢„è®­ç»ƒæ¨¡å‹</mark>ã€‚ä½¿ç”¨ RETROï¼Œ<mark style="background-color:yellow;">æ¨¡å‹ä¸ä»…é™äºè®­ç»ƒæœŸé—´çœ‹åˆ°çš„æ•°æ®ï¼Œå®ƒè¿˜å¯ä»¥é€šè¿‡æ£€ç´¢æœºåˆ¶è®¿é—®æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ã€‚</mark>ä¸å…·æœ‰ç›¸åŒæ•°é‡å‚æ•°çš„æ ‡å‡† Transformer ç›¸æ¯”ï¼Œè¿™ä¼šå¸¦æ¥æ˜¾ç€çš„æ€§èƒ½æå‡ã€‚

**æ•°æ®é›†**ï¼š[MassiveTextæ•°æ®é›†](https://paperswithcode.com/dataset/massivetext)(æ¥è‡ªgopheræ¨¡å‹è®ºæ–‡)

æå‡ºäº†ä¸€ç§**é¿å…æ•°æ³„éœ²çš„æ–¹æ³•**ï¼šæ£€ç´¢çš„è¿‡ç¨‹å°±èƒ½ç›´æ¥è®¿é—®è®­ç»ƒé›†æ‰€ä»¥é˜²æ­¢æ•°æ®æ³„éœ²å¾ˆé‡è¦-ä¸ºæ­¤è®ºæ–‡ä½œè€…æå‡ºäº†ä¸€ç§è¡¡é‡æµ‹è¯•æ–‡æ¡£ä¸è®­ç»ƒé›†æ¥è¿‘ç¨‹åº¦çš„è¯„ä¼°æ–¹å¼[Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)

**æ¨¡å‹ç»“æ„** RETROæ¨¡å‹æ¶æ„ç”±ä¸€ä¸ªç¼–ç å™¨å †æ ˆï¼ˆå¤„ç†è¿‘é‚»ï¼‰å’Œä¸€ä¸ªè§£ç å™¨å †æ ˆï¼ˆå¤„ç†è¾“å…¥ï¼‰ç»„æˆï¼š ç¼–ç å™¨å †æ ˆç”±æ ‡å‡†çš„ Transformer ç¼–ç å™¨å—ç»„æˆï¼›è§£ç å™¨å †æ ˆåŒ…å«äº†Transformerè§£ç å™¨å—å’ŒRETRO è§£ç å™¨å—ï¼ˆATTN + **Chunked cross attention (CCA)** + FFNNï¼ˆFeed-forward neural networkï¼‰ï¼‰ã€‚

* [ ] _è¿™é‡Œæ ‡æ³¨å’Œè¡¥å……_

![Alt text](../../figure/image26.png)

ç®€åŒ–æµç¨‹ï¼š&#x20;

<figure><img src="../../figure/image27.png" alt="" width="532"><figcaption></figcaption></figure>

å¯¹æ¯”ï¼š

<figure><img src="figure/image29.png" alt="" width="563"><figcaption></figcaption></figure>

**æ€è€ƒ**ï¼šé™¤äº†æ£€ç´¢splitæˆchunksï¼Œè¿˜å¯ä»¥æ€ä¹ˆå¤„ç†dbä¸­çš„æ•°æ®ï¼Ÿ

â†“

### [kNN-LM (Khandelwal et al. 2020) -- Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172)

æå‡ºkNN-LMsï¼ŒæŠŠè¯­ä¹‰ç¼–ç ç‰¹å¾å‘é‡çš„kæœ€è¿‘é‚»å’Œä¸€èˆ¬çš„è¯­è¨€æ¨¡å‹ç»“åˆä»è€Œæ˜¾è‘—æé«˜è¯­è¨€æ¨¡å‹çš„æ•ˆæœ

* â€œA different way of using retrieval, where the LM outputs a nonparametric distribution over every token in the data.â€ å¦ä¸€ç§ä½¿ç”¨æ£€ç´¢çš„æ–¹æ³•ï¼Œå…¶ä¸­LMåœ¨æ•°æ®ä¸­çš„æ¯ä¸ªæ ‡è®°ä¸Šè¾“å‡ºä¸€ä¸ªéå‚æ•°åˆ†å¸ƒã€‚
* â€œCan be seen as an incorporation in the â€˜outputâ€™ layerâ€ å¯ä»¥çœ‹åšæ˜¯åœ¨è¾“å‡ºå±‚çš„ä¸€ä¸ªåˆå¹¶

**åŠ¨æœº**ï¼šè¯­è¨€æ¨¡å‹ï¼ˆLanguage Model, LMï¼‰æŒ‡çš„æ˜¯åˆ©ç”¨é“¾å¼æ³•åˆ™ç»™å‡ºä¸€ä¸ªå¥å­çš„æ¦‚ç‡ï¼Œä¸»è¦è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼šï¼ˆ1ï¼‰å¾—åˆ°ä¸Šæ–‡è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰ç”¨ä¸Šæ–‡è¡¨ç¤ºé¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚è¿™ä¸¤ä¸ªé—®é¢˜ä¸€èˆ¬ä½¿ç”¨ä¸€ä¸ªautoregressiveæ¨¡å‹è§£å†³ã€‚ä½¿ç”¨ARæ¨¡å‹å»è¿›è¡Œè¯­è¨€å»ºæ¨¡çš„ä¸€ä¸ªæ™®éé—®é¢˜æ˜¯ï¼š**éš¾ä»¥å……åˆ†å»ºç«‹é•¿è·ç¦»ä¾èµ–**ã€‚ç”±æ­¤å‡ºå‘ï¼Œæœ¬æ–‡æå‡ºé€šè¿‡è®¡ç®—ä¸Šæ–‡è¡¨ç¤ºçš„kæœ€è¿‘é‚»å»ç»“åˆè¯­è¨€æ¨¡å‹ä»è€Œ**æ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»**ã€‚

**æ¨¡å‹ç»“æ„ï¼š**

<figure><img src="figure/image30.png" alt="" width="563"><figcaption></figcaption></figure>

å…·ä½“çš„æµç¨‹å¯ä»¥å»çœ‹slideè®²çš„å¾ˆæ¸…æ¥š

**æ¨¡å‹å®éªŒç»“æœ:**

&#x20;&#x20;

<figure><img src="figure/image31.png" alt="" width="563"><figcaption></figcaption></figure>

<figure><img src="figure/image32.png" alt="" width="563"><figcaption></figcaption></figure>

Can use in-domain datastore even if parameters were not trained in-domain

**å¯¹æ¯”æ€»ç»“**ï¼š

KNN-LMçš„ä¼˜ç‚¹:æ›´ç»†ç²’åº¦ï¼›å¯ä»¥æ›´å¥½åœ°å¤„ç†ç½•è§çš„æ¨¡å¼&åŸŸå¤–æ•°æ®ï¼Œå¯ä»¥éå¸¸é«˜æ•ˆï¼ˆå› ä¸ºKNNæœç´¢å¾ˆå¿«ï¼‰

ç¼ºç‚¹: <mark style="background-color:yellow;">è¾“å…¥å’Œæ£€ç´¢ç»“æœä¹‹é—´æ²¡æœ‰äº¤å‰æ³¨æ„ï¼›Datastoreæ¶ˆè€—æ¯”è¾ƒå¤§</mark>&#x20;

<figure><img src="figure/image33.png" alt="" width="563"><figcaption></figcaption></figure>

**æ€è€ƒ**: åœ¨when to retrieveä¸­ï¼Œevery n tokenså’Œevery tokensæ˜¯å¦å¯ä»¥å»åš adaptive ï¼Ÿ â†“

### Adaptive retrieval for efficiency

åˆ†ä¸ºä¸¤ç±»ï¼šAdaptive retrieval of **text chunks** (following retrieve-in-context)ï¼›Adaptive retrieval of **tokens** (following kNN-LM)

#### [Active Retrieval Augmented Generation--Forward-Looking Active REtrieval augmented generation(FLARE)](https://arxiv.org/abs/2305.06983)

**åŠ¨æœºå’Œæ¦‚è¿°**ï¼šå¤§å¤šæ•°ç°æœ‰çš„æ£€ç´¢å¢å¼ºå‹è¯­è¨€æ¨¡å‹éƒ½é‡‡ç”¨retrieve-and-generateè®¾ç½®ï¼Œæ ¹æ®queryè¿›è¡Œä¸€æ¬¡ä¿¡æ¯æ£€ç´¢ã€‚ç„¶è€Œï¼Œåœ¨æ¶‰åŠç”Ÿæˆé•¿æ–‡æœ¬çš„æ›´ä¸€èˆ¬åœºæ™¯ä¸­ï¼Œ**åœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­ä¸æ–­æ”¶é›†ä¿¡æ¯**è‡³å…³é‡è¦ã€‚è¿‡å»å·²ç»æœ‰ä¸€äº›åœ¨ç”Ÿæˆè¾“å‡ºæ—¶å¤šæ¬¡æ£€ç´¢ä¿¡æ¯çš„åŠªåŠ›ï¼Œè¿™äº›åŠªåŠ›å¤§å¤šä½¿ç”¨å…ˆå‰çš„ä¸Šä¸‹æ–‡ä½œä¸ºæŸ¥è¯¢ä»¥å›ºå®šçš„æ—¶é—´é—´éš”æ£€ç´¢æ–‡æ¡£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œ**æˆ‘ä»¬æä¾›äº†ä¸»åŠ¨æ£€ç´¢å¢å¼ºç”Ÿæˆ**çš„æ¦‚æ‹¬è§†å›¾ï¼Œ**å³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸»åŠ¨å†³å®šä½•æ—¶æ£€ç´¢ä»¥åŠæ£€ç´¢ä»€ä¹ˆå†…å®¹çš„æ–¹æ³•**ã€‚æˆ‘ä»¬æå‡ºäº†å‰ç»æ€§ä¸»åŠ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆFLAREï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œå®ƒè¿­ä»£åœ°ä½¿ç”¨å¯¹å³å°†åˆ°æ¥çš„å¥å­çš„é¢„æµ‹æ¥é¢„æµ‹æœªæ¥çš„å†…å®¹ï¼Œç„¶åå¦‚æœå®ƒåŒ…å«ä½å¯ä¿¡åº¦ä»¤ç‰Œï¼Œåˆ™å°†å…¶ç”¨ä½œæŸ¥è¯¢æ¥æ£€ç´¢ç›¸å…³æ–‡æ¡£ä»¥é‡æ–°ç”Ÿæˆå¥å­ã€‚

ç”±é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å¼•å‡ºï¼š**ä¸€æ¬¡æ£€ç´¢å¹¶ä¸èƒ½æ»¡è¶³éœ€è¦**ï¼Œä¸äººç±»åœ¨åˆ›å»ºè®ºæ–‡æˆ–ä¹¦ç±ç­‰å†…å®¹æ—¶é€æ¸æ”¶é›†ä¿¡æ¯çš„æ–¹å¼ç±»ä¼¼ï¼Œä½¿ç”¨ LM è¿›è¡Œé•¿æ ¼å¼ç”Ÿæˆéœ€è¦åœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­æ”¶é›†å¤šç§çŸ¥è¯†ã€‚ æœ¬æ–‡é‡‡å–çš„æ–¹æ³•æ˜¯é€šè¿‡ç”Ÿæˆä¸´æ—¶çš„ä¸‹ä¸€ä¸ªå¥å­ï¼Œå°†å…¶ä½œä¸ºæ£€ç´¢ç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢ï¼Œç„¶åæ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£é‡æ–°ç”Ÿæˆä¸‹ä¸€ä¸ªå¥å­æ¥é¢„æµ‹æœªæ¥ã€‚

FLAREè¿­ä»£ç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„ä¸‹ä¸€ä¸ªå¥å­ï¼Œå¦‚æœå®ƒåŒ…å«low-probability tokensï¼Œåˆ™å°†å…¶ç”¨ä½œæ£€ç´¢ç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢ï¼Œå¹¶é‡æ–°ç”Ÿæˆä¸‹ä¸€ä¸ªå¥å­å¥å­ç›´åˆ°ç»“æŸã€‚

**æ€è€ƒ**ï¼šä»€ä¹ˆæ˜¯<mark style="color:green;">low-probability tokens</mark> å¦‚ä½•ç•Œå®š

![Alt text](figure/image34.png)

è¯¦ç»†æµç¨‹å‚è€ƒslides

#### Adaptive retrieval of tokens -Judge necessity-- [Efficient Nearest Neighbor Language Models](https://arxiv.org/abs/2109.04212)

<figure><img src="../.gitbook/assets/image (6).png" alt="" width="563"><figcaption></figcaption></figure>

#### Adaptive retrieval of tokens Use local info -- RETOMATON -- [Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval](https://arxiv.org/abs/2201.12431)

![Alt text](figure/image36.png)

**æ€»ç»“**ï¼š

&#x20;

<figure><img src="figure/image37.png" alt="" width="563"><figcaption></figcaption></figure>

æ€è€ƒ: What else beyond text chunks and tokens to retrieve? â†“

### å®ä½“ä¸“å®¶æ¨¡å‹

#### [Entities as Experts:Entities as Experts: Sparse Memory Access with Entity Supervision](https://arxiv.org/abs/2004.07202)

Introduce a new modelâ€”Entities as Experts (EAE)that can access distinct memories of the entities mentioned in a piece of text . æå‡ºâ€œå®ä½“ä¸“å®¶â€æ¨¡å‹ï¼Œå¯ä»¥è®¿é—®æ–‡æœ¬ä¸­æåˆ°çš„å®ä½“çš„ä¸åŒmemoriesï¼Œä¸å…¶ä»–å°†å®ä½“ç‰¹å®šçŸ¥è¯†æ³¨å…¥åºåˆ—æ¨¡å‹çš„åŠªåŠ›ä¸åŒï¼Œæœ¬æ¨¡å‹ä»æ–‡æœ¬ä¸­å­¦ä¹ å®ä½“è¡¨ç¤ºä»¥åŠæ‰€æœ‰å…¶ä»–æ¨¡å‹å‚æ•°ã€‚

![Alt text](../figure/image39.png) ä¸Šå›¾å¯ä¸çœ‹åˆ°ï¼Œä¼ ç»Ÿçš„Transformeréœ€è¦æ ¹æ®â€œCharlesâ€å’Œâ€œDarwinâ€è¿™ä¸¤ä¸ªè¯æ„å»º Charles Darwin çš„å†…éƒ¨è¡¨ç¤ºï¼Œè¿™ä¸¤ä¸ªè¯éƒ½å¯ä»¥ä¹ŸæŒ‡ä¸åŒçš„å®ä½“ï¼Œä¾‹å¦‚æŸ¥å°”æ–¯æ²³æˆ–è¾¾å°”æ–‡å¸‚ã€‚ç›¸åï¼ŒEAE å¯ä»¥è®¿é—®â€œæŸ¥å°”æ–¯Â·è¾¾å°”æ–‡â€çš„ä¸“ç”¨è¡¨ç¤ºï¼Œå®ƒæ˜¯å…ˆå‰æåˆ°è¿‡è¯¥å®ä½“çš„æ‰€æœ‰ä¸Šä¸‹æ–‡çš„è®°å¿†ã€‚

![Alt text](figure/image38.png)

#### ä»æ¯ä¸ªå®ä½“ä¸€ä¸ªå‘é‡åˆ°æ¯ä¸ªå®ä½“æåŠä¸€ä¸ªå‘é‡çš„è½¬å˜--[Mention Memory:incorporating textual knowledge into Transformers through entity mention attention](https://arxiv.org/abs/2110.06176)é€šè¿‡å®ä½“æåŠæ³¨æ„åŠ›å°†æ–‡æœ¬çŸ¥è¯†èå…¥transformerä¸­

**æ‘˜è¦ç¿»è¯‘**ï¼š

è¯¸å¦‚å¼€æ”¾åŸŸé—®ç­”ä¹‹ç±»çš„è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡é€šå¸¸éœ€è¦ä»å¤šä¸ªæ¥æºæ£€ç´¢å’Œå¸æ”¶äº‹å®ä¿¡æ¯ã€‚æˆ‘ä»¬å»ºè®®é€šè¿‡**å°†å¤§å‹æ–‡æœ¬è¯­æ–™åº“çš„åŠå‚æ•°è¡¨ç¤ºé›†æˆåˆ° Transformer æ¨¡å‹ä¸­ä½œä¸ºäº‹å®çŸ¥è¯†çš„æ¥æºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚**

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”¨â€œæåŠè®°å¿†â€æ¥è¡¨ç¤ºçŸ¥è¯†ï¼Œâ€œæåŠè®°å¿†â€æ˜¯è¯­æ–™åº“ä¸­æåŠçš„æ¯ä¸ªå®ä½“çš„å¯†é›†å‘é‡è¡¨ç¤ºè¡¨ã€‚æ‰€æå‡ºçš„æ¨¡å‹ - TOME - æ˜¯ä¸€ä¸ª Transformerï¼Œå®ƒé€šè¿‡å†…éƒ¨è®°å¿†å±‚è®¿é—®ä¿¡æ¯ï¼Œå…¶ä¸­è¾“å…¥æ®µè½ä¸­æåŠçš„æ¯ä¸ªå®ä½“éƒ½æ¶‰åŠæåŠè®°å¿†ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨å•ä¸ª Transformer æ¨¡å‹ä¸­å¯¹è®¸å¤šä¸åŒçš„ä¿¡æ¯æºè¿›è¡Œç»¼åˆå’Œæ¨ç†ã€‚åœ¨ä½¿ç”¨ 1.5 äº¿æ¡ç»´åŸºç™¾ç§‘æåŠçš„å†…å­˜è¿›è¡Œçš„å®éªŒä¸­ï¼ŒTOME åœ¨å¤šä¸ªå¼€æ”¾é¢†åŸŸçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å£°æ˜éªŒè¯åŸºå‡† HoVer å’Œ FEVER ä»¥åŠå¤šä¸ªåŸºäºå®ä½“çš„ QA åŸºå‡†ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•ç›´æ¥ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¼šäº†å…³æ³¨informative mentionsã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜è¯¥æ¨¡å‹å¯ä»¥é€šè¿‡æ›´æ–°å†…å­˜è€Œæ— éœ€é‡æ–°è®­ç»ƒæ¥æ¨å¹¿åˆ°æ–°çš„çœ‹ä¸è§çš„å®ä½“ã€‚

![Alt text](figure/image40.png)

#### **æ€»ç»“ï¼š**

![Alt text](figure/image41.png)

ä¼˜åŠ¿ï¼šå¯¹äºä»¥å®ä½“ä¸ºä¸­å¿ƒçš„ä»»åŠ¡å¾ˆæœ‰æ•ˆ&ç©ºé—´é«˜æ•ˆ

åŠ£åŠ¿ï¼šéœ€è¦é¢å¤–çš„å®ä½“æ£€æµ‹

ä¸Šé¢æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯åŸºäºå¤–éƒ¨æ–‡æœ¬çš„ï¼Œè¿˜æœ‰å…¶ä»–æ–¹æ³•å—ï¼Ÿâ†“

### Retrieval for long-range LM

#### [Wu et al. 2022. Memorizing Transformers (Figure source)](https://arxiv.org/abs/2203.08913)

è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒæ‰èƒ½è·å–æ–°çŸ¥è¯†ï¼Œè¿™æ¶‰åŠæ›´æ–°å…¶æƒé‡ã€‚ç›¸åï¼Œæˆ‘ä»¬è®¾æƒ³è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨æ¨ç†æ—¶ç®€å•åœ°è¯»å–å’Œè®°å¿†æ–°æ•°æ®ï¼Œä»è€Œç«‹å³è·å–æ–°çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ‰©å±•äº†è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè®°ä½è¿‡å»è¾“å…¥çš„å†…éƒ¨è¡¨ç¤ºã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯¹æœ€è¿‘ï¼ˆé”®ã€å€¼ï¼‰å¯¹çš„ä¸å¯å¾®è®°å¿†è¿›è¡Œè¿‘ä¼¼ kNN æŸ¥æ‰¾å¯ä»¥æ”¹è¿›è·¨å„ç§åŸºå‡†å’Œä»»åŠ¡çš„è¯­è¨€å»ºæ¨¡ï¼ŒåŒ…æ‹¬é€šç”¨ç½‘ç»œæ–‡æœ¬ (C4)ã€æ•°å­¦è®ºæ–‡ (arXiv)ã€ä¹¦ç± (PG-19)ã€ä»£ç ï¼ˆGithubï¼‰ï¼Œä»¥åŠå½¢å¼å®šç†ï¼ˆIsabelleï¼‰ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“æˆ‘ä»¬å°†å†…å­˜å¤§å°å¢åŠ åˆ° 262K ä»¤ç‰Œæ—¶ï¼Œæ€§èƒ½ä¼šç¨³æ­¥æé«˜ã€‚åœ¨åŒ…æ‹¬ä»£ç å’Œæ•°å­¦åœ¨å†…çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æµ‹è¯•æœŸé—´ä½¿ç”¨æ–°å®šä¹‰çš„å‡½æ•°å’Œå®šç†ã€‚(åŸºäºKNNå»åšæ£€ç´¢)

å¯¹é•¿åºåˆ—çš„æ³¨æ„åŠ›ä½œä¸ºå¿«é€Ÿå­¦ä¹ çš„ä¸€ç§å½¢å¼ä¹Ÿå¾ˆæœ‰ç”¨ã€‚ä»¥æƒé‡çŸ©é˜µå½¢å¼å­˜å‚¨çš„äº‹å®å’Œä¿¡æ¯å¿…é¡»ç»è¿‡æ•°åä¸‡ä¸ªè®­ç»ƒæ­¥éª¤ç¼“æ…¢è®­ç»ƒã€‚ç„¶è€Œï¼Œé€šè¿‡ä½¿ç”¨æ³¨æ„åŠ›ï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡å°†äº‹å®ï¼ˆä¾‹å¦‚å‡½æ•°å®šä¹‰ï¼‰ä½œä¸ºï¼ˆé”®ï¼Œå€¼ï¼‰å¯¹å­˜å‚¨åœ¨é•¿æœŸè®°å¿†ä¸­æ¥ç®€å•åœ°è®°ä½å®ƒä»¬ï¼Œç„¶åé€šè¿‡åˆ›å»ºå…³æ³¨å®ƒä»¬çš„æŸ¥è¯¢æ¥æ£€ç´¢è¿™äº›äº‹å®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›å……å½“ä¿¡æ¯æ£€ç´¢çš„ä¸€ç§å½¢å¼ï¼Œå…è®¸æ¨¡å‹æŸ¥æ‰¾å®ƒä»¥å‰è§è¿‡çš„äº‹å®ã€‚

![Alt text](../../figure/image42.png)

â†‘æ‰©å±• Transformer æ¥è®¿é—®å…ˆå‰çœ‹åˆ°çš„å­åºåˆ—çš„ï¼ˆé”®ï¼Œå€¼ï¼‰å¯¹ã€‚

**Bertsch** et al. 2023. Unlimiformer: Long-Range Transformers with Unlimited Length Input

## é™„å½•ï¼šæ¦‚å¿µè¡¥å……

### æ¢¯åº¦åä¼ 

å…¶å®å°±æ˜¯æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­ å‚è€ƒï¼š[https://atcold.github.io/pytorch-Deep-Learning/zh/week02/02-1/](https://atcold.github.io/pytorch-Deep-Learning/zh/week02/02-1/)

### æ¢¯åº¦åè½¬

ç”¨äºé¢†åŸŸè‡ªé€‚åº” å‚è€ƒï¼š [https://zhuanlan.zhihu.com/p/75470256](https://zhuanlan.zhihu.com/p/75470256)
