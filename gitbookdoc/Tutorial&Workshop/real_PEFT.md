---
description: å«realpeftæ˜¯å› ä¸ºæ„Ÿè§‰ç°åœ¨çš„peftæ–¹å¼æœ‰æ¯”è¾ƒå¤§çš„é—®é¢˜ï¼Œå…·ä½“å“ªé‡Œæ€ªæ€ªçš„ç­‰æˆ‘æœ‰äº†æ¯”è¾ƒæˆç†Ÿçš„æƒ³æ³•å†è¯´QAQ
---

# ğŸ§ Real\_PEFT

### ç°æœ‰çš„PEFTæ–¹æ³•æ€»ç»“

#### LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS

#### Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks

#### P-Tuning: GPT Understands, Too

#### Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning

#### AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning

#### LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention

#### IA3: Infused Adapter by Inhibiting and Amplifying Inner Activations
