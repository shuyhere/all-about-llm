---
description: yao fu
---

# ğŸ”† æ‹†è§£è¿½æº¯ GPT-3.5 å„é¡¹èƒ½åŠ›çš„èµ·æº

åŸæ–‡é“¾æ¥ï¼š[https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)

ä¸Šä¸‹æ–‡å­¦ä¹ ï¼š[https://thegradient.pub/in-context-learning-in-context/](https://thegradient.pub/in-context-learning-in-context/)

## åŸæ–‡ï¼ˆ+ä¸€äº›ç¬”è®°ï¼‰

[**ç¬¦å°§**](https://franxyao.github.io), [yao.fu@ed.ac.uk](mailto:yao.fu@ed.ac.uk)

çˆ±ä¸å ¡å¤§å­¦åšå£«ç”Ÿï¼Œç¡•å£«æ¯•ä¸šäºå“¥ä¼¦æ¯”äºšå¤§å­¦ï¼Œæœ¬ç§‘æ¯•ä¸šäºåŒ—äº¬å¤§å­¦

ä¸ [**å½­æ˜Š**](https://haopeng-nlp.github.io)ï¼Œ[\*\*Tushar Khot](https://allenai.org/team/tushark)\*\*

åœ¨è‰¾ä¼¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ (Allen Institute for AI) å…±åŒå®Œæˆè‹±æ–‡åŸç¨¿

ä¸ å‰‘æ¡¥å¤§å­¦[**éƒ­å¿—æ±Ÿ**](https://cartus.github.io) å…±åŒç¿»è¯‘ä¸ºä¸­æ–‡

æ„Ÿè°¢ ä¸Šæµ·äº¤é€šå¤§å­¦[**ä½•ä¿Šè´¤**](https://jxhe.github.io)ï¼ŒåŠ å·å¤§å­¦æ´›æ‰çŸ¶åˆ†æ ¡[**é²ç›¼**](https://lupantech.github.io)ï¼Œè¾¾ç‰¹èŒ…æ–¯å­¦é™¢[**åˆ˜ç¿åš**](https://www.cs.dartmouth.edu/\~rbliu/) å¯¹åˆç¨¿çš„è®¨è®ºä¸å»ºè®®ã€‚

æ„Ÿè°¢ [**Raj Ammanabrolu**](http://prithvirajva.com) (Allen Institute for AI), [**Peter Liu**](https://peterjliu.com) (Google Brain), [**Brendan Dolan-Gavitt**](https://www.notion.so/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1?pvs=21) (New York University), [**Denny Zhou**](https://dennyzhou.github.io) (Google Brain) å¯¹ç»ˆç¨¿çš„è®¨è®ºå’Œå»ºè®®ï¼Œä»–ä»¬çš„å»ºè®®æå¤§ç¨‹åº¦ä¸Šå¢åŠ äº†æœ¬æ–‡çš„å®Œæ•´åº¦ã€‚

è‹±æ–‡ç‰ˆå®Œç¨¿äº 2022 å¹´ 12 æœˆ 11 æ—¥ï¼Œä¸­æ–‡ç‰ˆå®Œç¨¿äº 2022 å¹´ 12 æœˆ 18 æ—¥ã€‚

å…¶ä»–ç‰ˆæœ¬: \[pdf] \[Arxiv] \[[è‹±æ–‡åŸç‰ˆ](https://www.notion.so/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1?pvs=21)] \[å­¦æœ¯å¼•ç”¨]

åœ¨[æ¨ç‰¹](https://twitter.com/Francis\_YAO\_/status/1602213927102066688?s=20\&t=9wkRcr0wva\_RCaKpsRjFfw)ä¸Šä¸ä½œè€…äº’åŠ¨

åˆæ¬¡ç¿»è¯‘ï¼Œå“ªé‡Œæ²¡å†™å¥½ï¼Œä¸åœ°é“çš„åœ°æ–¹ï¼Œè¿˜è¯·é‚®ä»¶å¸®å¿™æŒ‡å‡º

è½¬å‘è¯·åœ¨æ–‡ç« çš„å¼€å¤´æ ‡æ˜å‡ºå¤„ï¼Œè€Œä¸æ˜¯åœ¨ç»“å°¾åˆ—ä¸€è¡Œå°å­—



æœ€è¿‘ï¼ŒOpenAIçš„é¢„è®­ç»ƒæ¨¡å‹ChatGPTç»™äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶äººå‘˜ç•™ä¸‹äº†æ·±åˆ»çš„å°è±¡å’Œå¯å‘ã€‚æ¯«æ— ç–‘é—®ï¼Œå®ƒåˆå¼ºåˆèªæ˜ï¼Œä¸”è·Ÿå®ƒè¯´è¯å¾ˆå¥½ç©ï¼Œè¿˜ä¼šå†™ä»£ç ã€‚å®ƒåœ¨å¤šä¸ªæ–¹é¢çš„èƒ½åŠ›è¿œè¿œè¶…è¿‡äº†è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶è€…ä»¬çš„é¢„æœŸã€‚äºæ˜¯æˆ‘ä»¬è‡ªç„¶å°±æœ‰ä¸€ä¸ªé—®é¢˜ï¼šChatGPT æ˜¯æ€ä¹ˆå˜å¾—è¿™ä¹ˆå¼ºçš„ï¼Ÿå®ƒçš„å„ç§å¼ºå¤§çš„èƒ½åŠ›åˆ°åº•ä»ä½•è€Œæ¥ï¼Ÿåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾å‰–æ ChatGPT çš„çªç°èƒ½åŠ›ï¼ˆEmergent Abilityï¼‰ï¼Œè¿½æº¯è¿™äº›èƒ½åŠ›çš„æ¥æºï¼Œå¸Œæœ›èƒ½å¤Ÿç»™å‡ºä¸€ä¸ªå…¨é¢çš„æŠ€æœ¯è·¯çº¿å›¾ï¼Œæ¥è¯´æ˜ GPT-3.5 æ¨¡å‹ç³»åˆ—ä»¥åŠç›¸å…³çš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥è¿›åŒ–æˆç›®å‰çš„å¼ºå¤§å½¢æ€ã€‚

æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¤Ÿä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ˜åº¦ï¼Œæˆä¸ºå¼€æºç¤¾åŒºå…±åŒåŠªåŠ›å¤ç° GPT-3.5 çš„è·¯çº¿å›¾ã€‚

Recently, the field has been greatly impressed and inspired by OpenAIâ€™s ChatGPT. It is undoubtedly clever, capable, and very fun to talk to. Its multi-faceted abilities are significantly beyond many NLP researchersâ€™ and practitionersâ€™ expectations based on the impression of (not-that-strong) original GPT-3. The natural question is how ChatGPT gets there, and where these fantastic abilities come from. In this post, we try to dissect the emergent abilities and trace them to their sources, hoping to give a comprehensive roadmap about how the GPT-3.5 model family, along with related large language models, evolved to their current forms.

We hope this post can promote the transparency of large language models and serve as the roadmap for the communityâ€™s ongoing efforts of reproducing GPT-3.5.

### &#x20;ä¸€ã€2020 ç‰ˆåˆä»£ GPT-3 ä¸å¤§è§„æ¨¡é¢„è®­ç»ƒ

åˆä»£GPT-3å±•ç¤ºäº†ä¸‰ä¸ªé‡è¦èƒ½åŠ›ï¼š

* **è¯­è¨€ç”Ÿæˆ**ï¼šéµå¾ªæç¤ºè¯ï¼ˆpromptï¼‰ï¼Œç„¶åç”Ÿæˆè¡¥å…¨æç¤ºè¯çš„å¥å­ (completion)ã€‚è¿™ä¹Ÿæ˜¯ä»Šå¤©äººç±»ä¸è¯­è¨€æ¨¡å‹æœ€æ™®éçš„äº¤äº’æ–¹å¼ã€‚
* **ä¸Šä¸‹æ–‡å­¦ä¹  (in-context learning)**: éµå¾ªç»™å®šä»»åŠ¡çš„å‡ ä¸ªç¤ºä¾‹ï¼Œç„¶åä¸ºæ–°çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚å¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯ï¼ŒGPT-3è™½ç„¶æ˜¯ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒçš„è®ºæ–‡å‡ ä¹æ²¡æœ‰è°ˆåˆ°â€œè¯­è¨€å»ºæ¨¡â€ (language modeling) â€”â€” ä½œè€…å°†ä»–ä»¬å…¨éƒ¨çš„å†™ä½œç²¾åŠ›éƒ½æŠ•å…¥åˆ°äº†å¯¹ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ„¿æ™¯ä¸Šï¼Œè¿™æ‰æ˜¯ GPT-3çš„çœŸæ­£é‡ç‚¹ã€‚
* **ä¸–ç•ŒçŸ¥è¯† (world knowledge)**ï¼šåŒ…æ‹¬äº‹å®æ€§çŸ¥è¯† (factual knowledge) å’Œå¸¸è¯† (commonsense)ã€‚

é‚£ä¹ˆè¿™äº›èƒ½åŠ›ä»ä½•è€Œæ¥å‘¢ï¼Ÿ

åŸºæœ¬ä¸Šï¼Œä»¥ä¸Šä¸‰ç§èƒ½åŠ›éƒ½æ¥è‡ªäºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼šåœ¨æœ‰3000äº¿å•è¯çš„è¯­æ–™ä¸Šé¢„è®­ç»ƒæ‹¥æœ‰1750äº¿å‚æ•°çš„æ¨¡å‹ï¼ˆ è®­ç»ƒè¯­æ–™çš„60%æ¥è‡ªäº 2016 - 2019 çš„ C4 + 22% æ¥è‡ªäº WebText2 + 16% æ¥è‡ªäºBooks + 3%æ¥è‡ªäºWikipediaï¼‰ã€‚å…¶ä¸­ï¼š

* **è¯­è¨€ç”Ÿæˆ**çš„èƒ½åŠ›æ¥è‡ªäºè¯­è¨€å»ºæ¨¡çš„**è®­ç»ƒç›®æ ‡** (language modeling)ã€‚
* **ä¸–ç•ŒçŸ¥è¯†**æ¥è‡ª 3000 äº¿å•è¯çš„**è®­ç»ƒè¯­æ–™åº“**ï¼ˆä¸ç„¶è¿˜èƒ½æ˜¯å“ªå„¿å‘¢ï¼‰ã€‚
* **æ¨¡å‹çš„ 1750 äº¿å‚æ•°**æ˜¯ä¸ºäº†**å­˜å‚¨çŸ¥è¯†**ï¼ŒLiang et al. (2022) çš„æ–‡ç« è¿›ä¸€æ­¥è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ ä»–ä»¬çš„ç»“è®ºæ˜¯ï¼Œ[çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„æ€§èƒ½ä¸æ¨¡å‹å¤§å°æ¯æ¯ç›¸å…³](https://crfm.stanford.edu/helm/v0.2.2/?group=knowledge)ã€‚
* ä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›æ¥æºåŠä¸ºä»€ä¹ˆä¸Šä¸‹æ–‡å­¦ä¹ å¯ä»¥æ³›åŒ–**ä»ç„¶éš¾ä»¥æº¯æº** ç›´è§‰ä¸Šï¼Œ<mark style="background-color:yellow;">è¿™ç§èƒ½åŠ›å¯èƒ½æ¥è‡ªäºåŒä¸€ä¸ªä»»åŠ¡çš„æ•°æ®ç‚¹åœ¨è®­ç»ƒæ—¶æŒ‰é¡ºåºæ’åˆ—åœ¨åŒä¸€ä¸ª batch ä¸­</mark>ã€‚ç„¶è€Œï¼Œ[<mark style="background-color:red;">å¾ˆå°‘æœ‰äººç ”ç©¶ä¸ºä»€ä¹ˆè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¼šä¿ƒä½¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»¥åŠä¸ºä»€ä¹ˆä¸Šä¸‹æ–‡å­¦ä¹ çš„è¡Œä¸ºä¸å¾®è°ƒ (fine-tuning) å¦‚æ­¤ä¸åŒã€‚</mark>](#user-content-fn-1)[^1]

There are three important abilities that the initial GPT-3 exhibit:

* **Language generation**: to follow a prompt and then generate a completion of the given prompt. Today, this might be the most ubiquitous way of human-LM interaction.
* **In-context learning**: to follow a few examples of a given task and then generate the solution for a new test case. It is interesting to note that, although being a language model, the original GPT-3 paper barely talks about â€œlanguage modelingâ€ â€” the authors devoted their writing efforts to their visions of in-context learning, which is the real focus of GPT-3.
* **World knowledge**: including factual knowledge and commonsense.

Where do these abilities come from?

Generally, the above three abilities should come from large-scale pretraining â€” to pretrain the 175B parameters model on 300B tokens (60% 2016 - 2019 C4 + 22% WebText2 + 16% Books + 3% Wikipedia). Where:

* The **language generation** ability comes from the language modeling **training objective**.
* The **world knowledge** comes from the 300B token **training corpora** (or where else it could be).
* The **175B model size** is for **storing knowledge**, which is further evidenced by Liang et al. (2022), who conclude that the performance on tasks requiring knowledge correlates with model size.
* The source of the in-context learning ability, as well as its generalization behavior, is still elusive. Intuitively, this ability may come from the fact that data points of the same task are ordered sequentially in the same batch during pretraining. Yet there is little study on why language model pretraining induces in-context learning, and why in-context learning behaves so differently than fine-tuning.

ä»¤äººå¥½å¥‡çš„æ˜¯ï¼Œåˆä»£**çš„GPT-3æœ‰å¤šå¼ºã€‚** å…¶å®æ¯”è¾ƒéš¾ç¡®å®šåˆä»£ GPT-3ï¼ˆåœ¨ OpenAI API ä¸­è¢«ç§°ä¸º`davinci`ï¼‰åˆ°åº•æ˜¯â€œå¼ºâ€è¿˜æ˜¯â€œå¼±â€ã€‚ä¸€æ–¹é¢ï¼Œå®ƒåˆç†åœ°å›åº”äº†æŸäº›ç‰¹å®šçš„æŸ¥è¯¢ï¼Œå¹¶åœ¨è®¸å¤šæ•°æ®é›†ä¸­è¾¾åˆ°äº†è¿˜ä¸é”™çš„æ€§èƒ½ï¼›å¦ä¸€æ–¹é¢ï¼Œå®ƒåœ¨è®¸å¤šä»»åŠ¡ä¸Šçš„**è¡¨ç°è¿˜ä¸å¦‚ T5 è¿™æ ·çš„å°æ¨¡å‹**ï¼ˆå‚è§å…¶åŸå§‹è®ºæ–‡ï¼‰ã€‚åœ¨ä»Šå¤©ï¼ˆ2022 å¹´ 12 æœˆï¼‰ChatGPT çš„æ ‡å‡†ä¸‹ï¼Œå¾ˆéš¾è¯´åˆä»£çš„ GPT-3 æ˜¯â€œæ™ºèƒ½çš„â€ã€‚Meta å¼€æºçš„ OPT æ¨¡å‹è¯•å›¾å¤ç°åˆä»£ GPT-3ï¼Œä½†å®ƒçš„èƒ½åŠ›ä¸å½“ä»Šçš„æ ‡å‡†ä¹Ÿå½¢æˆäº†å°–é”çš„å¯¹æ¯”ã€‚è®¸å¤šæµ‹è¯•è¿‡ OPT çš„äººä¹Ÿè®¤ä¸ºä¸ç°åœ¨çš„`text-davinci-002`ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹ç¡®å® â€œä¸å’‹åœ°â€ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒOPT å¯èƒ½æ˜¯åˆä»£ GPT-3 çš„ä¸€ä¸ªè¶³å¤Ÿå¥½çš„å¼€æºçš„è¿‘ä¼¼æ¨¡å‹äº†ï¼ˆæ ¹æ® OPT è®ºæ–‡å’Œæ–¯å¦ç¦å¤§å­¦çš„ HELM è¯„ä¼°ï¼‰ã€‚

è™½ç„¶åˆä»£çš„ GPT-3 å¯èƒ½è¡¨é¢ä¸Šçœ‹èµ·æ¥å¾ˆå¼±ï¼Œä½†åæ¥çš„å®éªŒè¯æ˜ï¼Œåˆä»£ GPT-3 æœ‰ç€éå¸¸å¼ºçš„æ½œåŠ›ã€‚è¿™äº›æ½œåŠ›åæ¥è¢«ä»£ç è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒ (instruction tuning) å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (reinforcement learning with human feedback, RLHF) è§£é”ï¼Œæœ€ç»ˆä½“å±•ç¤ºå‡ºæä¸ºå¼ºå¤§çš„çªç°èƒ½åŠ›ã€‚

A curious question is **how strong the initial GPT-3 is.**

It is rather challenging to determine whether the initial GPT-3 (`davinci` in OpenAI API) is â€œstrongâ€ or â€œweak.â€ On the one hand, it responds to certain queries reasonably and achieves OK-ish performance on many benchmarks; on the other, **it underperforms small models** like T5 on many tasks (see its original paper). It is also very hard to say the initial GPT-3 is â€œsmartâ€ in today's (= Dec 2022) ChatGPT standard. The sharp comparison of initial GPT-3â€™s ability v.s. todayâ€™s standard is replayed by Metaâ€™s OPT model, which is viewed as â€œjust badâ€ by many who have tested OPT (compared to `text-davinci-002`). Nevertheless, OPT might be a good enough open-source approximation to the initial GPT-3 (according to the OPT paper and Stanfordâ€™s HELM evaluation).

Although the initial GPT-3 might be superficially weak, it turns out later that these abilities serve as very important foundations of all the emergent abilities unlocked later by training on code, instruction tuning, and reinforcement learning with human feedback (RLHF).

### äºŒã€ä» 2020 ç‰ˆ GPT-3 åˆ° 2022 ç‰ˆ ChatGPT

ä»æœ€åˆçš„ GPT-3 å¼€å§‹ï¼Œä¸ºäº†å±•ç¤º OpenAI æ˜¯å¦‚ä½•å‘å±•åˆ°ChatGPTçš„ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹ GPT-3.5 çš„è¿›åŒ–æ ‘ï¼š

<figure><img src="../../.gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>

åœ¨ **2020 å¹´ 7 æœˆ**ï¼ŒOpenAI å‘å¸ƒäº†æ¨¡å‹ç´¢å¼•ä¸ºçš„ `davinci` çš„åˆä»£ [GPT-3 è®ºæ–‡](https://arxiv.org/abs/2005.14165)ï¼Œä»æ­¤å®ƒå°±å¼€å§‹ä¸æ–­è¿›åŒ–ã€‚åœ¨ **2021 å¹´ 7 æœˆ**ï¼Œ[Codex çš„è®ºæ–‡](https://arxiv.org/abs/2107.03374)å‘å¸ƒï¼Œå…¶ä¸­åˆå§‹çš„ Codex æ˜¯æ ¹æ®ï¼ˆå¯èƒ½æ˜¯å†…éƒ¨çš„ï¼‰120 äº¿å‚æ•°çš„ GPT-3 å˜ä½“è¿›è¡Œå¾®è°ƒçš„ã€‚åæ¥è¿™ä¸ª 120 äº¿å‚æ•°çš„æ¨¡å‹æ¼”å˜æˆ OpenAI API ä¸­çš„`code-cushman-001`ã€‚åœ¨ **2022 å¹´ 3 æœˆ**ï¼ŒOpenAI å‘å¸ƒäº†[æŒ‡ä»¤å¾®è°ƒ (instruction tuning) ](https://arxiv.org/abs/2203.02155)çš„è®ºæ–‡ï¼Œå…¶ç›‘ç£å¾®è°ƒ (supervised instruction tuning) çš„éƒ¨åˆ†å¯¹åº”äº†`davinci-instruct-beta`å’Œ`text-davinci-001`ã€‚åœ¨ **2022 å¹´ 4 æœˆè‡³ 7 æœˆçš„**ï¼ŒOpenAI å¼€å§‹å¯¹`code-davinci-002`æ¨¡å‹è¿›è¡Œ Beta æµ‹è¯•ï¼Œä¹Ÿç§°å…¶ä¸º Codexã€‚ç„¶åtext`-davinci-002`ã€`text-davinci-003`å’Œ`ChatGPT` éƒ½æ˜¯ä»`code-davinci-002`è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¾—åˆ°çš„ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜… OpenAIçš„æ¨¡å‹ç´¢å¼•æ–‡æ¡£ã€‚

å°½ç®¡ Codex å¬ç€åƒæ˜¯ä¸€ä¸ªåªç®¡ä»£ç çš„æ¨¡å‹ï¼Œä½†`code-davinci-002`å¯èƒ½æ˜¯æœ€å¼ºå¤§çš„é’ˆå¯¹**è‡ªç„¶è¯­è¨€**çš„GPT-3.5 å˜ä½“ï¼ˆä¼˜äº `text-davinci-002`å’Œ `-003`ï¼‰ã€‚`code-davinci-002`å¾ˆå¯èƒ½åœ¨æ–‡æœ¬å’Œä»£ç ä¸Šéƒ½ç»è¿‡è®­ç»ƒï¼Œç„¶åæ ¹æ®æŒ‡ä»¤è¿›è¡Œè°ƒæ•´ï¼ˆå°†åœ¨ä¸‹é¢è§£é‡Šï¼‰ã€‚ç„¶å**2022 å¹´ 5-6 æœˆ**å‘å¸ƒçš„`text-davinci-002`æ˜¯ä¸€ä¸ªåŸºäº`code-davinci-002`çš„æœ‰ç›‘ç£æŒ‡ä»¤å¾®è°ƒ (supervised instruction tuned) æ¨¡å‹ã€‚<mark style="background-color:red;">åœ¨</mark><mark style="background-color:red;">`text-davinci-002`</mark><mark style="background-color:red;">ä¸Šé¢è¿›è¡Œ</mark><mark style="background-color:red;">**æŒ‡ä»¤å¾®è°ƒ**</mark><mark style="background-color:red;">å¾ˆå¯èƒ½</mark><mark style="background-color:red;">**é™ä½**</mark><mark style="background-color:red;">äº†æ¨¡å‹çš„</mark><mark style="background-color:red;">**ä¸Šä¸‹æ–‡å­¦ä¹ **</mark><mark style="background-color:red;">èƒ½åŠ›ï¼Œ</mark><mark style="background-color:red;">**ä½†æ˜¯**</mark><mark style="background-color:red;">å¢å¼ºäº†</mark><mark style="background-color:red;">**æ¨¡å‹çš„**</mark><mark style="background-color:red;">é›¶æ ·æœ¬èƒ½åŠ›ï¼ˆå°†åœ¨ä¸‹é¢è§£é‡Šï¼‰</mark>ã€‚ç„¶åæ˜¯`text-davinci-003`å’Œ `ChatGPT`ï¼Œå®ƒä»¬éƒ½åœ¨ **2022 å¹´ 11 æœˆ**å‘å¸ƒï¼Œæ˜¯ä½¿ç”¨çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ çš„ç‰ˆæœ¬æŒ‡ä»¤å¾®è°ƒ (instruction tuning with reinforcement learning from human feedback) æ¨¡å‹çš„ä¸¤ç§ä¸åŒå˜ä½“ã€‚`text-davinci-003` æ¢å¤äº†ï¼ˆä½†ä»ç„¶æ¯”`code-davinci-002`å·®ï¼‰ä¸€äº›åœ¨`text-davinci-002` ä¸­ä¸¢å¤±çš„éƒ¨åˆ†**ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½**åŠ›ï¼ˆå¤§æ¦‚æ˜¯å› ä¸ºå®ƒåœ¨å¾®è°ƒçš„æ—¶å€™æ··å…¥äº†è¯­è¨€å»ºæ¨¡ï¼‰ å¹¶è¿›ä¸€æ­¥æ”¹è¿›äº†é›¶æ ·æœ¬èƒ½åŠ›ï¼ˆå¾—ç›ŠäºRLHFï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œ[<mark style="background-color:red;">ChatGPT ä¼¼ä¹</mark><mark style="background-color:red;">**ç‰ºç‰²äº†å‡ ä¹æ‰€æœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›**</mark><mark style="background-color:red;">æ¥</mark><mark style="background-color:red;">**æ¢å–**</mark><mark style="background-color:red;">å»ºæ¨¡å¯¹è¯å†å²çš„èƒ½åŠ›ã€‚</mark>](#user-content-fn-2)[^2]

æ€»çš„æ¥è¯´ï¼Œåœ¨ 2020 - 2021 å¹´æœŸé—´ï¼Œåœ¨`code-davinci-002`ä¹‹å‰ï¼ŒOpenAI å·²ç»æŠ•å…¥äº†å¤§é‡çš„ç²¾åŠ›é€šè¿‡ä»£ç è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ¥å¢å¼ºGPT-3ã€‚å½“ä»–ä»¬å®Œæˆ`code-davinci-002`æ—¶ï¼Œæ‰€æœ‰çš„èƒ½åŠ›éƒ½å·²ç»å­˜åœ¨äº†ã€‚å¾ˆå¯èƒ½åç»­çš„æŒ‡ä»¤å¾®è°ƒï¼Œæ— è®ºæ˜¯é€šè¿‡æœ‰ç›‘ç£çš„ç‰ˆæœ¬è¿˜æ˜¯å¼ºåŒ–å­¦ä¹ çš„ç‰ˆæœ¬ï¼Œéƒ½ä¼šåšä»¥ä¸‹äº‹æƒ…ï¼ˆç¨åä¼šè¯¦ç»†è¯´æ˜ï¼‰ï¼š

* æŒ‡ä»¤å¾®è°ƒ**ä¸ä¼šä¸ºæ¨¡å‹æ³¨å…¥æ–°çš„èƒ½åŠ›** â€”â€” <mark style="background-color:red;">æ‰€æœ‰çš„èƒ½åŠ›éƒ½å·²ç»å­˜åœ¨äº†ã€‚æŒ‡ä»¤å¾®è°ƒçš„ä½œç”¨æ˜¯</mark><mark style="background-color:red;">**è§£é” / æ¿€å‘è¿™äº›èƒ½åŠ›**</mark><mark style="background-color:red;">ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºæŒ‡ä»¤å¾®è°ƒçš„æ•°æ®é‡æ¯”é¢„è®­ç»ƒæ•°æ®é‡å°‘å‡ ä¸ªæ•°é‡çº§ï¼ˆåŸºç¡€çš„èƒ½åŠ›æ˜¯é€šè¿‡é¢„è®­ç»ƒæ³¨å…¥çš„ï¼‰ã€‚</mark>
* æŒ‡ä»¤å¾®è°ƒ**å°† GPT-3.5 çš„åˆ†åŒ–åˆ°ä¸åŒçš„æŠ€èƒ½æ ‘**ã€‚æœ‰äº›æ›´æ“…é•¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¦‚`text-davinci-003`ï¼Œæœ‰äº›æ›´æ“…é•¿å¯¹è¯ï¼Œå¦‚`ChatGPT`ã€‚
* æŒ‡ä»¤å¾®è°ƒ**é€šè¿‡ç‰ºç‰²æ€§èƒ½æ¢å–ä¸äººç±»çš„å¯¹é½ï¼ˆalignmentï¼‰**ã€‚ OpenAI çš„ä½œè€…åœ¨ä»–ä»¬çš„æŒ‡ä»¤å¾®è°ƒè®ºæ–‡ä¸­ç§°å…¶ä¸º â€œå¯¹é½ç¨â€ (alignment tax)ã€‚è®¸å¤šè®ºæ–‡éƒ½æŠ¥é“äº†`code-davinci-002`åœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼ˆä½†æ¨¡å‹ä¸ä¸€å®šç¬¦åˆäººç±»æœŸæœ›ï¼‰ã€‚ åœ¨`code-davinci-002`ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒåï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´åŠ ç¬¦åˆäººç±»æœŸå¾…çš„åé¦ˆï¼ˆæˆ–è€…è¯´æ¨¡å‹ä¸äººç±»å¯¹é½ï¼‰ï¼Œä¾‹å¦‚ï¼šé›¶æ ·æœ¬é—®ç­”ã€ç”Ÿæˆå®‰å…¨å’Œå…¬æ­£çš„å¯¹è¯å›å¤ã€æ‹’ç»è¶…å‡ºæ¨¡å‹å®ƒçŸ¥è¯†èŒƒå›´çš„é—®é¢˜ã€‚

In **Jul 2020**. OpenAI released the initial GPT-3 paper with the `davinci` model index, and it started to evolve. In **Jul 2021**, the Codex paper was released, where the initial Codex is fine-tuned from a (presumably internal) 12B GPT-3 variant. Later this 12B model evolved to be the `code-cushman-001` in OpenAI API. In **Mar 2022**, OpenAI released the instruction tuning paper, and its supervised tuning part corresponds to the `davinci-instruct-beta` and `text-davinci-001`. At some point in **Apr-Jul 2022**, OpenAI started to beta test the `code-davinci-002` model, also calling it Codex. Then `text-davinci-002`, `text-davinci-003`, and `ChatGPT` are all instruction-tuned from `code-davinci-002`. See OpenAIâ€™s Model Index document for more details.

Although called Codex, **code**-davinci-002 is probably **the most capable** GPT-3.5 variant for **natural language** (better than text-davinci-002 and 003). It is very likely code-davinci-002 is trained on both text and code, then tuned on instructions (will explain below). Then text-davinci-002, released in **May-Jun 2022**, is a supervised instruction-tuned model based on code-davinci-002. It is very likely that the **instruction tuning** on text-davinci-002 **decreased** the modelâ€™s **in-context learning** ability but **increased** the modelâ€™s **zero-shot** ability (will explain below). Then text-davinci-003 and ChatGPT, both released in **Nov 2022**, are two different variants of instruction-tuned models using Reinforcement Learning with Human Feedback. text-davinci-003 **recovered** (but still worse than code-davinci-002) some **in-context learning** ability that is lost in text-davinci-002 (presumably because it tunes the model with LM mix-in) **and further improved zero-shot** ability (thanks to RLHF). On the other hand, ChatGPT seems to have **sacrificed nearly all** of its **in-context learning** ability to **trade for** the ability to model **dialog** context.

In summary, during 2020-2021, before code-davinci-002, substantial efforts have been devoted to enhancing GPT-3 with code training and instruction tuning. When they have reached code-davinci-002, all the abilities are there. It is likely that the following-up instruction-tuning, either supervised or RLHF, does the following things (will detail later):

* Instruction tuning does **not inject new abilities** into the model â€” all abilities are already there. Instead, instruction tuning **unlocks/ elicit these abilities**. This is mostly because the instruction tuning data is orders or magnitudes less than the pretraining data.
* Instruction tuning **adjusts skillsets** of GPT-3.5 **towards different branches**. Some are better at in-context learning like text-davinci-003, some are better at dialog like ChatGPT.
* Instruction tuning **trade performance for alignment** with humans. The OpenAI authors call it â€œalignment taxâ€ in their instruction tuning paper. Also, many papers have reported code-davinci-002 achieves the best performance on benchmarks. Instruction tuning on code-davinci-002 gives the subsequent models alignments like zero-shot question answering, generating safe and impartial dialog responses, and rejecting questions beyond its knowledge scope.

### ä¸‰ã€Code-Davinci-002å’Œ Text-Davinci-002ï¼Œåœ¨ä»£ç ä¸Šè®­ç»ƒï¼Œåœ¨æŒ‡ä»¤ä¸Šå¾®è°ƒ

åœ¨`code-davinci-002`å’Œ`text-davinci-002`ä¹‹å‰ï¼Œæœ‰ä¸¤ä¸ªä¸­é—´æ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯ davinci-instruct-beta å’Œ text-davinci-001ã€‚ä¸¤è€…åœ¨å¾ˆå¤šæ–¹é¢éƒ½æ¯”ä¸Šè¿°çš„ä¸¤ä¸ª-002æ¨¡å‹å·®ï¼ˆä¾‹å¦‚ï¼Œtext-davinci-001 é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ä¸å¼ºï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­é‡ç‚¹ä»‹ç» -002 å‹å·ã€‚

Before code-davinci-002 and text-davinci-002, there are two intermediate models, namely davinci-instruct-beta and text-davinci-001. Both are worse than the two -002 models in many aspects (e.g., text-davinci-001 cannot do chain-of-thought reasoning). So we focus on the -002 models in this section.

#### **3.1 å¤æ‚æ¨ç†èƒ½åŠ›çš„æ¥æºå’Œæ³›åŒ–åˆ°æ–°ä»»åŠ¡çš„èƒ½åŠ›**

æˆ‘ä»¬å…³æ³¨`code-davinci-002`å’Œ`text-davinci-002`ï¼Œè¿™ä¸¤å…„å¼Ÿæ˜¯ç¬¬ä¸€ç‰ˆçš„ GPT3.5 æ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºä»£ç ï¼Œå¦ä¸€ä¸ªç”¨äºæ–‡æœ¬ã€‚å®ƒä»¬è¡¨ç°å‡ºäº†å››ç§ä¸åˆä»£ GPT-3 ä¸åŒçš„é‡è¦èƒ½åŠ›ï¼š

* **å“åº”äººç±»æŒ‡ä»¤**ï¼šä»¥å‰ï¼ŒGPT-3 çš„è¾“å‡ºä¸»è¦æ˜¯è®­ç»ƒé›†ä¸­å¸¸è§çš„å¥å­ã€‚ç°åœ¨çš„æ¨¡å‹ä¼šé’ˆå¯¹æŒ‡ä»¤ / æç¤ºè¯ç”Ÿæˆæ›´åˆç†çš„ç­”æ¡ˆï¼ˆè€Œä¸æ˜¯ç›¸å…³ä½†æ— ç”¨çš„å¥å­ï¼‰ã€‚
* **æ³›åŒ–åˆ°æ²¡æœ‰è§è¿‡çš„ä»»åŠ¡**ï¼šå½“ç”¨äºè°ƒæ•´æ¨¡å‹çš„æŒ‡ä»¤æ•°é‡è¶…è¿‡ä¸€å®šçš„è§„æ¨¡æ—¶ï¼Œæ¨¡å‹å°±å¯ä»¥è‡ªåŠ¨åœ¨ä»æ²¡è§è¿‡çš„æ–°æŒ‡ä»¤ä¸Šä¹Ÿèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å›ç­”ã€‚ **è¿™ç§èƒ½åŠ›å¯¹äºä¸Šçº¿éƒ¨ç½²è‡³å…³é‡è¦**ï¼Œå› ä¸ºç”¨æˆ·æ€»ä¼šææ–°çš„é—®é¢˜ï¼Œæ¨¡å‹å¾—ç­”å¾—å‡ºæ¥æ‰è¡Œã€‚
* **ä»£ç ç”Ÿæˆå’Œä»£ç ç†è§£**ï¼šè¿™ä¸ªèƒ½åŠ›å¾ˆæ˜¾ç„¶ï¼Œå› ä¸ºæ¨¡å‹ç”¨ä»£ç è®­ç»ƒè¿‡ã€‚
* **åˆ©ç”¨æ€ç»´é“¾ (chain-of-thought) è¿›è¡Œå¤æ‚æ¨ç†**ï¼šåˆä»£ GPT3 çš„æ¨¡å‹æ€ç»´é“¾æ¨ç†çš„èƒ½åŠ›å¾ˆå¼±ç”šè‡³æ²¡æœ‰ã€‚ **code-davinci-002 å’Œ text-davinci-002 æ˜¯ä¸¤ä¸ªæ‹¥æœ‰è¶³å¤Ÿå¼ºçš„æ€ç»´é“¾æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚**
  * æ€ç»´é“¾æ¨ç†ä¹‹æ‰€ä»¥é‡è¦ï¼Œæ˜¯å› ä¸ºæ€ç»´é“¾å¯èƒ½æ˜¯è§£é”çªç°èƒ½åŠ›å’Œè¶…è¶Šç¼©æ”¾æ³•åˆ™ (scaling laws) çš„å…³é”®ã€‚è¯·å‚é˜…[ä¸Šä¸€ç¯‡åšæ–‡](https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f)ã€‚

Now letâ€™s look at code-davinci-002 and text-davinci-002, the two first GPT3.5 models, one for code and the other for text. There are three important abilities they exhibit that differentiate them from the initial GPT-3

* **Responding to human instruction**: previously, the outputs of GPT-3 were mostly high-frequency prompt-completion patterns within the training set. Now the model generates reasonable answers to the prompt, rather than related but useless sentences.
* **Generalization to unseen tasks**: when the number of instructions used for tuning the model is beyond a certain scale, the model can automatically generate completions for new instructions that are not in the training set. **This ability is crucial for deployment**, as users with always come up with new prompts.
* **Code generation and code understanding**: obviously, because the model is trained on code.
* **Complex reasoning** **with chain-of-thought**: previously, the model could not do tasks requiring multi-step reasoning with chain-of-thought. **codex-davinci-002 and text-davinci-002 are the two initial models exhibiting chain-of-thought reasoning ability**.
  * The reason that chain-of-thought is important is because that CoT is likely to be the key to unlock the emergent abilities and transcend scaling laws. See the previous blog post.

è¿™äº›èƒ½åŠ›ä»ä½•è€Œæ¥ï¼Ÿ

ä¸ä¹‹å‰çš„æ¨¡å‹ç›¸æ¯”ï¼Œä¸¤ä¸ªä¸»è¦åŒºåˆ«æ˜¯**æŒ‡ä»¤å¾®è°ƒ**å’Œ**ä»£ç è®­ç»ƒ**ã€‚å…·ä½“æ¥è¯´

* èƒ½å¤Ÿ**å“åº”äººç±»æŒ‡ä»¤**çš„èƒ½åŠ›æ˜¯**æŒ‡ä»¤å¾®è°ƒ**çš„ç›´æ¥äº§ç‰©ã€‚
* <mark style="background-color:red;">**å¯¹æ²¡æœ‰è§è¿‡çš„æŒ‡ä»¤åšå‡ºåé¦ˆ**</mark><mark style="background-color:red;">çš„æ³›åŒ–èƒ½åŠ›æ˜¯åœ¨æŒ‡ä»¤æ•°é‡è¶…è¿‡ä¸€å®šç¨‹åº¦ä¹‹å</mark><mark style="background-color:red;">**è‡ªåŠ¨å‡ºç°çš„**</mark><mark style="background-color:red;">ï¼ŒT0ã€Flan å’Œ FlanPaLM è®ºæ–‡è¿›ä¸€æ­¥è¯æ˜äº†è¿™ä¸€ç‚¹</mark>
* <mark style="background-color:red;">ä½¿ç”¨</mark><mark style="background-color:red;">**æ€ç»´é“¾**</mark><mark style="background-color:red;">è¿›è¡Œ</mark><mark style="background-color:red;">**å¤æ‚æ¨ç†**</mark><mark style="background-color:red;">çš„èƒ½åŠ›å¾ˆå¯èƒ½æ˜¯</mark><mark style="background-color:red;">**ä»£ç è®­ç»ƒ**</mark><mark style="background-color:red;">çš„</mark><mark style="background-color:red;">**ä¸€ä¸ªç¥å¥‡çš„å‰¯äº§ç‰©**</mark><mark style="background-color:red;">ã€‚å¯¹æ­¤ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹çš„äº‹å®ä½œä¸ºä¸€äº›æ”¯æŒï¼š</mark>
  * æœ€åˆçš„ GPT-3 æ²¡æœ‰æ¥å—è¿‡ä»£ç è®­ç»ƒï¼Œå®ƒä¸èƒ½åš**æ€ç»´é“¾**ã€‚
  * text-davinci-001 æ¨¡å‹ï¼Œè™½ç„¶ç»è¿‡äº†æŒ‡ä»¤å¾®è°ƒï¼Œä½†ç¬¬ä¸€ç‰ˆæ€ç»´é“¾è®ºæ–‡æŠ¥å‘Šè¯´ï¼Œå®ƒçš„å®ƒæ€ç»´é“¾æ¨ç†çš„èƒ½åŠ›éå¸¸å¼± â€”â€” **æ‰€ä»¥æŒ‡ä»¤å¾®è°ƒå¯èƒ½ä¸æ˜¯æ€ç»´é“¾å­˜åœ¨çš„åŸå› ï¼Œä»£ç è®­ç»ƒæ‰æ˜¯æ¨¡å‹èƒ½åšæ€ç»´é“¾æ¨ç†çš„æœ€å¯èƒ½åŸå› ã€‚**
  * PaLM æœ‰ 5% çš„ä»£ç è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥åšæ€ç»´é“¾ã€‚
  * Codexè®ºæ–‡ä¸­çš„ä»£ç æ•°æ®é‡ä¸º 159G ï¼Œå¤§çº¦æ˜¯åˆä»£ GPT-3 5700 äº¿è®­ç»ƒæ•°æ®çš„28%ã€‚code-davinci-002 åŠå…¶åç»­å˜ä½“å¯ä»¥åšæ€ç»´é“¾æ¨ç†ã€‚
  * åœ¨ HELM æµ‹è¯•ä¸­ï¼ŒLiang et al. (2022) å¯¹ä¸åŒæ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ã€‚ ä»–ä»¬å‘ç°äº†é’ˆå¯¹ä»£ç è®­ç»ƒçš„æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„è¯­è¨€æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ 120äº¿å‚æ•°çš„code-cushman-001.ã€‚
  * æˆ‘ä»¬åœ¨ AI2 çš„å·¥ä½œä¹Ÿè¡¨æ˜ï¼Œå½“é…å¤‡å¤æ‚çš„æ€ç»´é“¾æ—¶ï¼Œcode-davinci-002 åœ¨ GSM8K ç­‰é‡è¦æ•°å­¦åŸºå‡†ä¸Šæ˜¯ç›®å‰è¡¨ç°æœ€å¥½çš„æ¨¡å‹
  * <mark style="background-color:green;">ç›´è§‰æ¥è¯´ï¼Œ</mark><mark style="background-color:green;">**é¢å‘è¿‡ç¨‹çš„ç¼–ç¨‹ (procedure-oriented programming)**</mark> <mark style="background-color:green;"></mark><mark style="background-color:green;">è·Ÿäººç±»</mark><mark style="background-color:green;">**é€æ­¥è§£å†³ä»»åŠ¡**</mark><mark style="background-color:green;">çš„è¿‡ç¨‹å¾ˆç±»ä¼¼ï¼Œ</mark><mark style="background-color:green;">**é¢å‘å¯¹è±¡ç¼–ç¨‹ (object-oriented programming)**</mark> <mark style="background-color:green;"></mark><mark style="background-color:green;">è·Ÿäººç±»</mark><mark style="background-color:green;">**å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç®€å•ä»»åŠ¡**</mark><mark style="background-color:green;">çš„è¿‡ç¨‹å¾ˆç±»ä¼¼ã€‚</mark>
  * <mark style="background-color:red;">ä»¥ä¸Šæ‰€æœ‰è§‚å¯Ÿç»“æœéƒ½æ˜¯ä»£ç ä¸æ¨ç†èƒ½åŠ› / æ€ç»´é“¾ ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä½†ä¸ä¸€å®šæ˜¯å› æœæ€§ã€‚è¿™ç§ç›¸å…³æ€§å¾ˆæœ‰è¶£ï¼Œä½†ç°åœ¨è¿˜æ˜¯ä¸€ä¸ªå¾…ç ”ç©¶çš„å¼€æ”¾æ€§é—®é¢˜ã€‚ç›®å‰çœ‹æ¥ï¼Œæˆ‘ä»¬</mark><mark style="background-color:red;">**æ²¡æœ‰éå¸¸ç¡®å‡¿çš„è¯æ®è¯æ˜ä»£ç å°±æ˜¯æ€ç»´é“¾å’Œå¤æ‚æ¨ç†çš„åŸå› **</mark><mark style="background-color:red;">ã€‚</mark>
* æ­¤å¤–ï¼Œ **ä»£ç è®­ç»ƒ**å¦ä¸€ä¸ªå¯èƒ½çš„å‰¯äº§å“æ˜¯é•¿è·ç¦»ä¾èµ–ï¼Œæ­£å¦‚Peter Liuæ‰€æŒ‡å‡ºï¼šâ€œè¯­è¨€ä¸­çš„ä¸‹ä¸ªè¯è¯­é¢„æµ‹é€šå¸¸æ˜¯éå¸¸å±€éƒ¨çš„ï¼Œè€Œä»£ç é€šå¸¸éœ€è¦æ›´é•¿çš„ä¾èµ–å…³ç³»æ¥åšä¸€äº›äº‹æƒ…ï¼Œæ¯”å¦‚å‰åæ‹¬å·çš„åŒ¹é…æˆ–å¼•ç”¨è¿œå¤„çš„å‡½æ•°å®šä¹‰â€ã€‚è¿™é‡Œæˆ‘æƒ³è¿›ä¸€æ­¥è¡¥å……çš„æ˜¯ï¼šç”±äºé¢å‘å¯¹è±¡ç¼–ç¨‹ä¸­çš„ç±»ç»§æ‰¿ï¼Œä»£ç ä¹Ÿå¯èƒ½æœ‰åŠ©äºæ¨¡å‹å»ºç«‹ç¼–ç å±‚æ¬¡ç»“æ„çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å¯¹è¿™ä¸€å‡è®¾çš„æ£€éªŒç•™ç»™æœªæ¥çš„å·¥ä½œã€‚



Where do these abilities come from?

Compared to the previous models, the two major differences are **instruction tuning** and **training on code**. Specifically

* The ability to **respond to** human **instructions** is a direct product of **instruction tuning**.
* The ability of **generalization** to **unseen instructions** is a **free lunch** given by **scaling** types of **instructions**, as is further evidenced by T0, Flan, and FlanPaLM papers
* The ability of **complex reasoning** with **chain-of-thought** is likely to be **a magical side product** of **training on code**:
  * The initial GPT-3 is not trained on code, and it cannot do chain-of-thought
  * The text-davinci-001, although being instruction tuned, ~~cannot do CoT~~ (corrected by Denny Zhou) can do CoT but the performance is significantly worse, as is reported by the first version of the CoT paper â€” so **instruction tuning may not be the reason for CoT. This leaves training on code to be be the number one suspect**.
  * PaLM has 5% code training data, and it can do chain-of-thought.
  * The code data in the codex paper is 159G, approximately 28% of the initial GPT-3 570G training data. code-davinci-002 and its subsequent variants can do chain-of-thought.
  * On the HELM evaluation, a massive-scale evaluation performed by Liang et al. (2022), the authors also found that models trained on/ for code has strong language reasoning abilities, including the 12B-sized code-cushman-001.
  * Our work at AI2 also shows that when equipped with complex chains of thought, code-davinci-002 is the SOTA model on important math benchmarks like GSM8K
  * As an intuition, think about how **procedure-oriented programming** is similar to **solving tasks step by step**, and how **object-oriented programming** is similar to **decomposing complex tasks into simpler ones**.
  * All the above observations are correlations between code and reasoning ability/ CoT. Such a correlation between code and reasoning ability/ CoT is very intriguing to the community and not well-understood. However, **there is still no hard evidence showing training on code is absolutely the reason for CoT and complex reasoning**. The source of CoT is still an open research problem.
* Additionally, **long-term dependency** might also be a nice side effect of **training on code.** As is pointed out by Peter Liu. â€œNext token prediction for language is usually very local, whereas code often requires longer dependencies to do things like close brackets or refer to distant defsâ€. I would further add: code may also give the model of encoding hierarchy due to inheritance in object-oriented programming. We leave the test of this hypothesis to future work.

å¦å¤–è¿˜è¦æ³¨æ„ä¸€äº›ç»†èŠ‚å·®å¼‚ï¼š

* **text-davinci-002 ä¸ code-davinci-002**
  * Code-davinci-002 æ˜¯åŸºç¡€æ¨¡å‹ï¼Œtext-davinci-002 æ˜¯æŒ‡ä»¤å¾®è°ƒ code-davinci-002 çš„äº§ç‰©ï¼ˆè§ OpenAI çš„æ–‡æ¡£ï¼‰ã€‚å®ƒåœ¨ä»¥ä¸‹æ•°æ®ä¸Šä½œäº†å¾®è°ƒï¼šï¼ˆä¸€ï¼‰äººå·¥æ ‡æ³¨çš„æŒ‡ä»¤å’ŒæœŸå¾…çš„è¾“å‡ºï¼›ï¼ˆäºŒï¼‰ç”±äººå·¥æ ‡æ³¨è€…é€‰æ‹©çš„æ¨¡å‹è¾“å‡ºã€‚
  * å½“æœ‰ä¸Šä¸‹æ–‡ç¤ºä¾‹ (in-context example) çš„æ—¶å€™ï¼Œ Code-davinci-002 æ›´æ“…é•¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼›å½“æ²¡æœ‰ä¸Šä¸‹æ–‡ç¤ºä¾‹ / é›¶æ ·æœ¬çš„æ—¶å€™ï¼Œ text-davinci-002 åœ¨é›¶æ ·æœ¬ä»»åŠ¡å®Œæˆæ–¹é¢è¡¨ç°æ›´å¥½ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œtext-davinci-002 æ›´ç¬¦åˆäººç±»çš„æœŸå¾…ï¼ˆå› ä¸ºå¯¹ä¸€ä¸ªä»»åŠ¡å†™ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯èƒ½ä¼šæ¯”è¾ƒéº»çƒ¦ï¼‰ã€‚
  * OpenAI ä¸å¤ªå¯èƒ½æ•…æ„ç‰ºç‰²äº†ä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›æ¢å–é›¶æ ·æœ¬èƒ½åŠ› â€”â€” ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„é™ä½æ›´å¤šæ˜¯æŒ‡ä»¤å­¦ä¹ çš„ä¸€ä¸ªå‰¯ä½œç”¨ï¼ŒOpenAI ç®¡è¿™å«å¯¹é½ç¨ã€‚
* **001 æ¨¡å‹ï¼ˆcode-cushman-001 å’Œ text-davinci-001ï¼‰v.s. 002 æ¨¡å‹ï¼ˆcode-davinci-002 å’Œ text-davinci-002ï¼‰**
  * 001 æ¨¡å‹ä¸»è¦æ˜¯ä¸ºäº†åšçº¯ä»£ç  / çº¯æ–‡æœ¬ä»»åŠ¡ï¼› 002 æ¨¡å‹åˆ™æ·±åº¦èåˆäº†ä»£ç è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»£ç å’Œæ–‡æœ¬éƒ½è¡Œã€‚
  * Code-davinci-002 å¯èƒ½æ˜¯ç¬¬ä¸€ä¸ªæ·±åº¦èåˆäº†ä»£ç è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹ã€‚è¯æ®æœ‰ï¼šcode-cushman-001 å¯ä»¥è¿›è¡Œæ¨ç†ä½†åœ¨çº¯æ–‡æœ¬ä¸Šè¡¨ç°ä¸ä½³ï¼Œtext-davinci-001 åœ¨çº¯æ–‡æœ¬ä¸Šè¡¨ç°ä¸é”™ä½†åœ¨æ¨ç†ä¸Šä¸å¤§è¡Œã€‚ code-davinci-002 åˆ™å¯ä»¥åŒæ—¶åšåˆ°è¿™ä¸¤ç‚¹ã€‚

There are certain detailed differences we would like to note:

* **text-davinci-002 v.s. code-davinci-002**
  * Code-davinci-002 is the base model, text-davinci-002 is the product of fine-tuning code-davinci-002 on (see documentation): (a). Human-annotated instructions and completions; (b). Self-generated completions chosen by human-annotators
  * Code-davinci-002 is better at in-context learning (when there are few task demonstrations); text-davinci-002 is better at zero-shot task completion (no demonstrations). In this sense, text-davinci-002 is more aligned with humans (because coming up with a task demonstration can be troublesome).
  * It is unlikely that OpenAI intentionally trades in-context learning ability for zero-shot ability â€” this tradeoff is more like a side effect of supervised instruction tuning (the alignment tax).
* **001 models (code-cushman-001 and text-davinci-001) v.s. 002 models (code-davinci-002 and text-davinci-002)**
  * The 001 models might be trained for code-~~only~~mainly / text-~~only~~mainly purpose (but still a mixture of text and code); the 002 models combines code tuning and instruction tuning
  * The first model after the combination is likely to be code-davinci-002. The supporting facts are that code-cushman-001 can do reasoning but not well on pure text, text-davinci-001 can do pure text but not well on reasoning. code-davinci-002 can do both.

#### 3.2 **è¿™äº›èƒ½åŠ›æ˜¯åœ¨é¢„è®­ç»ƒä¹‹åå·²ç»å­˜åœ¨è¿˜æ˜¯åœ¨ä¹‹åé€šè¿‡å¾®è°ƒæ³¨å…¥ï¼Ÿ**

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»ç¡®å®šäº†æŒ‡ä»¤å¾®è°ƒå’Œä»£ç è®­ç»ƒçš„å…³é”®ä½œç”¨ã€‚ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯å¦‚ä½•è¿›ä¸€æ­¥åˆ†æä»£ç è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒçš„å½±å“ï¼Ÿå…·ä½“æ¥è¯´ï¼š&#x20;

<mark style="background-color:orange;">ä¸Šè¿°ä¸‰ç§èƒ½åŠ›æ˜¯å¦</mark><mark style="background-color:orange;">**å·²ç»å­˜åœ¨äºåˆä»£çš„GPT-3**</mark><mark style="background-color:orange;">ä¸­ï¼Œåªæ˜¯</mark><mark style="background-color:orange;">**é€šè¿‡æŒ‡ä»¤å’Œä»£ç è®­ç»ƒè§¦å‘ / è§£é”**</mark><mark style="background-color:orange;">ï¼Ÿ</mark>&#x20;

æˆ–è€…è¿™äº›èƒ½åŠ›åœ¨åˆä»£çš„ GPT-3 ä¸­**å¹¶ä¸å­˜åœ¨**ï¼Œæ˜¯é€šè¿‡æŒ‡ä»¤å’Œä»£ç è®­ç»ƒ**æ³¨å…¥ï¼Ÿ**&#x20;

å¦‚æœç­”æ¡ˆå·²ç»åœ¨åˆä»£çš„ GPT-3 ä¸­ï¼Œ**é‚£ä¹ˆè¿™äº›èƒ½åŠ›ä¹Ÿåº”è¯¥åœ¨ OPT ä¸­ã€‚ å› æ­¤ï¼Œè¦å¤ç°è¿™äº›èƒ½åŠ›ï¼Œæˆ–è®¸å¯ä»¥ç›´æ¥é€šè¿‡æŒ‡ä»¤å’Œä»£ç è°ƒæ•´ OPTã€‚** ä½†æ˜¯ï¼Œcode-davinci-002 ä¹Ÿå¯èƒ½ä¸æ˜¯åŸºäºæœ€åˆçš„ GPT-3 davinciï¼Œè€Œæ˜¯åŸºäºæ¯”åˆä»£ GPT-3 æ›´å¤§çš„æ¨¡å‹ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œå¯èƒ½å°±æ²¡åŠæ³•é€šè¿‡è°ƒæ•´ OPT æ¥å¤ç°äº†ã€‚ç ”ç©¶ç¤¾åŒºéœ€è¦è¿›ä¸€æ­¥å¼„æ¸…æ¥š OpenAI è®­ç»ƒäº†ä»€ä¹ˆæ ·çš„æ¨¡å‹ä½œä¸º code-davinci-002 çš„åŸºç¡€æ¨¡å‹ã€‚

æˆ‘ä»¬æœ‰ä»¥ä¸‹çš„å‡è®¾å’Œè¯æ®ï¼š

* code-davinci-002çš„**åŸºç¡€æ¨¡å‹å¯èƒ½ä¸æ˜¯åˆä»£GPT-3 davinci æ¨¡å‹**ã€‚ä»¥ä¸‹æ˜¯è¯æ®ï¼š
  * åˆä»£çš„GPT-3åœ¨æ•°æ®é›† C4 2016 - 2019 ä¸Šè®­ç»ƒï¼Œè€Œ code-davinci-002 è®­ç»ƒé›†åˆ™åœ¨å»¶é•¿åˆ°2021å¹´æ‰ç»“æŸã€‚å› æ­¤ code-davinci-002 æœ‰å¯èƒ½åœ¨ C4 çš„ 2019-2021 ç‰ˆæœ¬ä¸Šè®­ç»ƒã€‚
  * åˆä»£çš„ GPT-3 æœ‰ä¸€ä¸ªå¤§å°ä¸º **2048** ä¸ªè¯çš„ä¸Šä¸‹æ–‡çª—å£ã€‚code-davinci-002 çš„ä¸Šä¸‹æ–‡çª—å£åˆ™ä¸º **8192**ã€‚GPT ç³»åˆ—ä½¿ç”¨ç»å¯¹ä½ç½®åµŒå…¥ (absolute positional embedding)ï¼Œç›´æ¥å¯¹ç»å¯¹ä½ç½®åµŒå…¥è¿›è¡Œå¤–æ¨è€Œä¸ç»è¿‡è®­ç»ƒæ˜¯æ¯”è¾ƒéš¾çš„ï¼Œå¹¶ä¸”ä¼šä¸¥é‡æŸå®³æ¨¡å‹çš„æ€§èƒ½ï¼ˆå‚è€ƒ Press et al., 2022ï¼‰ã€‚å¦‚æœ code-davinci-002 æ˜¯åŸºäºåˆä»£GPT-3ï¼Œé‚£OpenAI æ˜¯å¦‚ä½•æ‰©å±•ä¸Šä¸‹æ–‡çª—å£çš„ï¼Ÿ
* å¦ä¸€æ–¹é¢ï¼Œæ— è®ºåŸºç¡€æ¨¡å‹æ˜¯åˆä»£çš„ GPT-3 è¿˜æ˜¯åæ¥è®­ç»ƒçš„æ¨¡å‹ï¼Œ **éµå¾ªæŒ‡ä»¤å’Œé›¶æ ·æœ¬æ³›åŒ–çš„èƒ½åŠ›éƒ½å¯èƒ½å·²ç»å­˜åœ¨äºåŸºç¡€æ¨¡å‹**ä¸­ï¼Œåæ¥æ‰é€šè¿‡æŒ‡ä»¤å¾®è°ƒæ¥**è§£é”** ï¼ˆ**è€Œä¸æ˜¯æ³¨å…¥ï¼‰**
  * è¿™ä¸»è¦æ˜¯å› ä¸º OpenAI çš„è®ºæ–‡æŠ¥å‘Šçš„æŒ‡ä»¤æ•°æ®é‡å¤§å°åªæœ‰ 77Kï¼Œæ¯”é¢„è®­ç»ƒæ•°æ®å°‘äº†å‡ ä¸ªæ•°é‡çº§ã€‚
  * å…¶ä»–æŒ‡ä»¤å¾®è°ƒè®ºæ–‡è¿›ä¸€æ­¥è¯æ˜äº†æ•°æ®é›†å¤§å°å¯¹æ¨¡å‹æ€§èƒ½çš„å¯¹æ¯”ï¼Œä¾‹å¦‚ Chung et al. (2022) çš„å·¥ä½œä¸­ï¼Œ Flan-PaLM çš„æŒ‡ä»¤å¾®è°ƒä»…ä¸ºé¢„è®­ç»ƒè®¡ç®—çš„ 0.4%ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒæŒ‡ä»¤æ•°æ®ä¼šæ˜¾è‘—å°‘äºé¢„è®­ç»ƒæ•°æ®ã€‚
* ç„¶è€Œ **ï¼Œæ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›å¯èƒ½æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ä»£ç æ•°æ®æ³¨å…¥**
  * ä»£ç æ•°æ®é›†çš„è§„æ¨¡ä¸ä¸Šè¿°æŒ‡ä»¤å¾®è°ƒçš„æƒ…å†µä¸åŒã€‚è¿™é‡Œçš„ä»£ç æ•°æ®é‡è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å æ®è®­ç»ƒæ•°æ®çš„é‡è¦éƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼ŒPaLM æœ‰ 5% çš„ä»£ç è®­ç»ƒæ•°æ®ï¼‰
  * å¦‚ä¸Šæ‰€è¿°ï¼Œåœ¨ code-davinci-002 ä¹‹å‰çš„æ¨¡å‹ text-davinci-001 å¤§æ¦‚æ²¡æœ‰åœ¨ä»£ç æ•°æ®ä¸Šé¢å¾®è°ƒè¿‡ï¼Œæ‰€ä»¥å®ƒçš„æ¨ç† / æ€ç»´é“¾èƒ½åŠ›æ˜¯éå¸¸å·®çš„ï¼Œæ­£å¦‚ç¬¬ä¸€ç‰ˆæ€ç»´é“¾è®ºæ–‡ä¸­æ‰€æŠ¥å‘Šçš„é‚£æ ·ï¼Œæœ‰æ—¶ç”šè‡³æ¯”å‚æ•°é‡æ›´å°çš„ code-cushman-001 è¿˜å·®ã€‚
*   <mark style="background-color:red;">**åŒºåˆ†ä»£ç è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ•ˆæœçš„æœ€å¥½æ–¹æ³•**</mark><mark style="background-color:red;">å¯èƒ½æ˜¯</mark><mark style="background-color:red;">**æ¯”è¾ƒ code-cushman-001ã€T5 å’Œ FlanT5**</mark>

    * å› ä¸ºå®ƒä»¬å…·æœ‰ç›¸ä¼¼çš„æ¨¡å‹å¤§å°ï¼ˆ110äº¿ å’Œ 120äº¿ï¼‰ï¼Œç›¸ä¼¼çš„è®­ç»ƒæ•°æ®é›† (C4)ï¼Œå®ƒä»¬æœ€å¤§çš„åŒºåˆ«å°±æ˜¯æœ‰æ²¡æœ‰åœ¨ä»£ç ä¸Šè®­ç»ƒè¿‡ / æœ‰æ²¡æœ‰åšè¿‡æŒ‡ä»¤å¾®è°ƒã€‚
    * <mark style="background-color:red;">ç›®å‰è¿˜æ²¡æœ‰è¿™æ ·çš„æ¯”è¾ƒã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªç•™ç»™æœªæ¥çš„ç ”ç©¶ã€‚</mark>



At this stage, we already have identified the crucial role of instruction tuning and training on code. One important question is how to further disentangle the effects of code training and instruction tuning. Specifically:

**Are the above three abilities already there in the initial GPT-3** but **triggered/ unlocked by instruction and code training** or **not in the initial GPT-3** but **injected by instruction and code training?**

If the answer is already in the initial GPT-3, then these abilities **should also be in OPT**. **So to reproduce these abilities, one can directly instruction-and-code-tune OPT.** Yet it is also likely that code-davinci-002 is NOT based on the initial GPT-3 davinci, but some other models with unknown training procedures. If this is the case, tuning OPT might not be an option for reproduction, and the community need to figure out further what kind of model OpenAI has trained as the base model for code-davinci-002.

We have the following hypothesis and evidence:

* The **base model for code-davinci-002 is highly likely not be the initial GPT-3 davinci model**. Below are the evidence/ indicators
  * The initial GPT-3 is trained on C4 2016 - 2019. code-davinci-002 training set ends in 2021. So it is possible that code-davinci-002 is trained on the 2019-2021 version of C4.
  * The initial GPT-3 has a context window **2048**. code-davinci-002 has a context window ~~4096~~ **8192**. GPT series use absolute positional embedding, and directly extrapolating absolute positional embedding beyond training is challenging and can seriously harm the model performance (see Press et al., 2022). If code-davinci-002 is based on the initial GPT-3, how did OpenAI expand the context window?
  * There are recent works using sparse mixture-of-expert to substantially scale up the model parameter with constant computational cost, like Switch Transformers. If GPT-3.5 uses this technique, it can be **significantly larger** than GPT-3.
* On the other hand, either the base model is the initial GPT-3 or some later trained model, the ability to **follow instruction and zero-shot generalization may already be in the base model** and is later **unlocked** (**not injected**) by instruction tuning
  * This is primarily because the instruction data reported by OpenAIâ€™s paper is only 77K, which is orders of magnitudes less than the pretraining data.
  * The contrast of dataset size is further evidenced by other instruction tuning papers, e.g., Chung et al. (2022) where instruction tuning of Flan-PaLM is just 0.4% compute of pretraining. Instruction data is generally significantly less than pretraining data.
* Yet **complex reasoning may be injected from code data during the pretraining stage**
  * The scale of code data is different than the above instruction tuning case. Here the amount of code data is large enough to take a nontrivial portion of the training data (e.g., PaLM has 8% code training data)
  * As mentioned above, the reasoning/ chain of thought ability in text-davinci-001, the model before code-davinci-002, presumably not tuned on code, is very low, as is reported in the first version of the chain-of-thought paper, sometimes even worse than a smaller code-cushman-001.
* The best way to **disentangle the effects of code tuning and instruction tuning** might be to **compare code-cushman-001, T5, and FlanT5**
  * Because they have similar size (11B and 12B), and similar training data (C4), and the only difference are code/ instruction tuning.
  * There are no such comparisons yet. We leave this to future research.

### å››ã€text-davinci-003 å’Œ ChatGPTï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning from Human Feedback, RLHF) çš„å¨åŠ›

<mark style="background-color:red;">åœ¨å½“å‰é˜¶æ®µï¼ˆ2022 å¹´ 12 æœˆï¼‰ï¼Œ text-davinci-002ã€text-davinci-003 å’Œ ChatGPTä¹‹é—´</mark><mark style="background-color:red;">**å‡ ä¹æ²¡æœ‰ä¸¥æ ¼çš„ç»Ÿè®¡ä¸Šçš„æ¯”è¾ƒ**</mark> <mark style="background-color:red;"></mark><mark style="background-color:red;">ï¼Œä¸»è¦æ˜¯å› ä¸º</mark>

* text-davinci-003 å’Œ ChatGPT åœ¨æ’°å†™æœ¬æ–‡æ—¶æ‰å‘å¸ƒä¸åˆ°ä¸€ä¸ªæœˆã€‚
* ChatGPT ä¸èƒ½é€šè¿‡ OpenAI API è¢«è°ƒç”¨ï¼Œæ‰€ä»¥æƒ³è¦åœ¨æ ‡å‡†åŸºå‡†ä¸Šæµ‹è¯•å®ƒå¾ˆéº»çƒ¦ã€‚

æ‰€ä»¥åœ¨è¿™äº›æ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒæ›´å¤šæ˜¯**åŸºäºç ”ç©¶ç¤¾åŒºçš„é›†ä½“ç»éªŒ** ï¼ˆç»Ÿè®¡ä¸Šä¸æ˜¯å¾ˆä¸¥æ ¼ï¼‰ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬ç›¸ä¿¡åˆæ­¥çš„æè¿°æ€§æ¯”è¾ƒä»ç„¶å¯ä»¥æ­ç¤ºæ¨¡å‹çš„æœºåˆ¶ã€‚

At the current stage (Dec 2022), there are **few strict statistically clear comparisons** between text-davinci-002, text-davinci-003 and ChatGPT, mostly because

* text-davinci-003 and Chat have only been released less than a month when this article is written.
* ChatGPT cannot be called by OpenAI API, so it is troublesome to test it on standard benchmarks.

So the comparison between these models is more **based on the collective experiences of the community** (not very statistically strict). Yet we do believe certain initial descriptive comparisons still shed light on the underlying model mechanisms.

æˆ‘ä»¬é¦–å…ˆæ³¨æ„åˆ°ä»¥ä¸‹ text-davinci-002ï¼Œtext-davinci-003 å’Œ ChatGPT ä¹‹é—´çš„æ¯”è¾ƒï¼š

* æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹éƒ½ç»è¿‡**æŒ‡ä»¤å¾®è°ƒ**ã€‚
* **text-davinci-002** æ˜¯ä¸€ä¸ªç»è¿‡**ç›‘ç£å­¦ä¹ æŒ‡ä»¤å¾®è°ƒ** (supervised instruction tuning) \*\*\*\*çš„æ¨¡å‹
* **text-davinci-003 å’Œ ChatGPT** æ˜¯**åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ çš„æŒ‡ä»¤å¾®è°ƒ** (Instruction tuning with Reinforcement Learning from Human Feedback, RLHF)ã€‚è¿™æ˜¯å®ƒä»¬ä¹‹é—´æœ€æ˜¾ç€çš„åŒºåˆ«ã€‚

**è¿™æ„å‘³ç€å¤§å¤šæ•°æ–°æ¨¡å‹çš„è¡Œä¸ºéƒ½æ˜¯ RLHF çš„äº§ç‰©**ã€‚

é‚£ä¹ˆè®©æˆ‘ä»¬çœ‹çœ‹ RLHF è§¦å‘çš„èƒ½åŠ›ï¼š

* **è¯¦å®çš„å›åº”ï¼š** text-davinci-003 çš„ç”Ÿæˆé€šå¸¸æ¯” text-davinci-002é•¿ã€‚ ChatGPT çš„å›åº”åˆ™æ›´åŠ å†—é•¿ï¼Œä»¥è‡³äºç”¨æˆ·å¿…é¡»æ˜ç¡®è¦æ±‚â€œç”¨ä¸€å¥è¯å›ç­”æˆ‘â€ï¼Œæ‰èƒ½å¾—åˆ°æ›´åŠ ç®€æ´çš„å›ç­”ã€‚è¿™æ˜¯ RLHF çš„ç›´æ¥äº§ç‰©ã€‚
* **å…¬æ­£çš„å›åº”**ï¼šChatGPT é€šå¸¸å¯¹æ¶‰åŠå¤šä¸ªå®ä½“åˆ©ç›Šçš„äº‹ä»¶ï¼ˆä¾‹å¦‚æ”¿æ²»äº‹ä»¶ï¼‰ç»™å‡ºéå¸¸å¹³è¡¡çš„å›ç­”ã€‚è¿™ä¹Ÿæ˜¯RLHFçš„äº§ç‰©ã€‚
* **æ‹’ç»ä¸å½“é—®é¢˜ï¼š**è¿™æ˜¯å†…å®¹è¿‡æ»¤å™¨å’Œç”± RLHF è§¦å‘çš„æ¨¡å‹è‡ªèº«èƒ½åŠ›çš„ç»“åˆï¼Œè¿‡æ»¤å™¨è¿‡æ»¤æ‰ä¸€éƒ¨åˆ†ï¼Œç„¶åæ¨¡å‹å†æ‹’ç»ä¸€éƒ¨åˆ†ã€‚
* **æ‹’ç»å…¶çŸ¥è¯†èŒƒå›´ä¹‹å¤–çš„é—®é¢˜**ï¼šä¾‹å¦‚ï¼Œæ‹’ç»åœ¨2021 å¹´ 6 æœˆä¹‹åå‘ç”Ÿçš„æ–°äº‹ä»¶ï¼ˆå› ä¸ºå®ƒæ²¡åœ¨è¿™ä¹‹åçš„æ•°æ®ä¸Šè®­ç»ƒè¿‡ï¼‰ã€‚è¿™æ˜¯ RLHF æœ€ç¥å¥‡çš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿéšå¼åœ°åŒºåˆ†å“ªäº›é—®é¢˜åœ¨å…¶çŸ¥è¯†èŒƒå›´å†…ï¼Œå“ªäº›é—®é¢˜ä¸åœ¨å…¶çŸ¥è¯†èŒƒå›´å†…ã€‚

We first note that the following comparisons between text-davinci-002 v.s. text-davinci-003 v.s. ChatGPT:

* All three models are **instruction tuned**.
* t**ext-davinci-002** is a **supervised** instruction-tuned model
* t**ext-davinci-003 and ChatGPT** are instruction tuned with **Reinforcement Learning with Human Feedback (RLHF)**. This is the most prominent difference.

**This means that most of the new model behaviors are the product of RLHF**.

So letâ€™s look at the abilities triggered by RLHF:

* **Informative responses:** text-davinci-003â€™s generation is usually longer than text-davinci-002. ChatGPTâ€™s response is even more verbose such that one has to explicitly ask, â€œanswer me in one sentenceâ€ to make it concise. This is a direct product of RLHF.
* **Impartial responses**: ChatGPT often gives very balanced responses on events involving interests from multiple entities, such as political events. This is also a product of RLHF
* \*\*Rejecting improper questions.\*\*This is the combination of a content filter and the modelâ€™s own ability induced by RLHF.
* **Rejecting questions outside its knowledge scope**: for example, rejecting new events that happened after Jun 2021. This is the most amazing part of RLHF because it enables the model to implicitly and automatically classify which information is within its knowledge and which is not.

æœ‰ä¸¤ä»¶äº‹æƒ…å€¼å¾—æ³¨æ„ï¼š

* æ‰€æœ‰çš„èƒ½åŠ›éƒ½æ˜¯æ¨¡å‹æœ¬æ¥å°±æœ‰çš„ï¼Œ **è€Œä¸æ˜¯é€šè¿‡RLHF æ³¨å…¥çš„**ã€‚ RLHF çš„ä½œç”¨æ˜¯**è§¦å‘ / è§£é”çªç°èƒ½åŠ›**ã€‚è¿™ä¸ªè®ºç‚¹ä¸»è¦æ¥è‡ªäºæ•°æ®é‡å¤§å°çš„æ¯”è¾ƒï¼šå› ä¸ºä¸é¢„è®­ç»ƒçš„æ•°æ®é‡ç›¸æ¯”ï¼ŒRLHF å ç”¨çš„è®¡ç®—é‡ / æ•°æ®é‡è¦å°‘å¾—å¤šã€‚
* æ¨¡å‹[**çŸ¥é“å®ƒä¸çŸ¥é“ä»€ä¹ˆä¸æ˜¯é€šè¿‡ç¼–å†™è§„åˆ™æ¥å®ç°çš„**](#user-content-fn-3)[^3]**ï¼Œ** è€Œæ˜¯é€šè¿‡RLHFè§£é”çš„ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ä»¤äººæƒŠè®¶çš„å‘ç°ï¼Œå› ä¸º RLHF çš„æœ€åˆç›®æ ‡æ˜¯è®©æ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„å›ç­”ï¼Œè¿™æ›´å¤šæ˜¯è®©æ¨¡å‹ç”Ÿæˆå®‰å…¨çš„å¥å­ï¼Œè€Œä¸æ˜¯è®©æ¨¡å‹çŸ¥é“å®ƒä¸çŸ¥é“çš„å†…å®¹ã€‚

å¹•åå‘ç”Ÿçš„äº‹æƒ…å¯èƒ½æ˜¯ï¼š

* ChatGPT: é€šè¿‡**ç‰ºç‰²ä¸Šä¸‹æ–‡å­¦ä¹ **çš„èƒ½åŠ›**æ¢å–å»ºæ¨¡å¯¹è¯å†å²**çš„èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºç»éªŒçš„è§‚æµ‹ç»“æœï¼Œå› ä¸º ChatGPT ä¼¼ä¹ä¸åƒ text-davinci-003 é‚£æ ·å—åˆ°ä¸Šä¸‹æ–‡æ¼”ç¤ºçš„å¼ºçƒˆå½±å“ã€‚
* text-davinci-003ï¼š**æ¢å¤äº†** text-davinci-002 æ‰€ç‰ºç‰²çš„**ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›**ï¼Œ **æé«˜é›¶æ ·æœ¬çš„èƒ½åŠ›**ã€‚ ~~æˆ‘ä»¬ä¸ç¡®å®šè¿™æ˜¯å¦ä¹Ÿæ˜¯ RLHF æˆ–å…¶ä»–ä¸œè¥¿çš„å‰¯äº§å“ã€‚~~ æ ¹æ®instructGPTçš„è®ºæ–‡ï¼Œè¿™æ˜¯æ¥è‡ªäºå¼ºåŒ–å­¦ä¹ è°ƒæ•´é˜¶æ®µæ··å…¥äº†è¯­è¨€å»ºæ¨¡çš„ç›®æ ‡ï¼ˆè€Œä¸æ˜¯ RLHF æœ¬èº«ï¼‰ã€‚

There are two important things to notice:

* All the abilities are intrinsically within the model, **not injected by RLHF**. RLHF **triggers/unlock** these abilities to emerge. This is again because of the data size, as the RLHF tuning takes significantly less portion of computing compared to pretraining.
* **Knowing what it does not know is not achieved by writing rules;** it is also unlocked by RLHF. This is a very surprising finding, as the original goal of RLHF is for alignment, which is more related to generating safe responses than knowing what the model does not know.

What happens behind the scene might be:

* ChatGPT: **Trade in-context learning for dialog history modeling**. This is an empirical observation as ChatGPT seems not to be strongly affected by in-context demonstrations as text-davinci-003 does.
* Text-davinci-003: **recover the in-context learning ability** sacrificed by text-davinci-002 and **improve the zero-shot ability**. ~~We are not sure if this is also a side product of RLHF or something else.~~ According to the instructGPT paper, this is from the LM-mixing during the RL tuning stage (not RLHF itself).

### äº”ã€æ€»ç»“**å½“å‰é˜¶æ®µ GPT-3.5 çš„è¿›åŒ–å†ç¨‹**

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä»”ç»†æ£€æŸ¥äº†æ²¿ç€è¿›åŒ–æ ‘å‡ºç°çš„æ‰€æœ‰èƒ½åŠ›ï¼Œä¸‹è¡¨æ€»ç»“äº†æ¼”åŒ–è·¯å¾„ï¼š

| èƒ½åŠ›                                                                                                                                        | OpenAIæ¨¡å‹                                            | è®­ç»ƒæ–¹æ³•                | OpenAI API                            | OpenAIè®ºæ–‡                            | è¿‘ä¼¼çš„å¼€æºæ¨¡å‹                                      |
| ----------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | ------------------- | ------------------------------------- | ----------------------------------- | -------------------------------------------- |
| <mark style="background-color:yellow;">GPT-3ç³»åˆ—</mark>                                                                                     |                                                     |                     |                                       |                                     |                                              |
| <p>è¯­è¨€ç”Ÿæˆ</p><p>+ ä¸–ç•ŒçŸ¥è¯† </p><p>+ ä¸Šä¸‹æ–‡å­¦ä¹ </p>                                                                                                   | GPT-3åˆå§‹ç‰ˆæœ¬  **å¤§éƒ¨åˆ†çš„èƒ½åŠ›å·²ç»å­˜åœ¨äºæ¨¡å‹ä¸­ï¼Œå°½ç®¡è¡¨é¢ä¸Šçœ‹èµ·æ¥å¾ˆå¼±ã€‚**           | è¯­è¨€å»ºæ¨¡                | Davinci                               | GPT-3è®ºæ–‡                             | Meta OPT                                     |
| <p>+ éµå¾ªäººç±»çš„æŒ‡ä»¤</p><p>+ æ³›åŒ–åˆ°æ²¡æœ‰è§è¿‡çš„ä»»åŠ¡</p>                                                                                                       | Instruct-GPTåˆå§‹ç‰ˆæœ¬                                    | æŒ‡ä»¤å¾®è°ƒ                | Davinci-Instruct-Beta                 | Instruct-GPTè®ºæ–‡                      | <p>T0è®ºæ–‡ </p><p>Google FLANè®ºæ–‡</p>             |
| + ä»£ç ç†è§£+ ä»£ç ç”Ÿæˆ                                                                                                                              | Codexåˆå§‹ç‰ˆæœ¬                                           | åœ¨ä»£ç ä¸Šè¿›è¡Œè®­ç»ƒ            | Code-Cushman-001                      | Codexè®ºæ–‡                             | Salesforce CodeGen                           |
| <mark style="background-color:yellow;">GPT-3.5ç³»åˆ—</mark>                                                                                   |                                                     |                     |                                       |                                     |                                              |
| <p>++ ä»£ç ç†è§£ </p><p>++ ä»£ç ç”Ÿæˆ </p><p>++ å¤æ‚æ¨ç† / æ€ç»´é“¾ (ä¸ºä»€ä¹ˆ?)</p><p>+ é•¿è·ç¦»çš„ä¾èµ– (å¾ˆå¯èƒ½)</p>                                                            | ç°åœ¨çš„Codex**GPT3.5ç³»åˆ—ä¸­æœ€å¼ºå¤§çš„æ¨¡å‹**                         | åœ¨ä»£ç +æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒåœ¨æŒ‡ä»¤ä¸Šè¿›è¡Œå¾®è°ƒ | Code-Davinci-002 (ç›®å‰å…è´¹çš„ç‰ˆæœ¬ = 2022å¹´12æœˆ) | Codex è®ºæ–‡                            |                                              |
| <p>++ éµå¾ªäººç±»æŒ‡ä»¤</p><p>- ä¸Šä¸‹æ–‡å­¦ä¹ </p><p>- æ¨ç†èƒ½åŠ›</p><p>++ é›¶æ ·æœ¬ç”Ÿæˆ</p>                                                                                | æœ‰ç›‘ç£çš„Instruct-GPT **é€šè¿‡ç‰ºç‰²ä¸Šä¸‹æ–‡å­¦ä¹ æ¢å–é›¶æ ·æœ¬ç”Ÿæˆçš„èƒ½åŠ›**            | ç›‘ç£å­¦ä¹ ç‰ˆçš„æŒ‡ä»¤å¾®è°ƒ          | Text-Davinci-002                      | Instruct-GPTè®ºæ–‡, æœ‰ç›‘ç£çš„éƒ¨åˆ†              | <p>T0è®ºæ–‡</p><p>Google FLANè®ºæ–‡</p>              |
| <p>+ éµå¾ªäººç±»ä»·å€¼è§‚ </p><p>+ åŒ…å«æ›´å¤šç»†èŠ‚çš„ç”Ÿæˆ</p><p>+ ä¸Šä¸‹æ–‡å­¦ä¹ </p><p>+ é›¶æ ·æœ¬ç”Ÿæˆ</p>                                                                           | ç»è¿‡RLHFè®­ç»ƒçš„Instruct-GPT**å’Œ002æ¨¡å‹ç›¸æ¯”ï¼Œå’Œäººç±»æ›´åŠ å¯¹é½ï¼Œå¹¶ä¸”æ›´å°‘çš„æ€§èƒ½æŸå¤±** | å¼ºåŒ–å­¦ä¹ ç‰ˆçš„æŒ‡ä»¤å¾®è°ƒ          | Text-Davinci-003                      | Instruct-GPTè®ºæ–‡, RLHFéƒ¨åˆ†ï¼Œä»äººç±»åé¦ˆä¸­çš„å­¦ä¹ æ‘˜è¦ã€‚ | <p>DeepMind Sparrow è®ºæ–‡ </p><p>AI2 RL4LMs</p> |
| <p>++ éµå¾ªäººç±»ä»·å€¼è§‚ ++ åŒ…å«æ›´å¤šç»†èŠ‚çš„ç”Ÿæˆ </p><p>++ <mark style="background-color:red;">æ‹’ç»çŸ¥è¯†èŒƒå›´å¤–çš„é—®é¢˜ (ä¸ºä»€ä¹ˆ?)</mark> </p><p>++ å»ºæ¨¡å¯¹è¯å†å²çš„èƒ½åŠ› </p><p>-- ä¸Šä¸‹æ–‡å­¦ä¹ </p> | ChatGPT **é€šè¿‡ç‰ºç‰²ä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›å–å–å»ºæ¨¡å¯¹è¯å†å²çš„èƒ½åŠ›**                 | ä½¿ç”¨å¯¹è¯æ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ æŒ‡ä»¤å¾®è°ƒ    |                                       |                                     | <p>DeepMind Sparrowè®ºæ–‡</p><p>AI2 RL4LMs</p>   |

æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼š

* è¯­è¨€ç”Ÿæˆèƒ½åŠ› + åŸºç¡€ä¸–ç•ŒçŸ¥è¯† + ä¸Šä¸‹æ–‡å­¦ä¹ éƒ½æ˜¯æ¥è‡ªäºé¢„è®­ç»ƒï¼ˆ`davinci`ï¼‰
* å­˜å‚¨å¤§é‡çŸ¥è¯†çš„èƒ½åŠ›æ¥è‡ª 1750 äº¿çš„å‚æ•°é‡ã€‚
* éµå¾ªæŒ‡ä»¤å’Œæ³›åŒ–åˆ°æ–°ä»»åŠ¡çš„èƒ½åŠ›æ¥è‡ªäºæ‰©å¤§æŒ‡ä»¤å­¦ä¹ ä¸­æŒ‡ä»¤çš„æ•°é‡ï¼ˆ`Davinci-instruct-beta`)
* æ‰§è¡Œå¤æ‚æ¨ç†çš„èƒ½åŠ›å¾ˆå¯èƒ½æ¥è‡ªäºä»£ç è®­ç»ƒï¼ˆ`code-davinci-002`ï¼‰
* <mark style="background-color:green;">ç”Ÿæˆä¸­ç«‹ã€å®¢è§‚çš„èƒ½åŠ›ã€å®‰å…¨å’Œç¿”å®çš„ç­”æ¡ˆæ¥è‡ªä¸äººç±»çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼š</mark>
  * å¦‚æœæ˜¯ç›‘ç£å­¦ä¹ ç‰ˆï¼Œå¾—åˆ°çš„æ¨¡å‹æ˜¯`text-davinci-002`
  * å¦‚æœæ˜¯å¼ºåŒ–å­¦ä¹ ç‰ˆ (RLHF) ï¼Œå¾—åˆ°çš„æ¨¡å‹æ˜¯`text-davinci-003`
  * æ— è®ºæ˜¯æœ‰ç›‘ç£è¿˜æ˜¯ RLHF ï¼Œæ¨¡å‹åœ¨å¾ˆå¤šä»»åŠ¡çš„æ€§èƒ½éƒ½æ— æ³•è¶…è¿‡ code-davinci-002 ï¼Œè¿™ç§å› ä¸ºå¯¹é½è€Œé€ æˆæ€§èƒ½è¡°é€€çš„ç°è±¡å«åšå¯¹é½ç¨ã€‚
* å¯¹è¯èƒ½åŠ›ä¹Ÿæ¥è‡ªäº RLHFï¼ˆ`ChatGPT`ï¼‰ï¼Œå…·ä½“æ¥è¯´å®ƒç‰ºç‰²äº†ä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œæ¥æ¢å–ï¼š
  * å»ºæ¨¡å¯¹è¯å†å²
  * å¢åŠ å¯¹è¯ä¿¡æ¯é‡
  * æ‹’ç»æ¨¡å‹çŸ¥è¯†èŒƒå›´ä¹‹å¤–çš„é—®é¢˜

We have concluded:

* The language generation ability + basic world knowledge + in-context learning are from pretraining (`davinci`)
* The ability to store a large amount of knowledge is from the 175B scale.
* The ability to follow instructions and generalizing to new tasks are from scaling instruction tuning (`davinci-instruct-beta`)
* The ability to perform complex reasoning is likely to be from training on code (`code-davinci-002`)
* The ability to generate neutral, objective, safe, and informative answers are from alignment with human. Specifically:
  * If supervised tuning, the resulting model is `text-davinci-002`
  * If RLHF, the resulting model is `text-davinci-003`
  * Either supervised or RLHF, the models cannot outperform code-davinci-002 on many tasks, which is called the alignment tax.
* The dialog ability is also from RLHF (`ChatGPT`), specifically it tradeoffs in-context learning for:
  * Modeling dialog history
  * Increased informativeness
  * Rejecting questions outside the modelâ€™s knowledge scope



### å…­ã€GPT-3.5 ç›®å‰ä¸èƒ½åšä»€ä¹ˆ

è™½ç„¶GPT-3.5æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ä¸­çš„é‡è¦ä¸€æ­¥ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å®Œå…¨åŒ…å«è®¸å¤šç ”ç©¶äººå‘˜ï¼ˆåŒ…æ‹¬ AI2ï¼‰è®¾æƒ³çš„æ‰€æœ‰ç†æƒ³å±æ€§ã€‚ä»¥ä¸‹æ˜¯GPT-3.5ä¸å…·å¤‡çš„æŸäº›é‡è¦å±æ€§ï¼š

* **å®æ—¶æ”¹å†™æ¨¡å‹çš„ä¿¡å¿µ**ï¼šå½“æ¨¡å‹è¡¨è¾¾å¯¹æŸäº‹çš„ä¿¡å¿µæ—¶ï¼Œå¦‚æœè¯¥ä¿¡å¿µæ˜¯é”™è¯¯çš„ï¼Œæˆ‘ä»¬å¯èƒ½å¾ˆéš¾çº æ­£å®ƒï¼š
  * æˆ‘æœ€è¿‘é‡åˆ°çš„ä¸€ä¸ªä¾‹å­æ˜¯ï¼šChatGPT åšæŒè®¤ä¸º 3599 æ˜¯ä¸€ä¸ªè´¨æ•°ï¼Œå°½ç®¡å®ƒæ‰¿è®¤ 3599 = 59 \* 61ã€‚å¦å¤–ï¼Œè¯·å‚é˜…Redditä¸Šå…³äºæ¸¸å¾—æœ€å¿«çš„æµ·æ´‹å“ºä¹³åŠ¨ç‰©çš„ä¾‹å­ã€‚
  * ç„¶è€Œï¼Œæ¨¡å‹ä¿¡å¿µçš„å¼ºåº¦ä¼¼ä¹å­˜åœ¨ä¸åŒçš„å±‚æ¬¡ã€‚ä¸€ä¸ªä¾‹å­æ˜¯å³ä½¿æˆ‘å‘Šè¯‰å®ƒè¾¾æ–¯Â·ç»´è¾¾ï¼ˆæ˜Ÿçƒå¤§æˆ˜ç”µå½±ä¸­çš„äººç‰©ï¼‰èµ¢å¾—äº†2020å¹´å¤§é€‰ï¼Œæ¨¡å‹ä¾æ—§ä¼šè®¤ä¸ºç¾å›½ç°ä»»æ€»ç»Ÿæ˜¯æ‹œç™»ã€‚ä½†æ˜¯å¦‚æœæˆ‘å°†é€‰ä¸¾å¹´ä»½æ”¹ä¸º 2024 å¹´ï¼Œå®ƒå°±ä¼šè®¤ä¸ºæ€»ç»Ÿæ˜¯è¾¾æ–¯Â·ç»´è¾¾æ˜¯ 2026 å¹´çš„æ€»ç»Ÿã€‚
* **å½¢å¼æ¨ç†**ï¼šGPT-3.5ç³»åˆ—ä¸èƒ½åœ¨æ•°å­¦æˆ–ä¸€é˜¶é€»è¾‘ç­‰å½¢å¼ä¸¥æ ¼çš„ç³»ç»Ÿä¸­è¿›è¡Œæ¨ç†ï¼š
  * åœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„æ–‡çŒ®ä¸­ï¼Œ â€œæ¨ç†â€ ä¸€è¯çš„å®šä¹‰å¾ˆå¤šæ—¶å€™ä¸å¤ªæ˜ç¡®ã€‚ä½†å¦‚æœæˆ‘ä»¬ä»æ¨¡ç³Šæ€§çš„è§’åº¦æ¥çœ‹ï¼Œä¾‹å¦‚ä¸€äº›é—®é¢˜ (a) éå¸¸æ¨¡æ£±ä¸¤å¯ï¼Œæ²¡æœ‰æ¨ç†ï¼›(b) æœ‰ç‚¹å„¿é€»è¾‘åœ¨é‡Œé¢ï¼Œä½†æœ‰äº›åœ°æ–¹ä¹Ÿå¯ä»¥æ¨¡ç³Šï¼›(c) éå¸¸ä¸¥è°¨ï¼Œä¸èƒ½æœ‰ä»»ä½•æ­§ä¹‰ã€‚é‚£ä¹ˆï¼Œ
  * æ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°è¿›è¡Œ (b) ç±»çš„å¸¦æ¨¡ç³Šæ€§çš„æ¨ç†ï¼Œä¾‹å­æœ‰ï¼š
    * ç”Ÿæˆå¦‚ä½•åšè±†è…è„‘çš„æ–¹æ³•ã€‚åšè±†è…è„‘çš„æ—¶å€™ï¼Œä¸­é—´å¾ˆå¤šæ­¥éª¤æ¨¡ç³Šä¸€ç‚¹æ˜¯å¯ä»¥æ¥å—çš„ï¼Œæ¯”å¦‚åˆ°åº•æ˜¯åšå’¸çš„è¿˜æ˜¯åšç”œçš„ã€‚åªè¦æ•´ä½“æ­¥éª¤å¤§è‡´æ­£ç¡®ï¼Œåšå‡ºæ¥çš„è±†è…è„‘å„¿å°±èƒ½åƒã€‚
    * æ•°å­¦å®šç†çš„è¯æ˜æ€è·¯ã€‚è¯æ˜æ€è·¯æ˜¯ç”¨è¯­è¨€è¡¨è¾¾çš„éæ­£å¼çš„é€æ­¥è§£æ³•ï¼Œå…¶ä¸­æ¯ä¸€æ­¥çš„ä¸¥æ ¼æ¨å¯¼å¯ä»¥ä¸ç”¨å¤ªå…·ä½“ã€‚è¯æ˜æ€è·¯ç»å¸¸è¢«ç”¨åˆ°æ•°å­¦æ•™å­¦ï¼šåªè¦è€å¸ˆç»™ä¸€ä¸ªå¤§è‡´æ­£ç¡®çš„æ•´ä½“æ­¥éª¤ï¼Œå­¦ç”Ÿå°±å¯ä»¥å¤§æ¦‚æ˜ç™½ã€‚ç„¶åè€å¸ˆæŠŠå…·ä½“çš„è¯æ˜ç»†èŠ‚ä½œä¸ºä½œä¸šå¸ƒç½®ç»™å­¦ç”Ÿï¼Œç­”æ¡ˆç•¥ã€‚
  * GPT-3.5 ä¸èƒ½è¿›è¡Œç±»å‹ (c) çš„æ¨ç†ï¼ˆæ¨ç†ä¸èƒ½å®¹å¿æ­§ä¹‰ï¼‰ã€‚
    * ä¸€ä¸ªä¾‹å­æ˜¯ä¸¥æ ¼çš„æ•°å­¦è¯æ˜ï¼Œè¦æ±‚ä¸­é—´æ­¥éª¤ä¸­ä¸èƒ½è·³ï¼Œä¸èƒ½æ¨¡ç³Šï¼Œä¸èƒ½é”™ã€‚
    * ä½†è¿™ç§ä¸¥æ ¼æ¨ç†åˆ°åº•æ˜¯åº”è¯¥è®©è¯­è¨€æ¨¡å‹åšè¿˜æ˜¯è®©ç¬¦å·ç³»ç»Ÿåšè¿˜æœ‰å¾…è®¨è®ºã€‚ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œä¸å…¶åŠªåŠ›è®© GPT åšä¸‰ä½æ•°åŠ æ³•ï¼Œä¸å¦‚ç›´æ¥è°ƒ Pythonã€‚
* **ä»äº’è”ç½‘è¿›è¡Œæ£€ç´¢**ï¼šGPT-3.5 ç³»åˆ—ï¼ˆæš‚æ—¶ï¼‰ä¸èƒ½ç›´æ¥æœç´¢äº’è”ç½‘
  * ä½†æ˜¯æœ‰ä¸€ç¯‡ WebGPT è®ºæ–‡å‘è¡¨äº2021å¹´12æœˆï¼Œé‡Œé¢å°±è®© GPT è°ƒç”¨äº†æœç´¢å¼•æ“ã€‚æ‰€ä»¥æ£€ç´¢çš„èƒ½åŠ›å·²ç»åœ¨ OpenAI å†…éƒ¨è¿›è¡Œäº†æµ‹è¯•ã€‚
  * è¿™é‡Œéœ€è¦åŒºåˆ†çš„ä¸€ç‚¹æ˜¯ï¼ŒGPT-3.5 çš„ä¸¤ä¸ªé‡è¦ä½†ä¸åŒçš„èƒ½åŠ›æ˜¯ **çŸ¥è¯†** å’Œ **æ¨ç†**ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿ **å°†çŸ¥è¯†éƒ¨åˆ†å¸è½½åˆ°å¤–éƒ¨çš„æ£€ç´¢ç³»ç»Ÿï¼Œè®©è¯­è¨€æ¨¡å‹åªä¸“æ³¨äºæ¨ç†ï¼Œè¿™å°±å¾ˆä¸é”™äº†ã€‚** å› ä¸ºï¼š
    * æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†æ€»æ˜¯åœ¨æŸä¸ªæ—¶é—´è¢«åˆ‡æ–­ã€‚æ¨¡å‹å§‹ç»ˆéœ€è¦æœ€æ–°çš„çŸ¥è¯†æ¥å›ç­”æœ€æ–°çš„é—®é¢˜ã€‚
    * å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºè¿‡ 1750 äº¿çš„å‚æ•°å¤§é‡ç”¨äºå­˜å‚¨çŸ¥è¯†ã€‚å¦‚æœæˆ‘ä»¬å¯ä»¥å°†çŸ¥è¯†å¸è½½åˆ°æ¨¡å‹ä¹‹å¤–ï¼Œé‚£ä¹ˆæ¨¡å‹å‚æ•°å¯èƒ½ä¼šå¤§å¤§å‡å°‘ï¼Œæœ€ç»ˆå®ƒç”šè‡³å¯ä»¥åœ¨æ‰‹æœºä¸Šè¿è¡Œï¼ˆç–¯ç‹‚çš„æƒ³æ³•ï¼Œä½† ChatGPT å·²ç»è¶³å¤Ÿç§‘å¹»äº†ï¼Œè°çŸ¥é“æœªæ¥ä¼šæ€æ ·å‘¢).

Although it is a major step in NLP research, GPT-3.5 does not fully contain all the ideal properties envisaged by many NLP researchers (including AI2). Below are certain important properties that GPT-3.5 does not have:

* **On-the-fly overwriting the modelâ€™s belief**: when the model expresses its belief in something, it might be hard to correct it when the belief is wrong:
  * One recent example I encountered is that ChatGPT insists that 3599 is a prime number even though it acknowledged that 3599 = 59 \* 61. Also, see the fastest marine mammal example on Reddit.
  * Yet there seems to be a hierarchy of how strong the belief is. One example is that the model believes the current president of the US is Biden, even if I told it Darth Vader won the 2020 election. Yet if I change the election year to 2024, it believes that the president is Darth Vader in 2026.
* **Formal reasoning**: the GPT-3.5 series cannot do reasoning within formal, strict systems like math or first-order logic
  * In the NLP literature, the word â€œreasoningâ€ is less well-defined. Yet if we view there is a spectrum of ambiguity like (a) very ambiguous, no reasoning; (b) mixture of logic and ambiguous statements; (c). no ambiguity has to be very rigorous, then,
  * The model can do very well on type (b) reasoning with ambiguity; examples include:
    * Generating a procedure of how to cook pizza. It is acceptable if there exist ambiguities in the intermediate steps, like using sausage or pineapple. As long as the overall steps are approximately correct, the pizza is eatable (sorry if you are Italian).
    * Generating proof sketch of a theorem. Proof sketches are informal step-by-step procedures expressed in language, where the strict derivation of one step can be left unspecified. This is a useful math teaching and thinking tool. As long as the overall steps are approximately correct, the students can fill in the details as homework.
  * The model cannot do type (c) reasoning (reasoning does not tolerate ambiguity).
    * One example is deriving strict proofs that require no mistakes in intermediate steps.
    * Yet whether such reasoning should be done by a language model or a symbolic system is up for discussion. For example, instead of trying hard to make GPT do three digits addition, one might simply call Python.
* **Retrieval from the Internet**: the GPT-3.5 series cannot directly search the internet (for now)
  * Yet there was a WebGPT paper published in Dec 2021. It is likely that this is already tested internally within OpenAI.
  * The two important but different abilities of GPT-3.5 are **knowledge** and **reasoning**. Generally, it would be ideal if we could **offload the knowledge part to the outside retrieval system and let the language model only focus on reasoning.** This is because:
    * The modelâ€™s internal knowledge is always cut off at a certain time. The model always needs up-to-date knowledge to answer up-to-date questions.
    * Recall we have discussed that is 175B parameter is heavily used for storing knowledge. If we could offload knowledge to be outside the model, then the model parameter might be significantly reduced such that eventually, it can run on a cellphone (call this crazy here, but ChatGPT is already science fiction enough, who knows what the future will be).

## ä¸ƒã€ç»“è®º

åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»”ç»†æ£€æŸ¥äº†GPT-3.5ç³»åˆ—çš„èƒ½åŠ›èŒƒå›´ï¼Œå¹¶è¿½æº¯äº†å®ƒä»¬æ‰€æœ‰çªç°èƒ½åŠ›çš„æ¥æºã€‚åˆä»£GPT-3æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒè·å¾—ç”Ÿæˆèƒ½åŠ›ã€ä¸–ç•ŒçŸ¥è¯†å’Œin-context learningã€‚ç„¶åé€šè¿‡instruction tuningçš„æ¨¡å‹åˆ†æ”¯è·å¾—äº†éµå¾ªæŒ‡ä»¤å’Œèƒ½æ³›åŒ–åˆ°æ²¡æœ‰è§è¿‡çš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç»è¿‡ä»£ç è®­ç»ƒçš„åˆ†æ”¯æ¨¡å‹åˆ™è·å¾—äº†ä»£ç ç†è§£çš„èƒ½åŠ›ï¼Œä½œä¸ºä»£ç è®­ç»ƒçš„å‰¯äº§å“ï¼Œæ¨¡å‹åŒæ—¶æ½œåœ¨åœ°è·å¾—äº†å¤æ‚æ¨ç†çš„èƒ½åŠ›ã€‚ç»“åˆè¿™ä¸¤ä¸ªåˆ†æ”¯ï¼Œcode-davinci-002ä¼¼ä¹æ˜¯å…·æœ‰æ‰€æœ‰å¼ºå¤§èƒ½åŠ›çš„æœ€å¼ºGPT-3.5æ¨¡å‹ã€‚æ¥ä¸‹æ¥é€šè¿‡æœ‰ç›‘ç£çš„instruction tuningå’Œ RLHFé€šè¿‡ç‰ºç‰²æ¨¡å‹èƒ½åŠ›æ¢å–ä¸äººç±»å¯¹é½ï¼Œå³å¯¹é½ç¨ã€‚ RLHF ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´ç¿”å®å’Œå…¬æ­£çš„ç­”æ¡ˆï¼ŒåŒæ—¶æ‹’ç»å…¶çŸ¥è¯†èŒƒå›´ä¹‹å¤–çš„é—®é¢˜ã€‚

æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¤Ÿå¸®åŠ©æä¾›ä¸€ä¸ªæ¸…æ™°çš„GPTè¯„ä¼°å›¾ï¼Œå¹¶å¼•å‘ä¸€äº›å…³äºè¯­è¨€æ¨¡å‹ã€instruction tuningå’Œcode tuningçš„è®¨è®ºã€‚æœ€é‡è¦çš„æ˜¯ï¼Œ **æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯ä»¥ä½œä¸ºåœ¨å¼€æºç¤¾åŒºå†…å¤ç°GPT-3.5çš„è·¯çº¿å›¾ã€‚**

> â€œå› ä¸ºå±±å°±åœ¨é‚£é‡Œã€‚â€â€”â€”ä¹”æ²»Â·é©¬æ´›é‡Œï¼Œç ç©†æœ—ç›å³°æ¢é™©å…ˆé©±

In this post, we scrutinize the spectrum of abilities of the GPT-3.5 series and trace back to the sources of all their emergent abilities. The initial GPT-3 model gains its generation ability, world knowledge, and in-context learning from pretraining. Then the instruction tuning branch gains the ability to follow instructions and generalization to unseen tasks. The training on code branch gains the ability of code understanding, and potentially the side product of complex reasoning. Combining the two branches, code-davinci-002 seems to be the most capable GPT-3.5 model with all the powerful abilities. The following supervised instruction tuning and RLHF trades model ability for alignment with humans, i.e., the alignment tax. RLHF enables the model to generate more informative and impartial answers while rejecting questions outside its knowledge scope.

We hope this article can help provide a clear picture of the evaluation of GPT, and stir some discussion about language models, instruction tuning, and code tuning. Most importantly, **we hope this article can serve as the roadmap for reproducing GPT-3.5 within the open-source community.**

> â€œBecause it's thereâ€ â€” George Mallory, the pioneer of the Mount Everest expedition

## **å¸¸è§é—®é¢˜**

* è¿™ç¯‡æ–‡ç« ä¸­çš„è¿™äº›è¯´æ³•æ›´åƒæ˜¯å‡è®¾ (hypothesis) è¿˜æ˜¯ç»“è®º (conclusion)ï¼Ÿ
  * **å¤æ‚æ¨ç†çš„èƒ½åŠ›æ¥è‡ªäºä»£ç è®­ç»ƒ**æ˜¯æˆ‘ä»¬å€¾å‘äºç›¸ä¿¡çš„å‡è®¾ (hypothesis)
  * **å¯¹æ²¡æœ‰è§è¿‡çš„ä»»åŠ¡æ³›åŒ–èƒ½åŠ›æ¥è‡ªå¤§è§„æ¨¡æŒ‡ä»¤å­¦ä¹ ** æ˜¯è‡³å°‘ 4 ç¯‡è®ºæ–‡çš„ç»“è®º (conclusion)
  * **GPT-3.5æ¥è‡ªäºå…¶ä»–å¤§å‹åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸æ˜¯1750äº¿å‚æ•°çš„GPT-3** æ˜¯æœ‰æ ¹æ®çš„çŒœæµ‹ (educated guess)ã€‚
  * **æ‰€æœ‰è¿™äº›èƒ½åŠ›éƒ½å·²ç»å­˜åœ¨äº†ï¼Œé€šè¿‡instruction tuningï¼Œæ— è®ºæ˜¯æœ‰ç›‘ç£å­¦ä¹ æˆ–å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼æ¥è§£é”è€Œä¸æ˜¯æ³¨å…¥è¿™äº›èƒ½åŠ›** æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¼ºçš„å‡è®¾ (strong assumption)ã€‚ ä¸»è¦æ˜¯å› ä¸ºinstruction tuningæ•°æ®é‡æ¯”é¢„è®­ç»ƒæ•°æ®é‡å°‘äº†å‡ ä¸ªæ•°é‡çº§ã€‚
  * ç»“è®º (conclusion) = è®¸å¤šè¯æ®æ”¯æŒè¿™äº›è¯´æ³•çš„æ­£ç¡®æ€§ï¼›å‡è®¾ (hypothesis) = æœ‰æ­£é¢è¯æ®ä½†ä¸å¤Ÿæœ‰åŠ›ï¼›æœ‰æ ¹æ®çš„çŒœæµ‹ (educated guess) = æ²¡æœ‰ç¡®å‡¿çš„è¯æ®ï¼Œä½†æŸäº›å› ç´ ä¼šæŒ‡å‘è¿™ä¸ªæ–¹å‘
*   ä¸ºä»€ä¹ˆå…¶ä»–æ¨¡å‹ï¼ˆå¦‚ OPT å’Œ BLOOMï¼‰æ²¡æœ‰é‚£ä¹ˆå¼ºå¤§ï¼Ÿ

    * OPTå¤§æ¦‚æ˜¯å› ä¸ºè®­ç»ƒè¿‡ç¨‹å¤ªä¸ç¨³å®š
    * BLOOMçš„æƒ…å†µåˆ™æœªçŸ¥ã€‚å¦‚æœæ‚¨æœ‰æ›´å¤šæ„è§ï¼Œè¯·ä¸æˆ‘è”ç³»


* Are these claims in this article more like hypothesis or conclusions?
  * **Complex reasoning is from training on code** is a hypothesis we tend to believe
  * **Generalization on unseen tasks is from scaling instruction tuning** is a conclusion from at least 4 papers
  * **GPT-3.5 is from a large base model than GPT-3 175B** is an educated guess.
  * **All these abilities are there, instruction tuning, either supervised or reinforce, unlocks, but not inject, these abilities** is a strong hypothesis, very strong such that it is hard not to believe. Mostly because instruction tuning data are orders of magnitudes less than pretraining data
  * Conclusion = many evidences supporting its correctness; Hypothesis = there are positive evidences but not strong enough; Educated guess = no hard evidence but certain factors indicate so
* Why other models like OPT and BLOOM are not so strong?
  * OPT probably because training process too unstable
  * BLOOM do not know. Do contact me if you have more comments

## é™„å½• - ä¸­è‹±æœ¯è¯­å¯¹ç…§è¡¨

| è‹±æ–‡                                                | ä¸­æ–‡          | é‡Šä¹‰                                     |
| ------------------------------------------------- | ----------- | -------------------------------------- |
| Emergent Ability                                  | çªç°èƒ½åŠ›        | å°æ¨¡å‹æ²¡æœ‰ï¼Œåªåœ¨æ¨¡å‹å¤§åˆ°ä¸€å®šç¨‹åº¦æ‰ä¼šå‡ºç°çš„èƒ½åŠ›                |
| Prompt                                            | æç¤ºè¯         | æŠŠ prompt è¾“å…¥ç»™å¤§æ¨¡å‹ï¼Œå¤§æ¨¡å‹ç»™å‡º completion       |
| In-Context Learning                               | ä¸Šä¸‹æ–‡å­¦ä¹        | åœ¨ prompt é‡Œé¢å†™å‡ ä¸ªä¾‹å­ï¼Œæ¨¡å‹å°±å¯ä»¥ç…§ç€è¿™äº›ä¾‹å­åšç”Ÿæˆ        |
| Instruction Tuning                                | æŒ‡ä»¤å¾®è°ƒ        | ç”¨ instruction æ¥ fine-tune å¤§æ¨¡å‹          |
| Code Tuning                                       | åœ¨ä»£ç ä¸Šå¾®è°ƒ      | ç”¨ä»£ç æ¥ fine-tune å¤§æ¨¡å‹                     |
| Reinforcement Learning with Human Feedback (RLHF) | åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  | è®©äººç»™æ¨¡å‹ç”Ÿæˆçš„ç»“æœæ‰“åˆ†ï¼Œç”¨äººæ‰“çš„åˆ†æ¥è°ƒæ•´æ¨¡å‹                |
| Chain-of-Thought                                  | æ€ç»´é“¾         | åœ¨å†™ prompt çš„æ—¶å€™ï¼Œä¸ä»…ç»™å‡ºç»“æœï¼Œè¿˜è¦ä¸€æ­¥ä¸€æ­¥åœ°å†™ç»“æœæ˜¯æ€ä¹ˆæ¨å‡ºæ¥çš„ |
| Scaling Laws                                      | ç¼©æ”¾æ³•åˆ™        | æ¨¡å‹çš„æ•ˆæœçš„çº¿æ€§å¢é•¿è¦æ±‚æ¨¡å‹çš„å¤§å°æŒ‡æ•°å¢é•¿                  |
| Alignment                                         | ä¸äººç±»å¯¹é½       | è®©æœºå™¨ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„ï¼Œç¬¦åˆäººç±»ä»·å€¼è§‚çš„å¥å­                |

[^1]: * &#x20;[https://thegradient.pub/in-context-learn](https://thegradient.pub/in-context-learning-in-context/)

    <!---->

    * [Min et. al. 2022. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)
    * [An Explanation of In-context Learning as Implicit Bayesian Inference](https://arxiv.org/abs/2111.02080)

    <!---->

    *

    <img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b9f99cde-06ab-473c-b67d-f08447a7f3ce/Untitled.png" alt="Untitled" data-size="original">













[^2]: ?



[^3]: GPT4çš„è§£é‡Šï¼šå¯¹äºä¸€ä¸ªç³»ç»Ÿï¼ˆå¦‚äººå·¥æ™ºèƒ½æˆ–ä¸ªä½“ï¼‰ï¼Œè¦æ„è¯†åˆ°è‡ªå·±æ‰€ä¸çŸ¥é“çš„ä¿¡æ¯æˆ–çŸ¥è¯†ï¼Œå¹¶éé€šè¿‡åˆ¶å®šä¸€ç³»åˆ—è§„åˆ™æ¥è¾¾åˆ°è¿™ä¸€ç›®æ ‡ã€‚æ¢å¥è¯è¯´ï¼Œé€šè¿‡è§„åˆ™ç¼–å†™å¯èƒ½æ— æ³•ä½¿ä¸€ä¸ªç³»ç»Ÿå®Œå…¨äº†è§£è‡ªå·±çš„çŸ¥è¯†è¾¹ç•Œå’Œä¸è¶³ä¹‹å¤„ã€‚å®ç°è¿™ä¸€ç›®æ ‡å¯èƒ½éœ€è¦æ›´ä¸ºå¤æ‚çš„æ–¹æ³•ï¼Œå¦‚åŠ¨æ€å­¦ä¹ å’Œè‡ªæˆ‘è°ƒæ•´ã€‚
